{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EXPLORED EDA"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## IMPORTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ---------------------------------------------------------\n",
                "# GENERAL IMPORTS\n",
                "# ---------------------------------------------------------\n",
                "# CORE PYTHON & DATA MANIPULATION LIBRARIES\n",
                "import numpy as np                # Numerical computations, arrays, math operations\n",
                "import pandas as pd               # Data handling, DataFrames, time-series structures\n",
                "import warnings                   # Warning control and suppression\n",
                "import math                       # Math utilities (sqrt, log, floor, ceil, etc.)\n",
                "import random                     # Generate random values\n",
                "import os                         # OS-level utilities (path handling, directory checks, file management)\n",
                "# VISUALIZATION LIBRARIES\n",
                "import matplotlib.pyplot as plt   # Main plotting library\n",
                "import seaborn as sns             # Statistical and enhanced visualization tools\n",
                "\n",
                "from wordcloud import WordCloud   # Wordcloud plotting\n",
                "# STEP 10) SPLIT\n",
                "from sklearn.model_selection import train_test_split   # Split dataset into train / test subsets\n",
                "# STEP 14) FEATURE SELECTION\n",
                "from sklearn.feature_selection import VarianceThreshold        # Variance feature selection for NLP or TABULAR UNSUPERVISED learnings\n",
                "from sklearn.feature_selection import f_regression, chi2        # f_regression ‚Üí F-test for numerical target, chi2 ‚Üí Chi-square test for categorical targets for NLP - SUPERVISED learnings\n",
                "from sklearn.feature_selection import SelectKBest, f_classif   # Univariate feature selection for classification for TABULAR - SUPERVISED learnings\n",
                "# MODEL SELECTION\n",
                "from sklearn.model_selection import GridSearchCV       # Hyperparameter optimization via grid search\n",
                "from sklearn.tree import plot_tree                     # Visualization of decision tree structures\n",
                "from pickle import dump                                # Save trained models to disk (serialization)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# UNSUPERVISED ALGORITHMS\n",
                "# ---------------------------------------------------------\n",
                "# Clustering models\n",
                "from sklearn.cluster import KMeans # Centroid-based clustering algorithm that partitions data into K clusters by minimizing within-cluster sum of squared distances (WCSS)\n",
                "from sklearn.cluster import DBSCAN # Density-based clustering algorithm that groups samples based on local density and identifies noise points that do not belong to any cluster\n",
                "from sklearn.cluster import AgglomerativeClustering # Hierarchical clustering algorithm that builds clusters bottom-up by iteratively merging the closest clusters according to a linkage criterion\n",
                "# Dimensionality reduction\n",
                "from sklearn.decomposition import TruncatedSVD # Method similar to PCA but suitable for sparse / high-dimensional data (common for NLP vectors like TF-IDF), computes a low-rank approximation via SVD without centering the data\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# CLASSIFICATION ALGORITHMS\n",
                "# ---------------------------------------------------------\n",
                "# METRICS\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   # Core classification metrics\n",
                "# PREDICTION MODELS\n",
                "from sklearn.linear_model import LogisticRegression     # Logistic regression classifier\n",
                "from sklearn.tree import DecisionTreeClassifier         # Decision tree classifier\n",
                "from sklearn.naive_bayes import GaussianNB              # Gaussian Naive Bayes for continuous inputs\n",
                "from sklearn.naive_bayes import MultinomialNB           # Multinomial Naive Bayes (common for NLP)\n",
                "from sklearn.naive_bayes import BernoulliNB             # Bernoulli Naive Bayes (binary features)\n",
                "from sklearn.neighbors import KNeighborsClassifier      # K-Nearest Neighbors model (classification)\n",
                "# BAGGING ENSEMBLE\n",
                "from sklearn.ensemble import RandomForestClassifier     # Ensemble of decision trees (bagging)\n",
                "# BOOSTING ENSEMBLE\n",
                "from sklearn.ensemble import AdaBoostClassifier         # AdaBoost boosting algorithm\n",
                "from sklearn.ensemble import GradientBoostingClassifier # Gradient boosting classifier\n",
                "from xgboost import XGBClassifier                       # Extreme Gradient Boosting (high-performance)\n",
                "from lightgbm import LGBMClassifier                     # LightGBM (optimized gradient boosting)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# REGRESSION ALGORITHMS\n",
                "# ---------------------------------------------------------\n",
                "# METRICS\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   # Regression performance metrics\n",
                "# PREDICTION MODELS\n",
                "from sklearn.linear_model import LinearRegression       # Linear regression model\n",
                "from sklearn.linear_model import Lasso                  # L1 regularized regression\n",
                "from sklearn.linear_model import Ridge                  # L2 regularized regression\n",
                "from sklearn.tree import DecisionTreeRegressor          # Regression decision tree\n",
                "from sklearn.neighbors import KNeighborsRegressor       # K-Nearest Neighbors model (regression)\n",
                "# BAGGING ENSEMBLE\n",
                "from sklearn.ensemble import RandomForestRegressor      # Ensemble of regression trees (bagging)\n",
                "# BOOSTING ENSEMBLE\n",
                "from sklearn.ensemble import AdaBoostRegressor          # Boosting algorithm for regression\n",
                "from sklearn.ensemble import GradientBoostingRegressor  # Gradient boosting regressor\n",
                "from xgboost import XGBRegressor                        # XGBoost regressor\n",
                "from lightgbm import LGBMRegressor                      # LightGBM regressor\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# NLP DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# STEP 6) TEXT PROCESSING\n",
                "import regex as re                          # Advanced regular expressions for robust text cleaning\n",
                "# STEP 7) TEXT LEMMATIZATION\n",
                "import nltk                                 # NLP toolkit required for lemmatization and POS tagging\n",
                "from nltk.stem import WordNetLemmatizer     # Lemmatizer based on WordNet lexical database\n",
                "from nltk.corpus import stopwords           # Stopword lists for multiple languages\n",
                "from nltk.corpus import wordnet             # WordNet POS tags for accurate lemmatization\n",
                "from PIL import Image                       # Load and manipulate image files (used for mask shapes in WordCloud)\n",
                "try:\n",
                "    nltk.data.find(\"corpora/wordnet\")\n",
                "except LookupError:\n",
                "    nltk.download(\"wordnet\", quiet=True)\n",
                "try:\n",
                "    nltk.data.find(\"corpora/stopwords\")\n",
                "except LookupError:\n",
                "    nltk.download(\"stopwords\", quiet=True)\n",
                "try:\n",
                "    nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
                "except LookupError:\n",
                "    nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
                "try:\n",
                "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
                "except LookupError:\n",
                "    nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
                "# STEP 13) VECTORIZATION\n",
                "from sklearn.feature_extraction.text import CountVectorizer     # Convert text into token frequency counts\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer     # Convert text into TF-IDF weighted features\n",
                "from sklearn.decomposition import PCA                           # Dimensionality reduction (e.g., for visualization)\n",
                "from sklearn.metrics import silhouette_score                    # Clustering quality metric (optional for NLP clustering)\n",
                "# STEP 16) PREDICTION MODELS\n",
                "from sklearn.svm import SVC                                     # Support Vector Machine classifier (kernel-based classification)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TIME-SERIES DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# STEP 3) DECOMPOSING\n",
                "from statsmodels.tsa.seasonal import seasonal_decompose         # Decompose time-series into trend, seasonal, and residual components\n",
                "# STEP 4) STATIONARITY ANALYSIS\n",
                "from statsmodels.tsa.stattools import adfuller                  # Dickey-Fuller test for stationarity evaluation\n",
                "# STEP 5) VARIABILITY ANALYSIS\n",
                "from statsmodels.stats.diagnostic import acorr_ljungbox         # Ljung-Box test for checking autocorrelation in residuals\n",
                "# STEP 6) AUTOCORRELATION ANALYSIS\n",
                "from statsmodels.tsa.stattools import acf                       # Compute autocorrelation values  \n",
                "\n",
                "from statsmodels.tsa.stattools import pacf                      # Compute partial autocorrelation values\n",
                "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf   # Plot ACF & PACF with confidence bands\n",
                "# STEP 16) PREDICTION MODELS\n",
                "from statsmodels.tsa.arima.model import ARIMA                   # ARIMA(p,d,q) model for forecasting\n",
                "from pmdarima import auto_arima                                 # Automatic ARIMA/SARIMA parameter selection\n",
                "from statsmodels.tools.sm_exceptions import ConvergenceWarning  # Warning raised when ARIMA optimizer fails\n",
                "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # Suppress convergence warnings globally (keeps logs clean)\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# TABULAR DATASETS - EDA\n",
                "# ---------------------------------------------------------\n",
                "# STEP 10) REMOVE NOISY ATTRIBUTES\n",
                "from scipy.stats import chi2_contingency                # Chi-square test for categorical dependencies\n",
                "# STEP 12) SCALLING\n",
                "from sklearn.preprocessing import StandardScaler        # Standardization (mean=0, std=1)\n",
                "from sklearn.preprocessing import MinMaxScaler          # Min-max scaling to [0,1]\n",
                "# STEP 13) ENCODING\n",
                "from sklearn.preprocessing import LabelEncoder          # Encode categories into integer labels\n",
                "from sklearn.preprocessing import OneHotEncoder         # Encode categories into binary vectors"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## COMMON FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# Log printer\n",
                "# ===============================\n",
                "def log(message: str, level: int = 1, type: str = \"INFO\", custom_icon: str = None, bold: bool = False):\n",
                "    # Default icons according to message type\n",
                "    icons = {\n",
                "        \"INFO\": \"‚ÑπÔ∏è\",\n",
                "        \"FOUND\": \"üîç\",\n",
                "        \"SUCCESS\": \"‚úÖ\",\n",
                "        \"ERROR\": \"‚ùå\",\n",
                "        \"WARNING\": \"‚ö†Ô∏è\",\n",
                "    }\n",
                "    # Use custom icon if provided\n",
                "    if custom_icon:\n",
                "        icon = custom_icon\n",
                "    else:\n",
                "        icon = icons.get(type.upper(), \"‚ÑπÔ∏è\")\n",
                "    # Bold wrapper (ANSI)\n",
                "    if bold:\n",
                "        message = f\"\\033[1m{message}\\033[0m\"\n",
                "    # First level ‚Üí bullet\n",
                "    if level == 1:\n",
                "        prefix = \"‚Ä¢\"\n",
                "    # Second level ‚Üí indent + hyphen\n",
                "    elif level == 2:\n",
                "        prefix = \"   -\"\n",
                "    # Level 3 ‚Üí deeper indent + middle dot\n",
                "    elif level == 3:\n",
                "        prefix = \"      ¬∑\"\n",
                "    # Fallback\n",
                "    else:\n",
                "        prefix = \"-\"\n",
                "    # Final print\n",
                "    print(f\"{prefix} {icon} {message}\")\n",
                "\n",
                "# ===============================\n",
                "# Get revision number - Returns the next free integer revision based on existing files\n",
                "# ===============================\n",
                "def get_revision_number(base_path, base_name):\n",
                "    rev = 0\n",
                "    while True:\n",
                "        full_path = os.path.join(base_path, base_name + \"_\" + str(rev) + \".csv\")\n",
                "        if not os.path.exists(full_path):\n",
                "            return rev\n",
                "        rev += 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## NLP DATASETS EDA FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# Adaptive text preprocessing: for natural text, URLs, and mixed content\n",
                "# ===============================\n",
                "def preprocess_text(text,\n",
                "                    mode=\"auto\",                # \"auto\", \"text\", \"url\"\n",
                "                    lowercase_text=True,\n",
                "                    remove_urls=True,           # Only applies in text mode\n",
                "                    remove_emails=True,\n",
                "                    remove_html_tags=True,\n",
                "                    remove_non_letters=True,    # Only applies in text mode\n",
                "                    remove_single_char_tokens=True,\n",
                "                    reduce_whitespace=True,\n",
                "                    tokenize_output=True):\n",
                "    # Convert to string safely\n",
                "    if text is None:\n",
                "        return {\"output\": [], \"mode_used\": \"none\"}\n",
                "    text = str(text)\n",
                "    # Automatic URL detection\n",
                "    def looks_like_url(s):\n",
                "        if s.startswith(\"http://\") or s.startswith(\"https://\"):\n",
                "            return True\n",
                "        if re.search(r\"\\.[a-z]{2,4}(/|$)\", s):   # domain-like pattern\n",
                "            return True\n",
                "        return False\n",
                "    # Decide mode\n",
                "    if mode == \"auto\":\n",
                "        is_url = looks_like_url(text)\n",
                "    elif mode == \"url\":\n",
                "        is_url = True\n",
                "    else:\n",
                "        is_url = False\n",
                "    # URL MODE\n",
                "    if is_url:\n",
                "        mode_used = \"url\"\n",
                "        if lowercase_text:\n",
                "            text = text.lower()\n",
                "        # Split URL into meaningful tokens\n",
                "        text = re.sub(r'[:/\\.\\?\\=\\&\\-\\_#]+', ' ', text)\n",
                "        if remove_single_char_tokens:\n",
                "            text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
                "        if reduce_whitespace:\n",
                "            text = re.sub(r'\\s+', ' ', text)\n",
                "        text = text.strip()\n",
                "        if tokenize_output:\n",
                "            tokens = text.split()\n",
                "            clean_tokens = []\n",
                "            for tok in tokens:\n",
                "                if tok:\n",
                "                    clean_tokens.append(tok)\n",
                "            return {\"output\": clean_tokens, \"mode_used\": mode_used}\n",
                "        return {\"output\": text, \"mode_used\": mode_used}\n",
                "    # TEXT MODE\n",
                "    mode_used = \"text\"\n",
                "    if lowercase_text:\n",
                "        text = text.lower()\n",
                "    if remove_urls:\n",
                "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
                "    if remove_emails:\n",
                "        text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
                "    if remove_html_tags:\n",
                "        text = re.sub(r'<.*?>', ' ', text)\n",
                "    if remove_non_letters:\n",
                "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
                "    if remove_single_char_tokens:\n",
                "        text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
                "    if reduce_whitespace:\n",
                "        text = re.sub(r'\\s+', ' ', text)\n",
                "    text = text.strip()\n",
                "    if tokenize_output:\n",
                "        tokens = text.split()\n",
                "        return {\"output\": tokens, \"mode_used\": mode_used}\n",
                "    return {\"output\": text, \"mode_used\": mode_used}\n",
                "\n",
                "# ===============================\n",
                "# Convert NLTK POS tags to WordNet POS tags\n",
                "# ===============================\n",
                "def nltk_pos_to_wordnet_pos(nltk_pos):\n",
                "    if nltk_pos.startswith(\"J\"):\n",
                "        return wordnet.ADJ\n",
                "    if nltk_pos.startswith(\"V\"):\n",
                "        return wordnet.VERB\n",
                "    if nltk_pos.startswith(\"N\"):\n",
                "        return wordnet.NOUN\n",
                "    if nltk_pos.startswith(\"R\"):\n",
                "        return wordnet.ADV\n",
                "    return None\n",
                "\n",
                "# ===============================\n",
                "# Lemmatize tokens\n",
                "# ===============================\n",
                "def lemmatize_tokens(tokens,\n",
                "                     apply_stopword_removal=True,\n",
                "                     stopword_language=\"english\",\n",
                "                     case_sensitive_stopwords=False,\n",
                "                     apply_pos_tagging=False,\n",
                "                     exclude_digits=True,\n",
                "                     min_word_length=3):\n",
                "    # Return empty list if input is invalid (none or not a list)\n",
                "    if tokens is None or not isinstance(tokens, list):\n",
                "        return []\n",
                "    # Initialize lemmatizer\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    # Load stopwords\n",
                "    if apply_stopword_removal:\n",
                "        try:\n",
                "            stop_words = stopwords.words(stopword_language)\n",
                "        except:\n",
                "            stop_words = stopwords.words(\"english\")\n",
                "        if not case_sensitive_stopwords:\n",
                "            normalized_stopwords = []\n",
                "            for w in stop_words:\n",
                "                word_lower = w.lower()\n",
                "                normalized_stopwords.append(word_lower)\n",
                "            stop_words = normalized_stopwords\n",
                "    else:\n",
                "        stop_words = []\n",
                "    # If POS tagging enabled, get tags\n",
                "    if apply_pos_tagging:\n",
                "        pos_tagged = nltk.pos_tag(tokens)\n",
                "    else:\n",
                "        pos_tagged = []\n",
                "        for tok in tokens:\n",
                "            pos_tagged.append((tok, None)) \n",
                "    # Process each token\n",
                "    clean_tokens = []\n",
                "    for token, pos_tag in pos_tagged:\n",
                "        if token is None:\n",
                "            continue\n",
                "        tok = token.lower()\n",
                "        # Remove stopwords\n",
                "        if apply_stopword_removal and tok in stop_words:\n",
                "            continue\n",
                "        # Remove tokens made only of digits\n",
                "        if exclude_digits and tok.isdigit():\n",
                "            continue\n",
                "        # Apply lemmatization\n",
                "        if pos_tag is not None:\n",
                "            wn_pos = nltk_pos_to_wordnet_pos(pos_tag)\n",
                "            if wn_pos is not None:\n",
                "                tok = lemmatizer.lemmatize(tok, pos=wn_pos)\n",
                "            else:\n",
                "                tok = lemmatizer.lemmatize(tok)\n",
                "        else:\n",
                "            tok = lemmatizer.lemmatize(tok)\n",
                "        # Remove tokens that are too short after processing\n",
                "        if len(tok) < min_word_length:\n",
                "            continue\n",
                "        clean_tokens.append(tok)\n",
                "    return clean_tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## TIME-SERIES DATASETS EDA FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# Determines granularity given seconds\n",
                "# ===============================\n",
                "def determine_granularity(seconds: float):\n",
                "    if seconds < 1:\n",
                "        granularity = \"sub-second\"\n",
                "    elif seconds >= 1 and seconds < 60:\n",
                "        granularity = \"second\"\n",
                "    elif seconds >= 60 and seconds < 3600:\n",
                "        granularity = \"minute\"\n",
                "    elif seconds >= 3600 and seconds < 14400:\n",
                "        granularity = \"hour\"\n",
                "    elif seconds == 14400:\n",
                "        granularity = \"4-hour\"\n",
                "    elif seconds > 14400 and seconds < 86400:\n",
                "        granularity = \"multi-hour\"\n",
                "    elif seconds == 86400:\n",
                "        granularity = \"day\"\n",
                "    elif seconds > 86400 and seconds <= 86400 * 7:\n",
                "        granularity = \"multi-day\"\n",
                "    elif seconds == 86400 * 7:\n",
                "        granularity = \"week\"\n",
                "    elif seconds > 86400 * 7 and seconds < 86400 * 28:\n",
                "        granularity = \"weekly-to-monthly\"\n",
                "    elif seconds >= 86400 * 28 and seconds <= 86400 * 31:\n",
                "        granularity = \"month\"\n",
                "    elif seconds > 86400 * 31 and seconds <= 86400 * 92:\n",
                "        granularity = \"quarter\"\n",
                "    else:\n",
                "        granularity = \"year-or-more\"\n",
                "    return granularity\n",
                "\n",
                "# ===============================\n",
                "# Detects whether the seasonal decomposition model should be\n",
                "# 'additive' (constant amplitude) or \n",
                "# 'multiplicative'(amplitude grows with the trend)\n",
                "# ===============================\n",
                "def infer_seasonal_component_type(series: pd.Series, threshold: float) -> str:\n",
                "    # Compute amplitude and mean\n",
                "    amplitude = series.max() - series.min()\n",
                "    mean_val = series.mean()\n",
                "    # Edge case: zero-mean ‚Üí multiplicative impossible\n",
                "    if mean_val == 0:\n",
                "        return \"additive\"\n",
                "    # Threshold for deciding multiplicative\n",
                "    if amplitude / abs(mean_val) > threshold:\n",
                "        return \"multiplicative\"\n",
                "    else:\n",
                "        return \"additive\"\n",
                "\n",
                "# ===============================\n",
                "#  Detects seasonality period automatically using the first significant ACF peak\n",
                "# ===============================\n",
                "def infer_period_from_acf(series: pd.Series, max_lag_ratio: float = 0.1):\n",
                "    n = len(series)\n",
                "    max_lag = max(5, int(n * max_lag_ratio))\n",
                "    # Compute autocorrelation using FFT\n",
                "    autocorr = acf(series, nlags=max_lag, fft=True, missing=\"drop\")\n",
                "    # Ignore lag 0\n",
                "    autocorr[0] = 0\n",
                "    # Find the highest correlation peak\n",
                "    peak_lag = np.argmax(autocorr)\n",
                "    # If the peak is too low ‚Üí no seasonality\n",
                "    if autocorr[peak_lag] < 0.3:\n",
                "        return None\n",
                "    return peak_lag\n",
                "\n",
                "# ===============================\n",
                "#  Detects seasonality period automatically using Granularity (fallback when ACF fails or data is too noisy)\n",
                "# ===============================\n",
                "def infer_period_from_granularity(granularity: str):\n",
                "    if granularity == \"sub-second\":\n",
                "        return None\n",
                "    if granularity == \"second\":\n",
                "        return 60\n",
                "    if granularity == \"minute\":\n",
                "        return 60\n",
                "    if granularity == \"hour\":\n",
                "        return 24\n",
                "    if granularity == \"4-hour\":\n",
                "        return 6\n",
                "    if granularity == \"day\":\n",
                "        return 7  # Most common weekly cycle IF the series shows any seasonality\n",
                "    if granularity == \"multi-day\":\n",
                "        return 7\n",
                "    if granularity == \"week\":\n",
                "        return 52\n",
                "    if granularity == \"weekly-to-monthly\":\n",
                "        return 12\n",
                "    if granularity == \"month\":\n",
                "        return 12\n",
                "    if granularity == \"quarter\":\n",
                "        return 4\n",
                "    return None # Yearly or undefined ‚Üí no decomposition\n",
                "\n",
                "# ===============================\n",
                "# 6) Evaluates how strong the seasonality is using:\n",
                "# 1) Variance ratio: Var(seasonal) / Var(original)\n",
                "# 2) ACF at the seasonal period\n",
                "# ===============================\n",
                "def assess_seasonality_strength(original: pd.Series, seasonal: pd.Series, period: int, acf_threshold: float, var_ratio: float):\n",
                "    # Align indices and remove NaN values from the seasonal component\n",
                "    valid_mask = seasonal.notna()\n",
                "    original_valid = original[valid_mask]\n",
                "    seasonal_valid = seasonal[valid_mask]\n",
                "    # If there are not enough valid points ‚Üí cannot assess\n",
                "    if len(original_valid) < max(10, period * 2):\n",
                "        metrics = {\n",
                "            \"seasonal_var_ratio\": np.nan,\n",
                "            \"acf_at_period\": np.nan\n",
                "        }\n",
                "        return False, metrics\n",
                "    # 1) Variance ratio\n",
                "    total_var = np.var(original_valid)\n",
                "    seasonal_var = np.var(seasonal_valid)\n",
                "    if total_var == 0:\n",
                "        seasonal_var_ratio = 0.0\n",
                "    else:\n",
                "        seasonal_var_ratio = seasonal_var / total_var\n",
                "    # 2) ACF at seasonal period\n",
                "    acf_values = acf(\n",
                "        original_valid,\n",
                "        nlags = period,\n",
                "        fft = True,\n",
                "        missing = \"drop\"\n",
                "    )\n",
                "    acf_at_period = acf_values[period]\n",
                "    # 3) Decision rule\n",
                "    strong_seasonality = ((seasonal_var_ratio >= var_ratio) and (acf_at_period >= acf_threshold))\n",
                "    # 4) Metrics\n",
                "    metrics = {\n",
                "        \"seasonal_var_ratio\": seasonal_var_ratio,\n",
                "        \"acf_at_period\": acf_at_period\n",
                "    }\n",
                "    return strong_seasonality, metrics\n",
                "\n",
                "# ===============================\n",
                "# Performs Dickey-Fuller test to determine if a series is stacionary or not\n",
                "# ===============================\n",
                "def test_stationarity(series):\n",
                "    dftest = adfuller(series, autolag = \"AIC\")\n",
                "    dfoutput = pd.Series(dftest[0:4], index = [\"Test Statistic\", \"p-value\", \"#Lags Used\", \"Number of Observations Used\"])\n",
                "    for key,value in dftest[4].items():\n",
                "        dfoutput[\"Critical Value (%s)\"%key] = value\n",
                "    return dfoutput\n",
                "\n",
                "# ===============================\n",
                "# Recursively differences the time-series until Dickey-Fuller test accepts stationarity (p < alpha)\n",
                "# ===============================\n",
                "def make_stationary_recursive(series, alpha: float = 0.05, max_diff: int = 5):\n",
                "    current_series = series.copy()\n",
                "    diff_count = 0\n",
                "    while diff_count <= max_diff:\n",
                "        test_results = test_stationarity(current_series)\n",
                "        if test_results[\"p-value\"] < alpha:\n",
                "            return current_series, diff_count, test_results\n",
                "        current_series = current_series.diff().dropna()\n",
                "        diff_count += 1\n",
                "    # If exceeded max_diff ‚Üí return last attempt\n",
                "    return current_series, diff_count, test_results\n",
                "\n",
                "# ===============================\n",
                "# Function to get recommended lag based on granularity\n",
                "# ===============================\n",
                "def get_recommended_lag(granularity):\n",
                "    if granularity == \"sub-second\" or granularity == \"second\":\n",
                "        return 600   # Captures 10 minutes of autocorrelation\n",
                "    if granularity == \"minute\":\n",
                "        return 300   # Captures 5 hours\n",
                "    if granularity == \"hour\":\n",
                "        return 200   # Captures up to weekly cycles (168h)\n",
                "    if granularity == \"4-hour\":\n",
                "        return 200   # Captures up to weekly cycles (42*4h = 168h)\n",
                "    if granularity == \"day\":\n",
                "        return 60    # Enough for weekly + monthly seasonality\n",
                "    if granularity == \"multi-day\":\n",
                "        return 60\n",
                "    if granularity == \"week\":\n",
                "        return 60    # Enough to detect annual cycle (52 weeks)\n",
                "    if granularity == \"month\":\n",
                "        return 48    # Captures 4 years of monthly pattern\n",
                "    if granularity == \"quarter\":\n",
                "        return 20\n",
                "    return 10        # Yearly or undefined ‚Üí very small possible lags\n",
                "\n",
                "# ===============================\n",
                "# Function to get recommended cutoff for short-lag autocorrelation detection based on granularity\n",
                "# ===============================\n",
                "def get_short_lag_cutoff(granularity):\n",
                "    if granularity == \"sub-second\" or granularity == \"second\":\n",
                "        return 30   # 30 seconds of persistence\n",
                "    if granularity == \"minute\":\n",
                "        return 60   # 1 hour of short-term memory\n",
                "    if granularity == \"hour\":\n",
                "        return 24   # One day's worth of lags\n",
                "    if granularity == \"4-hour\":\n",
                "        return 42   # One week's worth of lags (42*4h = 168h)\n",
                "    if granularity == \"day\":\n",
                "        return 14   # Two weeks ‚Üí enough to detect trend\n",
                "    if granularity == \"multi-day\":\n",
                "        return 10\n",
                "    if granularity == \"week\":\n",
                "        return 8    # ~2 months of weekly persistence\n",
                "    if granularity == \"month\":\n",
                "        return 12   # 1 year of autocorrelation\n",
                "    if granularity == \"quarter\":\n",
                "        return 8\n",
                "    return 3        # Very limited short-term interpretation\n",
                "\n",
                "# ===============================\n",
                "# Return a pandas-compatible frequency string based on granularity\n",
                "# ===============================\n",
                "def get_freq_from_granularity(granularity: str):\n",
                "    freq_map = {\n",
                "        \"sub-second\": None,             # No valid pandas freq for < 1 second\n",
                "        \"second\": \"S\",                  # Second-level frequency\n",
                "        \"minute\": \"T\",                  # Minute-level frequency\n",
                "        \"hour\": \"H\",                    # Hourly frequency\n",
                "        \"4-hour\": \"4H\",                 # 4-hour frequency\n",
                "        \"day\": \"D\",                     # Daily frequency\n",
                "        \"multi-day\": \"D\",               # Best stable approximation\n",
                "        \"week\": \"W\",                    # Weekly frequency\n",
                "        \"weekly-to-monthly\": \"W\",       # Ambiguous ‚Üí weekly fits more stable\n",
                "        \"month\": \"M\",                   # Monthly frequency\n",
                "        \"quarter\": \"Q\",                 # Quarterly frequency\n",
                "        \"year-or-more\": \"A\"             # Annual frequency\n",
                "    }\n",
                "    if granularity not in freq_map:\n",
                "        return None\n",
                "    return freq_map[granularity]\n",
                "\n",
                "# ===============================\n",
                "# Get seasonal period m for auto_arima\n",
                "# ===============================\n",
                "def get_auto_arima_m(period: int, seasonal_peaks: list):\n",
                "    if period is None:\n",
                "        return 1\n",
                "    if seasonal_peaks is None:\n",
                "        return 1\n",
                "    if len(seasonal_peaks) == 0:\n",
                "        return 1\n",
                "    if period >= 2:\n",
                "        return int(period)\n",
                "    return 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## TABULAR DATASETS EDA FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "#  Computes correlation between CATEGORIC attributes by using Cram√©r's V\n",
                "# ===============================\n",
                "def cramers_v(x, y): \n",
                "    # Step 1: confusion matrix\n",
                "    confusion_matrix = pd.crosstab(x, y)\n",
                "    # Step 2: chi-square statistic\n",
                "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
                "    # Step 3: phi-squared\n",
                "    total_samples = confusion_matrix.sum().sum()\n",
                "    phi2 = chi2 / total_samples\n",
                "    # Shape of confusion matrix\n",
                "    r, k = confusion_matrix.shape\n",
                "    num_rows = confusion_matrix.shape[0]\n",
                "    num_cols = confusion_matrix.shape[1]\n",
                "    # Step 4: bias correction (recommended formula)\n",
                "    correction = ((num_cols - 1) * (num_rows - 1)) / (total_samples - 1)\n",
                "    phi2_corrected = max(0, phi2 - correction)\n",
                "    # Corrected dimensions\n",
                "    rows_corrected = num_rows - ((num_rows - 1) ** 2) / (total_samples - 1)\n",
                "    cols_corrected = num_cols - ((num_cols - 1) ** 2) / (total_samples - 1)\n",
                "    # Step 5: compute Cram√©r's V\n",
                "    denominator = min(rows_corrected - 1, cols_corrected - 1)\n",
                "    if denominator <= 0:\n",
                "        return 0  # avoid division by zero for degenerate tables\n",
                "    cramers_v_value = np.sqrt(phi2_corrected / denominator)\n",
                "    return cramers_v_value"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SUPERVISED MODELS FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# Determines de number of different parameters to include in the grid\n",
                "# ===============================\n",
                "def n_grid_param(n_samples, n_features,\n",
                "                 n_features_BIG_dataset ,n_features_MEDIUM_dataset, n_features_SMALL_dataset,\n",
                "                 n_samples_BIG_dataset, n_samples_MEDIUM_dataset, n_samples_SMALL_dataset, n_samples_MICRO_dataset):\n",
                "    # 1) ULTRA FAST ‚Üí Big datasets (2 values per parameters)\n",
                "    if (n_samples >= n_samples_MEDIUM_dataset and n_features >= n_features_SMALL_dataset) or \\\n",
                "       (n_samples >= n_samples_BIG_dataset) or \\\n",
                "       (n_features >= n_features_BIG_dataset):\n",
                "        log(\"Dataset size: BIG -> Grid's parameters will be filled with 2 values\", type=\"WARNING\")\n",
                "        return 2\n",
                "    # 2) FAST ‚Üí Medium datasets (3 values per parameters)\n",
                "    if (n_samples_SMALL_dataset <= n_samples < n_samples_MEDIUM_dataset) or \\\n",
                "       (n_features_MEDIUM_dataset <= n_features < n_features_BIG_dataset):\n",
                "        log(\"Dataset size: MEDIUM -> Grid's parameters will be filled with 3 values\", type=\"WARNING\")\n",
                "        return 3\n",
                "    # 3) NORMAL ‚Üí Small datasets (4 values per parameters)\n",
                "    if (n_samples_MICRO_dataset <= n_samples < n_samples_SMALL_dataset) or \\\n",
                "       (n_features_SMALL_dataset <= n_features < n_features_MEDIUM_dataset):\n",
                "        log(\"Dataset size: SMALL -> Grid's parameters will be filled with 4 values\", type=\"WARNING\")\n",
                "        return 4\n",
                "    # 4) SLOW ‚Üí Micro datasets (5 values per parameters)\n",
                "    log(\"Dataset size: MICRO -> Grid's parameters will be filled with 5 values\", type=\"WARNING\")\n",
                "    return 5\n",
                "\n",
                "# Generates k exponentially (log-spaced) values between min_val and max_val.\n",
                "def smart_logspace_choose(min_val, max_val, datatype, num_grid_param):\n",
                "    # Convert to log10 space\n",
                "    log_min = np.log10(min_val)\n",
                "    log_max = np.log10(max_val)\n",
                "    # Generate k values evenly spaced in LOG space (exponential sequence)\n",
                "    raw_values  = np.logspace(log_min, log_max, num_grid_param)\n",
                "    # Round them for readability and return\n",
                "    rounded_values = []\n",
                "    for v in raw_values:\n",
                "        if datatype == \"integer\":\n",
                "            rounded_values.append(round(v))\n",
                "        elif datatype == \"float\":\n",
                "            rounded_values.append(round(v, 6))\n",
                "    return rounded_values\n",
                "\n",
                "# ===============================\n",
                "# Maps the external GridSearchCV scoring name to a coherent XGBoost eval_metric\n",
                "# ===============================\n",
                "def map_eval_metric(scoring):\n",
                "    mapping = {\n",
                "        # Classification\n",
                "        \"accuracy\": \"merror\",\n",
                "        \"precision\": \"logloss\",\n",
                "        \"recall\": \"logloss\",\n",
                "        \"f1\": \"logloss\",\n",
                "        # Regression\n",
                "        \"r2\": \"rmse\",\n",
                "        \"neg_root_mean_squared_error\": \"rmse\"\n",
                "    }\n",
                "    return mapping.get(scoring, \"logloss\")\n",
                "\n",
                "# ===============================\n",
                "def map_xgb_eval_metric(model_name, scoring):\n",
                "    # Classification model\n",
                "    if model_name == \"XGBClassifier\":\n",
                "        return {\n",
                "            \"accuracy\": \"merror\",\n",
                "            \"precision\": \"logloss\",\n",
                "            \"recall\": \"logloss\",\n",
                "            \"f1\": \"logloss\"\n",
                "        }.get(scoring, \"logloss\")\n",
                "    # Regression model\n",
                "    if model_name == \"XGBRegressor\":\n",
                "        return {\n",
                "            \"r2\": \"rmse\",\n",
                "            \"neg_root_mean_squared_error\": \"rmse\",\n",
                "            \"neg_mean_squared_error\": \"rmse\",\n",
                "            \"neg_mean_absolute_error\": \"mae\"\n",
                "        }.get(scoring, \"rmse\")\n",
                "\n",
                "# ===============================\n",
                "def compute_classification_metrics(y_true, y_pred, avg, pos_label):\n",
                "    metrics = {}\n",
                "    metrics[\"Accuracy\"]  = accuracy_score(y_true, y_pred)\n",
                "    metrics[\"Precision\"] = precision_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
                "    metrics[\"Recall\"]    = recall_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
                "    metrics[\"F1_score\"]  = f1_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
                "    return metrics\n",
                "\n",
                "# ===============================\n",
                "def compute_regression_metrics(y_true, y_pred):\n",
                "    metrics = {}\n",
                "    metrics[\"MAE\"]  = mean_absolute_error(y_true, y_pred)\n",
                "    metrics[\"MSE\"]  = mean_squared_error(y_true, y_pred)\n",
                "    metrics[\"RMSE\"] = np.sqrt(metrics[\"MSE\"])\n",
                "    metrics[\"R2\"]   = r2_score(y_true, y_pred)\n",
                "    return metrics\n",
                "\n",
                "# ===============================\n",
                "def set_average_proposal(y, scoring_target):\n",
                "    unique_count = y.nunique()\n",
                "    if unique_count == 2:\n",
                "        freq = y.value_counts()\n",
                "        pos_label = freq.index[-1]\n",
                "        return \"binary\", pos_label, scoring_target\n",
                "    freq_norm = y.value_counts(normalize=True)\n",
                "    imbalance_ratio = freq_norm.max() / freq_norm.min()\n",
                "    if imbalance_ratio <= 1.2:\n",
                "        return \"micro\", None, scoring_target + \"_micro\" if scoring_target != \"accuracy\" else scoring_target\n",
                "    if imbalance_ratio <= 1.5:\n",
                "        return \"macro\", None, scoring_target + \"_macro\" if scoring_target != \"accuracy\" else scoring_target\n",
                "    return \"weighted\", None, scoring_target + \"_weighted\" if scoring_target != \"accuracy\" else scoring_target\n",
                "\n",
                "# ===============================\n",
                "def set_time_per_fit_factor(n_samples, n_features, n_features_BIG_dataset, n_features_MEDIUM_dataset, n_features_SMALL_dataset, n_samples_BIG_dataset, n_samples_MEDIUM_dataset, n_samples_SMALL_dataset):\n",
                "    # BIG dataset\n",
                "    if (n_samples >= n_samples_BIG_dataset) or (n_features >= n_features_BIG_dataset):\n",
                "        return 2.5\n",
                "    # MEDIUM dataset\n",
                "    elif (n_samples_MEDIUM_dataset <= n_samples < n_samples_BIG_dataset) or (n_features_MEDIUM_dataset <= n_features < n_features_BIG_dataset):\n",
                "        return 1.5\n",
                "    # SMALL dataset\n",
                "    elif (n_samples_SMALL_dataset <= n_samples < n_samples_MEDIUM_dataset) or (n_features_SMALL_dataset <= n_features < n_features_MEDIUM_dataset):\n",
                "        return 1.0\n",
                "    # MICRO dataset\n",
                "    elif n_samples < n_samples_SMALL_dataset:\n",
                "        return 0.8\n",
                "    else:\n",
                "        return 1.0\n",
                "    \n",
                "# ===============================\n",
                "# Computes the total number of model trainings required by GridSearchCV\n",
                "# ===============================\n",
                "def count_grid_combinations(grid_params, cv):\n",
                "    total = 1\n",
                "    for key, values in grid_params.items():\n",
                "        total *= len(values)\n",
                "    return total * cv\n",
                "\n",
                "# ===============================\n",
                "# Lasso / Ridge hyperparameter plotter\n",
                "# ===============================\n",
                "def plotter_Lasso_Ridge_hparame(model_name, grid_params, grid_scores, ax, label_fontsize, title_fontsize, reg_scoring_target):\n",
                "    ax.plot(grid_params, grid_scores, marker=\"o\")\n",
                "    ax.set_xlabel(\"Alpha\", fontsize=label_fontsize)\n",
                "    ax.set_ylabel(reg_scoring_target, fontsize=label_fontsize)\n",
                "    ax.set_title(f\"{model_name} - Alpha vs Scoring ({reg_scoring_target})\", fontsize=title_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "\n",
                "# ===============================\n",
                "# Decision Tree plotter\n",
                "# ===============================\n",
                "def plotter_DecisionTree_model(model, X, ax):\n",
                "    plot_tree(model, feature_names=X.columns, filled=True, ax=ax)\n",
                "\n",
                "# ===============================\n",
                "# Generic RandomForest hyperparameter plotter (Classifier + Regressor)\n",
                "# ===============================\n",
                "def plotter_RandomForest_hparam(model_name, est_list, depth_list, depth_score_dict, ax, label_fontsize, title_fontsize, reg_scoring_target, cla_scoring_target):\n",
                "    # Choose scoring label depending on model type\n",
                "    if model_name == \"RandomForestClassifier\":\n",
                "        scoring_label = cla_scoring_target\n",
                "    else:\n",
                "        scoring_label = reg_scoring_target\n",
                "    # Create a color palette (one color per n_estimators)\n",
                "    palette_colors = sns.color_palette(\"bright\", len(est_list))\n",
                "    # One curve per n_estimators\n",
                "    for i, est in enumerate(est_list):\n",
                "        # Retrieve scores for this n_estimators across all depths\n",
                "        est_scores = []\n",
                "        for d in depth_list:\n",
                "            est_scores.append(depth_score_dict[d][i])\n",
                "        # Plot curve: X = depth_list, Y = scores\n",
                "        ax.plot(depth_list, est_scores, marker=\"o\", color=palette_colors[i], label=f\"No. estimators = {int(est)}\")\n",
                "    # Labels and formatting\n",
                "    ax.set_xlabel(\"Max depth\", fontsize=label_fontsize)\n",
                "    ax.set_ylabel(scoring_label, fontsize=label_fontsize)\n",
                "    ax.set_title(f\"{model_name} - Hyperparameter Analysis\", fontsize=title_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    ax.legend()\n",
                "\n",
                "# ===============================\n",
                "# KNN hyperparameter plotter: k (n_neighbors) vs score\n",
                "# ===============================\n",
                "def plotter_KNN_hparam_k(model_name, k_list, train_scores,test_scores, ax, label_fontsize, title_fontsize, reg_scoring_target, cla_scoring_target):\n",
                "    # Choose scoring label depending on model type\n",
                "    if \"Classifier\" in model_name:\n",
                "        scoring_label = cla_scoring_target\n",
                "    else:\n",
                "        scoring_label = reg_scoring_target\n",
                "    # Defensive checks\n",
                "    if k_list is None or len(k_list) == 0:\n",
                "        return\n",
                "    if train_scores is None or test_scores is None:\n",
                "        return\n",
                "    # Plot\n",
                "    ax.plot(k_list, train_scores, marker=\"o\", label=\"Train score\")\n",
                "    ax.plot(k_list, test_scores, marker=\"o\", label=\"Test score\")\n",
                "\n",
                "    ax.set_xlabel(\"Number of neighbors (k)\", fontsize=label_fontsize)\n",
                "    ax.set_ylabel(scoring_label, fontsize=label_fontsize)\n",
                "    ax.set_title(f\"{model_name} ‚Äî Train vs Test scores\", fontsize=title_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    ax.legend()\n",
                "\n",
                "# ===============================\n",
                "# Generic Boosting hyperparameter plotter\n",
                "# ===============================\n",
                "def plotter_Boosting_hparam(model_name, est_list, depth_list, depth_score_dict, ax, label_fontsize, title_fontsize, reg_scoring_target, cla_scoring_target):\n",
                "    # Choose scoring label depending on model type\n",
                "    if \"Classifier\" in model_name:\n",
                "        scoring_label = cla_scoring_target\n",
                "    else:\n",
                "        scoring_label = reg_scoring_target\n",
                "    # Create a color palette (one color per n_estimators)\n",
                "    palette_colors = sns.color_palette(\"bright\", len(est_list))\n",
                "    # One curve per n_estimators\n",
                "    for i, est in enumerate(est_list):\n",
                "        # Retrieve scores for this n_estimators across all depths\n",
                "        est_scores = []\n",
                "        for d in depth_list:\n",
                "            est_scores.append(depth_score_dict[d][i])\n",
                "        ax.plot(depth_list, est_scores, marker=\"o\", color=palette_colors[i], label=f\"No. estimators = {int(est)}\")\n",
                "    # Labels and formatting\n",
                "    ax.set_xlabel(\"Max depth\", fontsize=label_fontsize)\n",
                "    ax.set_ylabel(scoring_label, fontsize=label_fontsize)\n",
                "    ax.set_title(f\"{model_name} - Hyperparameter Analysis\", fontsize=title_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    ax.legend()\n",
                "\n",
                "# ===============================\n",
                "# Linear Model Coefficient Plotter (LogReg, LinReg, LinearSVC)\n",
                "# ===============================\n",
                "def plotter_linear_coefficients(model, X, ax, model_name, label_fontsize, title_fontsize, top_n=20):\n",
                "    # Extract coefficients\n",
                "    try:\n",
                "        coef = model.coef_.flatten()\n",
                "    except:\n",
                "        coef = model.coef_\n",
                "    # Defensive check: mismatch between model and dataset\n",
                "    if len(coef) != X.shape[1]:\n",
                "        ax.text(\n",
                "            0.5, 0.5,\n",
                "            \"‚ö†Ô∏è Cannot plot coefficients:\\nModel coef_ size does NOT match X columns\",\n",
                "            ha=\"center\", va=\"center\",\n",
                "            fontsize=label_fontsize,\n",
                "            bbox=dict(boxstyle=\"round\", fc=\"lightcoral\", alpha=0.4)\n",
                "        )\n",
                "        ax.set_title(f\"{model_name} ‚Äî Coefficients NOT plottable\", fontsize=title_fontsize)\n",
                "        ax.axis(\"off\")\n",
                "        return\n",
                "    # Defensive cap\n",
                "    n_features = len(coef)\n",
                "    top_n_safe = min(top_n, n_features)\n",
                "    # Compute ranking\n",
                "    abs_coef = np.abs(coef)\n",
                "    top_indices = np.argsort(abs_coef)[-top_n_safe:]\n",
                "    # Selected coefficients\n",
                "    top_features = X.columns[top_indices]\n",
                "    top_values   = coef[top_indices]\n",
                "    # Sort for plotting clarity\n",
                "    sort_idx     = np.argsort(top_values)\n",
                "    top_features = top_features[sort_idx]\n",
                "    top_values   = top_values[sort_idx]\n",
                "    # Plot\n",
                "    ax.barh(top_features, top_values)\n",
                "    ax.set_xlabel(\"Coefficient value\", fontsize=label_fontsize)\n",
                "    ax.set_title(f\"{model_name} ‚Äî Top {top_n_safe} coefficients\", fontsize=title_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## UNSUPERVISED MODELS FUNCTIONS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -------------------------------\n",
                "# Resolve metric for model\n",
                "# -------------------------------\n",
                "def resolve_distance_metric(dataset_type, model_name=None, linkage=None):\n",
                "    if dataset_type == \"NLP\":\n",
                "        base_metric = \"cosine\"\n",
                "    else:\n",
                "        base_metric = \"euclidean\"\n",
                "    if model_name == \"AgglomerativeClustering\":\n",
                "        if linkage == \"ward\":\n",
                "            return \"euclidean\"\n",
                "        return base_metric\n",
                "    return base_metric\n",
                "\n",
                "# ===============================\n",
                "# Builds an elbow-friendly x_list (~n_points) based on grid values,\n",
                "# reusing smart_logspace_choose for consistency with optimization grids\n",
                "# ===============================\n",
                "def build_elbow_x_list(grid_values, datatype, n_points=10):\n",
                "    # Clean grid values (remove None, duplicates, sort)\n",
                "    clean_values = []\n",
                "    for v in grid_values:\n",
                "        if v is not None:\n",
                "            clean_values.append(v)\n",
                "    clean_values = sorted(list(dict.fromkeys(clean_values)))\n",
                "    # If grid already dense enough, reuse it\n",
                "    if len(clean_values) >= n_points:\n",
                "        return clean_values\n",
                "    # If grid too small, densify within its bounds\n",
                "    min_val = float(clean_values[0])\n",
                "    max_val = float(clean_values[-1])\n",
                "    # Guard against degenerate ranges\n",
                "    if min_val == max_val:\n",
                "        if datatype == \"integer\":\n",
                "            min_val = max(2, int(min_val) - 2)\n",
                "            max_val = int(max_val) + 2\n",
                "        else:\n",
                "            min_val = max(1e-6, min_val * 0.5)\n",
                "            max_val = max_val * 1.5\n",
                "    # Generate elbow points using same philosophy as grids\n",
                "    x_list = smart_logspace_choose(min_val=min_val, max_val=max_val, datatype=datatype, num_grid_param=n_points)\n",
                "    # Final clean (remove duplicates, sort)\n",
                "    x_list = sorted(list(dict.fromkeys(x_list)))\n",
                "    return x_list\n",
                "\n",
                "# ===============================\n",
                "# Inertia-like metric (WCSS): sum of squared distances to cluster centroids\n",
                "# Noise points (label = -1) are ignored\n",
                "# ===============================\n",
                "def compute_inertia_from_labels(X, labels):\n",
                "    unique_labels = []\n",
                "    for lab in np.unique(labels):\n",
                "        if lab != -1:\n",
                "            unique_labels.append(lab)\n",
                "    if len(unique_labels) == 0:\n",
                "        return np.nan\n",
                "    inertia_value = 0.0\n",
                "    for lab in unique_labels:\n",
                "        mask = (labels == lab)\n",
                "        X_cluster = X[mask]\n",
                "        if X_cluster.shape[0] == 0:\n",
                "            continue\n",
                "        centroid = np.mean(X_cluster, axis=0)\n",
                "        diffs = X_cluster - centroid\n",
                "        # Ensure scalar inertia (handles DataFrame / ndarray)\n",
                "        sq_dist = np.sum(diffs.values ** 2) if hasattr(diffs, \"values\") else np.sum(diffs ** 2)\n",
                "        inertia_value = inertia_value + float(sq_dist)\n",
                "    return inertia_value\n",
                "\n",
                "# ===============================\n",
                "# Computes clustering diagnostics from labels (model-agnostic).\n",
                "# ===============================\n",
                "def compute_clustering_diagnostics(X, labels):\n",
                "    # Inertia-like (WCSS proxy) computed from labels\n",
                "    inertia_like = compute_inertia_from_labels(X, labels)\n",
                "    # Number of clusters found (exclude noise = -1)\n",
                "    unique_clusters = []\n",
                "    for c in np.unique(labels):\n",
                "        if c != -1:\n",
                "            unique_clusters.append(c)\n",
                "    n_clusters_found = len(unique_clusters)\n",
                "    noise_ratio = 0.0\n",
                "    # Noise ratio (only meaningful if model can output -1)\n",
                "    if (-1 in np.unique(labels)):\n",
                "        noise_ratio = float(np.sum(labels == -1) / len(labels))\n",
                "    # Silhouette (valid only if >= 2 clusters excluding noise)\n",
                "    sil = None\n",
                "    try:\n",
                "        if n_clusters_found >= 2:\n",
                "            sil = float(silhouette_score(X, labels))\n",
                "    except Exception:\n",
                "        sil = None\n",
                "    return {\n",
                "        \"inertia_like\": inertia_like,\n",
                "        \"silhouette\": sil,\n",
                "        \"n_clusters_found\": n_clusters_found,\n",
                "        \"noise_ratio\": noise_ratio\n",
                "    }\n",
                "\n",
                "# ===============================\n",
                "# Returns index of elbow (max distance to line) (0-based), and elbow x value\n",
                "# ===============================\n",
                "def find_elbow_point(x_list, y_list):\n",
                "    x = np.array(x_list, dtype=float)\n",
                "    y = np.array(y_list, dtype=float)\n",
                "    valid_mask = ~np.isnan(y)\n",
                "    x = x[valid_mask]\n",
                "    y = y[valid_mask]\n",
                "    if len(x) < 3:\n",
                "        return None, None\n",
                "    x1, y1 = x[0], y[0]\n",
                "    x2, y2 = x[-1], y[-1]\n",
                "    # Line vector\n",
                "    line_vec = np.array([x2 - x1, y2 - y1], dtype=float)\n",
                "    line_len = np.sqrt(line_vec[0]**2 + line_vec[1]**2)\n",
                "    if line_len == 0:\n",
                "        return None, None\n",
                "    # Compute distance from each point to the line\n",
                "    distances = []\n",
                "    for i in range(len(x)):\n",
                "        px, py = x[i], y[i]\n",
                "        point_vec = np.array([px - x1, py - y1], dtype=float)\n",
                "        # Area of parallelogram / base = distance\n",
                "        cross = abs(line_vec[0]*point_vec[1] - line_vec[1]*point_vec[0])\n",
                "        dist = cross / line_len\n",
                "        distances.append(dist)\n",
                "    elbow_idx = int(np.argmax(distances))\n",
                "    return elbow_idx, float(x[elbow_idx])\n",
                "\n",
                "# ===============================\n",
                "# Computes elbow diagnostic curves for KMeans, DBSCAN and AgglomerativeClustering.\n",
                "# - KMeans: one curve per n_init\n",
                "# - DBSCAN: one curve per min_samples\n",
                "# - Agglomerative: one curve per linkage\n",
                "# ===============================\n",
                "def compute_elbow_diagnostics_curves(model_name, X_train_unsup, clustering_grids, dataset_type, random_seed, n_points_elbow=10):\n",
                "    results = {\n",
                "        \"model_name\": model_name,\n",
                "        \"x_label\": None,\n",
                "        \"x_list\": None,\n",
                "        \"curves\": {}\n",
                "    }\n",
                "    # KMEANS ‚Üí x = n_clusters, curve = n_init\n",
                "    if model_name == \"KMeans\":\n",
                "        results[\"x_label\"] = \"n_clusters\"\n",
                "        x_grid = clustering_grids[model_name][\"n_clusters\"]\n",
                "        x_list = build_elbow_x_list(grid_values=x_grid, datatype=\"integer\", n_points=n_points_elbow)\n",
                "        results[\"x_list\"] = x_list\n",
                "        n_init_list = clustering_grids[model_name][\"n_init\"]\n",
                "        for n_init in n_init_list:\n",
                "            inertia_list = []\n",
                "            sil_list = []\n",
                "            ncl_list = []\n",
                "            noise_list = []\n",
                "            for k in x_list:\n",
                "                tmp_model = KMeans(n_clusters=int(k), n_init=int(n_init), random_state=random_seed)\n",
                "                tmp_model.fit(X_train_unsup)\n",
                "                labels = tmp_model.labels_\n",
                "                diag = compute_clustering_diagnostics(X_train_unsup, labels)\n",
                "                inertia_list.append(diag[\"inertia_like\"])\n",
                "                sil_list.append(diag[\"silhouette\"])\n",
                "                ncl_list.append(diag[\"n_clusters_found\"])\n",
                "                noise_list.append(diag[\"noise_ratio\"])\n",
                "            elbow_idx, elbow_x = find_elbow_point(x_list, inertia_list)\n",
                "            results[\"curves\"][int(n_init)] = {\n",
                "                \"label\": f\"n_init={int(n_init)}\",\n",
                "                \"inertia_like\": inertia_list,\n",
                "                \"silhouette\": sil_list,\n",
                "                \"n_clusters_found\": ncl_list,\n",
                "                \"noise_ratio\": noise_list,\n",
                "                \"elbow_x\": elbow_x\n",
                "            }\n",
                "        return results\n",
                "    # DBSCAN ‚Üí x = eps, curve = min_samples\n",
                "    if model_name == \"DBSCAN\":\n",
                "        results[\"x_label\"] = \"eps\"\n",
                "        x_grid = clustering_grids[model_name][\"eps\"]\n",
                "        x_list = build_elbow_x_list(grid_values=x_grid, datatype=\"float\", n_points=n_points_elbow)\n",
                "        results[\"x_list\"] = x_list\n",
                "        min_samples_list = clustering_grids[model_name][\"min_samples\"]\n",
                "        for ms in min_samples_list:\n",
                "            inertia_list = []\n",
                "            sil_list = []\n",
                "            ncl_list = []\n",
                "            noise_list = []\n",
                "            for eps in x_list:\n",
                "                tmp_model = DBSCAN(eps=float(eps), min_samples=int(ms))\n",
                "                labels = tmp_model.fit_predict(X_train_unsup)\n",
                "                diag = compute_clustering_diagnostics(X_train_unsup, labels)\n",
                "                inertia_list.append(diag[\"inertia_like\"])\n",
                "                sil_list.append(diag[\"silhouette\"])\n",
                "                ncl_list.append(diag[\"n_clusters_found\"])\n",
                "                noise_list.append(diag[\"noise_ratio\"])\n",
                "            elbow_idx, elbow_x = find_elbow_point(x_list, inertia_list)\n",
                "            results[\"curves\"][int(ms)] = {\n",
                "                \"label\": f\"min_samples={int(ms)}\",\n",
                "                \"inertia_like\": inertia_list,\n",
                "                \"silhouette\": sil_list,\n",
                "                \"n_clusters_found\": ncl_list,\n",
                "                \"noise_ratio\": noise_list,\n",
                "                \"elbow_x\": elbow_x\n",
                "            }\n",
                "        return results\n",
                "    # AGGLOMERATIVE ‚Üí x = n_clusters, curve = linkage\n",
                "    if model_name == \"AgglomerativeClustering\":\n",
                "        results[\"x_label\"] = \"n_clusters\"\n",
                "        x_grid = clustering_grids[model_name][\"n_clusters\"]\n",
                "        x_list = build_elbow_x_list(grid_values=x_grid, datatype=\"integer\", n_points=n_points_elbow)\n",
                "        results[\"x_list\"] = x_list\n",
                "        linkage_list = clustering_grids[model_name][\"linkage\"]\n",
                "        for link in linkage_list:\n",
                "            inertia_list = []\n",
                "            sil_list = []\n",
                "            ncl_list = []\n",
                "            noise_list = []\n",
                "            # Metric rules: ward requires euclidean\n",
                "            metric_to_use = resolve_distance_metric(dataset_type=dataset_type, model_name=\"AgglomerativeClustering\", linkage=link)\n",
                "            for k in x_list:\n",
                "                tmp_model = AgglomerativeClustering(n_clusters=int(k), linkage=link, metric=metric_to_use)\n",
                "                labels = tmp_model.fit_predict(X_train_unsup)\n",
                "                diag = compute_clustering_diagnostics(X_train_unsup, labels)\n",
                "                inertia_list.append(diag[\"inertia_like\"])\n",
                "                sil_list.append(diag[\"silhouette\"])\n",
                "                ncl_list.append(diag[\"n_clusters_found\"])\n",
                "                noise_list.append(diag[\"noise_ratio\"])\n",
                "            elbow_idx, elbow_x = find_elbow_point(x_list, inertia_list)\n",
                "            results[\"curves\"][str(link)] = {\n",
                "                \"label\": f\"linkage={link}\",\n",
                "                \"inertia_like\": inertia_list,\n",
                "                \"silhouette\": sil_list,\n",
                "                \"n_clusters_found\": ncl_list,\n",
                "                \"noise_ratio\": noise_list,\n",
                "                \"elbow_x\": elbow_x,\n",
                "                \"metric\": metric_to_use\n",
                "            }\n",
                "        return results\n",
                "    raise ValueError(f\"Unsupported model_name for elbow diagnostics: {model_name}\")\n",
                "\n",
                "# ===============================\n",
                "# Selects the elbow_x corresponding to the curve with the best silhouette.\n",
                "# Strategy: maximize mean silhouette across the curve.\n",
                "# ===============================\n",
                "def select_best_elbow_by_silhouette(elbow_results):\n",
                "    best_score = None\n",
                "    best_elbow = None\n",
                "    for curve_key, curve in elbow_results[\"curves\"].items():\n",
                "        sil_values = curve.get(\"silhouette\", [])\n",
                "        # Keep only valid silhouette values\n",
                "        valid_sil = []\n",
                "        for s in sil_values:\n",
                "            if s is not None:\n",
                "                valid_sil.append(s)\n",
                "        if len(valid_sil) == 0:\n",
                "            continue\n",
                "        mean_sil = float(np.mean(valid_sil))\n",
                "        if (best_score is None) or (mean_sil > best_score):\n",
                "            best_score = mean_sil\n",
                "            best_elbow = curve.get(\"elbow_x\", None)\n",
                "    return best_elbow\n",
                "\n",
                "# ===============================\n",
                "# Refines clustering grids around an elbow suggestion (model-specific)\n",
                "# ===============================\n",
                "def refine_clustering_grid_around_elbow(model_name, clustering_grids, elbow_x, dataset_type, num_grid_param):\n",
                "    # Flag to indicate if refinement happened\n",
                "    refined = False\n",
                "    # Security check if no elbow detected\n",
                "    if elbow_x is None:\n",
                "        log(f\"[GRID REFINE] {model_name}: elbow_x is None ‚Üí grid NOT refined\", type=\"WARNING\")\n",
                "        return clustering_grids, refined\n",
                "    # -------------------------------\n",
                "    # KMEANS / AGGLOMERATIVE ‚Üí refine n_clusters\n",
                "    # -------------------------------\n",
                "    if model_name in [\"KMeans\", \"AgglomerativeClustering\"]:\n",
                "        try:\n",
                "            elbow_k = int(round(float(elbow_x)))\n",
                "        except Exception:\n",
                "            log(f\"[GRID REFINE] {model_name}: elbow_x not numeric ‚Üí grid NOT refined\", type=\"WARNING\")\n",
                "            return clustering_grids, refined\n",
                "        # Global safety bounds\n",
                "        k_min_global = 2\n",
                "        k_max_global = 25\n",
                "        # Local window around elbow\n",
                "        k_low = max(k_min_global, elbow_k - 3)\n",
                "        k_high = min(k_max_global, elbow_k + 3)\n",
                "        # Build refined grid\n",
                "        new_k_list = smart_logspace_choose(min_val=k_low, max_val=max(k_low + 1, k_high), datatype=\"integer\", num_grid_param=max(5, num_grid_param))\n",
                "        # Clean + ensure elbow included\n",
                "        new_k_list = sorted(set(int(k) for k in new_k_list))\n",
                "        if elbow_k not in new_k_list:\n",
                "            new_k_list.append(elbow_k)\n",
                "            new_k_list = sorted(new_k_list)\n",
                "        old_k_list = clustering_grids[model_name].get(\"n_clusters\", [])\n",
                "        clustering_grids[model_name][\"n_clusters\"] = new_k_list\n",
                "        refined = True\n",
                "        log(f\"[GRID REFINE] {model_name}: n_clusters refined around elbow={elbow_k} ‚Üí {new_k_list}\", type=\"SUCCESS\", bold=True)\n",
                "        log(f\"[GRID REFINE] {model_name}: previous n_clusters grid ‚Üí {old_k_list}\", level=2, type=\"INFO\")\n",
                "        if model_name == \"AgglomerativeClustering\":\n",
                "            linkages = clustering_grids[model_name].get(\"linkage\", [])\n",
                "            if any(link != \"ward\" for link in linkages):\n",
                "                metric = resolve_distance_metric(dataset_type=dataset_type, model_name=\"AgglomerativeClustering\")\n",
                "                clustering_grids[model_name][\"metric\"] = [metric]\n",
                "                log(f\"[GRID REFINE] AgglomerativeClustering: metric enforced ‚Üí {metric}\", level=2, type=\"INFO\")\n",
                "            else:\n",
                "                clustering_grids[model_name].pop(\"metric\", None)\n",
                "                log(\"[GRID REFINE] AgglomerativeClustering: linkage=ward only ‚Üí metric ignored\", level=2, type=\"INFO\")\n",
                "    # -------------------------------\n",
                "    # DBSCAN ‚Üí refine eps\n",
                "    # -------------------------------\n",
                "    if model_name == \"DBSCAN\":\n",
                "        try:\n",
                "            elbow_eps = float(elbow_x)\n",
                "        except Exception:\n",
                "            log(\"[GRID REFINE] DBSCAN: elbow_x not numeric ‚Üí grid NOT refined\", type=\"WARNING\")\n",
                "            return clustering_grids, refined\n",
                "        # Global safety bounds\n",
                "        eps_min_global = 1e-4\n",
                "        eps_max_global = 5.0\n",
                "        # Multiplicative local window (robust for eps)\n",
                "        eps_low = max(eps_min_global, elbow_eps * 0.6)\n",
                "        eps_high = min(eps_max_global, elbow_eps * 1.6)\n",
                "        new_eps_list = smart_logspace_choose(min_val=eps_low, max_val=max(eps_low * 1.05, eps_high), datatype=\"float\", num_grid_param=max(7, num_grid_param))\n",
                "        # Clean + ensure elbow included\n",
                "        new_eps_list = sorted(set(float(e) for e in new_eps_list))\n",
                "        if elbow_eps not in new_eps_list:\n",
                "            new_eps_list.append(elbow_eps)\n",
                "            new_eps_list = sorted(new_eps_list)\n",
                "        old_eps_list = clustering_grids[model_name].get(\"eps\", [])\n",
                "        clustering_grids[model_name][\"eps\"] = new_eps_list\n",
                "        clustering_grids[model_name][\"metric\"] = [resolve_distance_metric(dataset_type)]\n",
                "        refined = True\n",
                "        log(f\"[GRID REFINE] DBSCAN: eps refined around elbow={round(elbow_eps, 6)} ‚Üí {new_eps_list}\", type=\"SUCCESS\", bold=True)\n",
                "        log(f\"[GRID REFINE] DBSCAN: previous eps grid ‚Üí {old_eps_list}\", level=2, type=\"INFO\")\n",
                "    # -------------------------------\n",
                "    # Final status\n",
                "    # -------------------------------\n",
                "    if not refined:\n",
                "        log(f\"[GRID REFINE] {model_name}: no refinement applied\", level=2, type=\"INFO\")\n",
                "    return clustering_grids, refined\n",
                "\n",
                "# ===============================\n",
                "# Plots elbow diagnostics with two panels:\n",
                "# Left: inertia-like (WCSS proxy)\n",
                "# Right: silhouette score\n",
                "# One curve per secondary hyperparameter.\n",
                "# ===============================\n",
                "def plotter_elbow_diagnostics_dual(elbow_results, axes, label_fontsize, title_fontsize, text_fontsize, show_elbows=True):\n",
                "    ax_inertia = axes[0]\n",
                "    ax_sil     = axes[1]\n",
                "    model_name = elbow_results[\"model_name\"]\n",
                "    x_label    = elbow_results[\"x_label\"]\n",
                "    x_list     = elbow_results[\"x_list\"]\n",
                "    # -------------------------------\n",
                "    # Inertia plot\n",
                "    # -------------------------------\n",
                "    ax_inertia.set_title(f\"{model_name} - Inertia\", fontsize=title_fontsize)\n",
                "    ax_inertia.set_xlabel(x_label, fontsize=label_fontsize)\n",
                "    ax_inertia.set_ylabel(\"Inertia-like (WCSS)\", fontsize=label_fontsize)\n",
                "    ax_inertia.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    # -------------------------------\n",
                "    # Silhouette plot\n",
                "    # -------------------------------\n",
                "    ax_sil.set_title(f\"{model_name} - Silhouette\", fontsize=title_fontsize)\n",
                "    ax_sil.set_xlabel(x_label, fontsize=label_fontsize)\n",
                "    ax_sil.set_ylabel(\"Silhouette score\", fontsize=label_fontsize)\n",
                "    ax_sil.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    for key, curve in elbow_results[\"curves\"].items():\n",
                "        label = curve[\"label\"]\n",
                "        inertia_vals = curve[\"inertia_like\"]\n",
                "        sil_vals     = curve[\"silhouette\"]\n",
                "        # Inertia curve\n",
                "        line, = ax_inertia.plot(x_list, inertia_vals, marker=\"o\", label=label)\n",
                "        line_color = line.get_color()\n",
                "        # Silhouette curve (skip None safely)\n",
                "        sil_clean = []\n",
                "        x_clean   = []\n",
                "        for x, s in zip(x_list, sil_vals):\n",
                "            if s is not None:\n",
                "                x_clean.append(x)\n",
                "                sil_clean.append(s)\n",
                "        if len(sil_clean) > 0:\n",
                "            ax_sil.plot(x_clean, sil_clean, marker=\"o\", label=label)\n",
                "        # Elbow marker\n",
                "        if show_elbows:\n",
                "            elbow_x = curve.get(\"elbow_x\", None)\n",
                "            if elbow_x is not None:\n",
                "                ax_inertia.axvline(x=elbow_x, linestyle=\"--\", linewidth=2,color=line_color)\n",
                "                ax_inertia.text(elbow_x, max(inertia_vals),\n",
                "                                f\"  Elbow at {round(elbow_x, 3)}\",\n",
                "                                fontsize=text_fontsize,\n",
                "                                fontstyle=\"italic\",\n",
                "                                rotation=90,\n",
                "                                va=\"top\",\n",
                "                                ha=\"left\", \n",
                "                                color=line_color,\n",
                "                                clip_on=True)\n",
                "    ax_inertia.legend(fontsize=text_fontsize)\n",
                "    ax_sil.legend(fontsize=text_fontsize)\n",
                "\n",
                "# ===============================\n",
                "# Assigns cluster labels to new samples for clustering models without predict().\n",
                "# Supports:\n",
                "#   - DBSCAN  ‚Üí proximity to core samples (noise = -1)\n",
                "#   - AgglomerativeClustering ‚Üí proximity to cluster centroids\n",
                "# ===============================\n",
                "def clustering_predict(model, X_train, X_new, dataset_type):\n",
                "    # Ensure numeric arrays\n",
                "    X_train = np.asarray(X_train, dtype=float)\n",
                "    X_new   = np.asarray(X_new, dtype=float)\n",
                "    # Initialize labels\n",
                "    y_new = np.ones(shape=len(X_new), dtype=int) * (-1)\n",
                "    # Resolve distance metric\n",
                "    metric = resolve_distance_metric(dataset_type)\n",
                "    if metric == \"euclidean\":\n",
                "        def distance(a, b):\n",
                "            return np.linalg.norm(a - b)\n",
                "    elif metric == \"cosine\":\n",
                "        def distance(a, b):\n",
                "            num = np.dot(a, b)\n",
                "            den = np.linalg.norm(a) * np.linalg.norm(b)\n",
                "            if den == 0:\n",
                "                return 1.0\n",
                "            return 1.0 - (num / den)\n",
                "    else:\n",
                "        raise ValueError(f\"Unsupported distance metric: {metric}\")\n",
                "    # --------------------------------\n",
                "    # DBSCAN CASE\n",
                "    # --------------------------------\n",
                "    if isinstance(model, DBSCAN):\n",
                "        # DBSCAN: use core samples as cluster representatives\n",
                "        core_samples = model.components_\n",
                "        core_labels  = model.labels_[model.core_sample_indices_]\n",
                "        for j, x_new in enumerate(X_new):\n",
                "            for i, x_core in enumerate(core_samples):\n",
                "                # Assign label if within eps radius\n",
                "                if distance(x_new, x_core) < model.eps:\n",
                "                    y_new[j] = core_labels[i]\n",
                "                    break\n",
                "        return y_new\n",
                "    # --------------------------------\n",
                "    # AGGLOMERATIVE CLUSTERING CASE\n",
                "    # --------------------------------\n",
                "    if isinstance(model, AgglomerativeClustering):\n",
                "        # Compute centroids from training data and labels\n",
                "        unique_labels = np.unique(model.labels_)\n",
                "        centroids = {}\n",
                "        for lab in unique_labels:\n",
                "            mask = (model.labels_ == lab)\n",
                "            centroids[lab] = np.mean(X_train[mask], axis=0)\n",
                "        # Assign each new sample to closest centroid\n",
                "        for j, x_new in enumerate(X_new):\n",
                "            best_label = None\n",
                "            best_dist  = None\n",
                "            for lab, centroid in centroids.items():\n",
                "                d = distance(x_new, centroid)\n",
                "                if best_dist is None or d < best_dist:\n",
                "                    best_dist  = d\n",
                "                    best_label = lab\n",
                "            y_new[j] = best_label\n",
                "        return y_new\n",
                "    # --------------------------------\n",
                "    # Unsupported model\n",
                "    # --------------------------------\n",
                "    raise TypeError(f\"Clustering_predict does not support model of type {type(model)}\")\n",
                "\n",
                "# ===============================\n",
                "# Clustering scatter plotter\n",
                "# ===============================\n",
                "def plotter_clusters_scatter(X_2d, labels, ax, title, label_fontsize, title_fontsize):\n",
                "    # X_2d expected shape: (n_samples, 2)\n",
                "    df_plot = pd.DataFrame({\n",
                "        \"x\": X_2d[:, 0],\n",
                "        \"y\": X_2d[:, 1],\n",
                "        \"cluster\": labels.astype(int)\n",
                "    })\n",
                "    # Noise cluster (-1) will appear as its own color\n",
                "    sns.scatterplot(data=df_plot, x=\"x\", y=\"y\", hue=\"cluster\", palette=\"bright\", ax=ax, s=60)\n",
                "    ax.set_title(title, fontsize=title_fontsize)\n",
                "    ax.set_xlabel(\"Component 1\", fontsize=label_fontsize)\n",
                "    ax.set_ylabel(\"Component 2\", fontsize=label_fontsize)\n",
                "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "    ax.legend(title=\"Cluster\")\n",
                "\n",
                "# ===============================\n",
                "# Clustering triangular pairwise scatter plotter (original features)\n",
                "# ===============================\n",
                "def plotter_clusters_pairwise_triangular(X_original, labels, figWidth_unit, figHeight_unit, label_fontsize):\n",
                "    # -------------------------------\n",
                "    # Prepare data\n",
                "    # -------------------------------\n",
                "    if not isinstance(X_original, pd.DataFrame):\n",
                "        X_original = pd.DataFrame(X_original)\n",
                "    X_plot = X_original.copy().reset_index(drop=True)\n",
                "    X_plot[\"cluster\"] = pd.Series(labels).astype(int).reset_index(drop=True)\n",
                "    # Keep only numeric features\n",
                "    feature_cols = []\n",
                "    for c in X_plot.columns:\n",
                "        if c != \"cluster\" and pd.api.types.is_numeric_dtype(X_plot[c]):\n",
                "            feature_cols.append(c)\n",
                "    n_feat = len(feature_cols)\n",
                "    if n_feat < 2:\n",
                "        log(\"Not enough numeric features to build pairwise plots\", type=\"WARNING\")\n",
                "        return\n",
                "    # -------------------------------\n",
                "    # Build triangular subplot grid\n",
                "    # -------------------------------\n",
                "    n_rows = n_feat - 1\n",
                "    n_cols = n_feat - 1\n",
                "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(figWidth_unit * n_cols, figHeight_unit * n_rows))\n",
                "    # Normalize axes to 2D array\n",
                "    if n_rows == 1 and n_cols == 1:\n",
                "        axes = np.array([[axes]])\n",
                "    elif n_rows == 1:\n",
                "        axes = np.array([axes])\n",
                "    elif n_cols == 1:\n",
                "        axes = np.array([[ax] for ax in axes])\n",
                "    # -------------------------------\n",
                "    # Plot upper-triangle pairs\n",
                "    # Row i uses feature i as X\n",
                "    # and features (i+1 ... end) as Y\n",
                "    # -------------------------------\n",
                "    first_used_ax = None\n",
                "    for i in range(n_rows):\n",
                "        x_feat = feature_cols[i]\n",
                "        y_list = feature_cols[i + 1 :]\n",
                "        for j in range(n_cols):\n",
                "            ax = axes[i, j]\n",
                "            # In row i we only use the first len(y_list) columns\n",
                "            if j >= len(y_list):\n",
                "                ax.axis(\"off\")\n",
                "                continue\n",
                "            y_feat = y_list[j]\n",
                "            sns.scatterplot(data=X_plot, x=x_feat, y=y_feat, hue=\"cluster\", palette=\"deep\", ax=ax, legend=True)\n",
                "            ax.set_xlabel(x_feat, fontsize=label_fontsize)\n",
                "            ax.set_ylabel(y_feat, fontsize=label_fontsize)\n",
                "            ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
                "            if first_used_ax is None:\n",
                "                first_used_ax = ax\n",
                "    # -------------------------------\n",
                "    # Global legend (single)\n",
                "    # -------------------------------\n",
                "    if first_used_ax is not None:\n",
                "        handles, legend_labels = first_used_ax.get_legend_handles_labels()\n",
                "        if len(handles) > 0:\n",
                "            fig.legend(handles, legend_labels, title=\"Cluster\", loc=\"upper right\")\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# ===============================\n",
                "# Selects best clustering model for pseudo-tagging based on:\n",
                "#    1) Optimized models\n",
                "#    2) Maximize (silhouette - alpha * noise_ratio)\n",
                "# ===============================\n",
                "def select_best_pseudo_clustering(optimized_results, default_results, alpha_noise=0.5):\n",
                "    candidates = []\n",
                "    # Optimized first\n",
                "    for name, res in optimized_results.items():\n",
                "        sil = res.get(\"silhouette\")\n",
                "        noise = res.get(\"noise_ratio\", 0.0)\n",
                "        if sil is not None:\n",
                "            score = sil - alpha_noise * noise\n",
                "            candidates.append((score, name, \"optimized\"))\n",
                "    # Fallback to default if no optimized valid\n",
                "    if len(candidates) == 0:\n",
                "        for name, res in default_results.items():\n",
                "            sil = res.get(\"silhouette\")\n",
                "            noise = res.get(\"noise_ratio\", 0.0)\n",
                "            if sil is not None:\n",
                "                score = sil - alpha_noise * noise\n",
                "                candidates.append((score, name, \"default\"))\n",
                "    if len(candidates) == 0:\n",
                "        return None, None\n",
                "    # Best score wins\n",
                "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
                "    return candidates[0][1], candidates[0][2]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color:red\">General Inputs</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PIPELINE DEFINITION\n",
                "# ===============================\n",
                "supervised_learning = True\n",
                "pseudo_tagging      = False\n",
                "\n",
                "# ===============================\n",
                "# DATASET SETTINGS\n",
                "# ===============================\n",
                "raw_data_separator = \";\"\n",
                "raw_data_input_path = \"../data/raw/raw_price_dowloaded_4h_START_31-12-2011_END_20-12-2025.csv\"\n",
                "processed_data_output_path = \"../data/processed/\"\n",
                "models_output_path = \"../models/\"\n",
                "\n",
                "# ===============================\n",
                "# PLOTTING\n",
                "# ===============================\n",
                "num_values_to_plot = 40     # Max number of different values to plot (for CATEGORY_var)\n",
                "num_bins = 100              # Num of bins (for NUMERIC_var plots)\n",
                "figHeight_unit = 8\n",
                "figWidth_unit = 12\n",
                "plot_palette = \"pastel\"\n",
                "plot_tick_font_size = 15\n",
                "plot_label_font_size = 15\n",
                "plot_text_font_size = 20\n",
                "plot_title_font_size = 30\n",
                "\n",
                "# ===============================\n",
                "# RANDOMNESS AND TEST SIZE\n",
                "# ===============================\n",
                "random_seed = 42\n",
                "test_size = 0.2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 0 - LOAD RAW DATAFRAME"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Thresholds for dataset type proposal (NLP)\n",
                "min_text_avg_length = 25      # Recommended: 20‚Äì30 chars ‚Üí typical minimum for real text\n",
                "min_text_avg_words  = 3       # Recommended: >3 words ‚Üí avoids titles/labels\n",
                "min_points_nlp = 70           # Min points to be considered NLP dataset (max point = 100)\n",
                "# Thresholds for dataset type proposal (Time-Series)\n",
                "ts_main_col_index = 0         # Index of the datetime column to be used as the primary time axis\n",
                "min_rows_in_dataset = 75      # Recommended: > 75\n",
                "max_numeric_var = 5           # More than 5 is odd for a time-series\n",
                "freq_ratio_threshold = 0.7    # Recommended: > 0.7 (values from 0 to 1)\n",
                "min_points_ts = 70            # Min points to be considered Time-Series dataset (max point = 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "learning_type = \"SUPERVISED\" if supervised_learning else \"UNSUPERVISED\"\n",
                "print(\"===============================\")\n",
                "print(\"STEP 0) LOAD RAW DATAFRAME - \",learning_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# LOAD DATAFRAME\n",
                "# ===============================\n",
                "df_raw = pd.read_csv(raw_data_input_path, sep=raw_data_separator)\n",
                "log(\"DataFrame loaded successfully!\", type=\"SUCCESS\")\n",
                "\n",
                "# ===============================\n",
                "# DATASET TYPE CHECKING\n",
                "# ===============================\n",
                "# -------------------------------\n",
                "# NLP CHECKING (probabilistic)\n",
                "# -------------------------------\n",
                "log(\"NLP cheking:\", custom_icon=\"üìù\")\n",
                "nlp_score = 0            # Final probability score (0‚Äì100)\n",
                "# Evidence 1: dataset has at least one text-like column\n",
                "object_cols = []\n",
                "for col in df_raw.columns:\n",
                "    if df_raw[col].dtype in [\"object\", \"category\"]:\n",
                "        object_cols.append(col)\n",
                "if len(object_cols) >= 1:\n",
                "    nlp_score += 25\n",
                "    log(f\"Object-type columns found: {object_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No object-type columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 2: long text columns exist\n",
                "text_cols = []\n",
                "for col in object_cols:\n",
                "    avg_len = df_raw[col].dropna().str.len().mean()\n",
                "    if avg_len is not None and avg_len > min_text_avg_length:\n",
                "        text_cols.append(col)\n",
                "if len(text_cols) >= 1:\n",
                "    nlp_score += 25\n",
                "    log(f\"Long text-like columns found: {text_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No long text-like columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 3: rich text structure (words per entry)\n",
                "rich_text_cols = []\n",
                "for col in text_cols:\n",
                "    avg_words = df_raw[col].dropna().str.split().str.len().mean()\n",
                "    if avg_words is not None and avg_words > min_text_avg_words:\n",
                "        rich_text_cols.append(col)\n",
                "if len(rich_text_cols) > 0:\n",
                "    nlp_score += 25\n",
                "    log(f\"At least one column shows word-rich entries (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No column shows word-rich entries\", level=2, type=\"ERROR\")\n",
                "# Evidence 4: number of text columns realistic for NLP\n",
                "if len(text_cols) == 1:\n",
                "    nlp_score += 15\n",
                "    log(f\"There is only 1 long text-like column, it is typical for NLP (+15 points)\", level=2, type=\"SUCCESS\")\n",
                "elif len(text_cols) > 1:\n",
                "    nlp_score += 5\n",
                "    log(f\"There are more than 1 long text-like column, it could be possible for NLP (+5 points)\", level=2, type=\"SUCCESS\")\n",
                "else:\n",
                "    log(\"No long text-like columns found\", level=2, type=\"ERROR\")\n",
                "# Evidence 5: proportion of non-empty text entries\n",
                "if len(text_cols) >= 1:\n",
                "    non_empty_ratio = df_raw[text_cols[0]].dropna().str.len().gt(10).mean()\n",
                "    if non_empty_ratio >= 0.6:\n",
                "        nlp_score += 10\n",
                "        log(f\"Majority of rows contain meaningful text (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"Too many empty/short text entries\", level=2, type=\"ERROR\")\n",
                "# Cap score at 100\n",
                "nlp_score = min(nlp_score, 100)\n",
                "log(f\"Score to be a NLP Dataset: {nlp_score}/100 points\", level=2, type=\"INFO\", bold=True)\n",
                "\n",
                "# -------------------------------\n",
                "# TIME-SERIES CHECKING (probabilistic) - REWRITTEN (robust)\n",
                "# -------------------------------\n",
                "log(\"TIME-SERIES checking:\", custom_icon=\"‚è±Ô∏è\")\n",
                "ts_score = 0\n",
                "ts_cols = []  # detected datetime-like columns\n",
                "# Evidence 1) Detect datetime columns robustly\n",
                "# - Only try parsing on non-numeric columns (avoid open/close being parsed as datetimes)\n",
                "# - Require high parse success ratio\n",
                "# - Require high uniqueness ratio (avoid garbage/constant parses)\n",
                "parse_threshold = 0.85   # >=85% parseable -> candidate datetime-like\n",
                "unique_threshold = 0.50  # >=50% unique among parsed -> realistic TS index\n",
                "for col in df_raw.columns:\n",
                "    s = df_raw[col]\n",
                "    # ‚úÖ Skip numeric columns directly (prevents open/max/min/close/vol being detected as datetime)\n",
                "    if pd.api.types.is_numeric_dtype(s):\n",
                "        continue\n",
                "    # ‚úÖ Only attempt datetime parsing on object/string/category columns\n",
                "    if not (\n",
                "        pd.api.types.is_object_dtype(s)\n",
                "        or pd.api.types.is_string_dtype(s)\n",
                "        or pd.api.types.is_categorical_dtype(s)\n",
                "    ):\n",
                "        continue\n",
                "    # Parse assuming EU format day-first (like \"31/12/2011 8:00\")\n",
                "    dt_tmp = pd.to_datetime(s, errors=\"coerce\", dayfirst=True, utc=False)\n",
                "    # Parse success ratio\n",
                "    success_ratio = dt_tmp.notna().mean()\n",
                "    if success_ratio < parse_threshold:\n",
                "        log(f\"Column '{col}' not datetime-like (parse success {success_ratio:.2%})\", level=3, type=\"INFO\")\n",
                "        continue\n",
                "    # Uniqueness ratio among successfully parsed timestamps\n",
                "    parsed_count = int(dt_tmp.notna().sum())\n",
                "    unique_ratio = (dt_tmp.nunique(dropna=True) / parsed_count) if parsed_count > 0 else 0.0\n",
                "    if unique_ratio < unique_threshold:\n",
                "        log(\n",
                "            f\"Column '{col}' parsed but not TS-like (unique ratio {unique_ratio:.2%})\",\n",
                "            level=3,\n",
                "            type=\"INFO\"\n",
                "        )\n",
                "        continue\n",
                "    ts_cols.append(col)\n",
                "    log(\n",
                "        f\"Datetime-like column detected: '{col}' (parse success {success_ratio:.2%}, unique ratio {unique_ratio:.2%})\",\n",
                "        level=2,\n",
                "        type=\"SUCCESS\"\n",
                "    )\n",
                "# ‚úÖ Prefer the 'time' column if it exists\n",
                "if \"time\" in df_raw.columns and \"time\" in ts_cols:\n",
                "    ts_cols = [\"time\"] + [c for c in ts_cols if c != \"time\"]\n",
                "    ts_main_col_index = 0\n",
                "# Case 1 ‚Üí no datetime columns\n",
                "if len(ts_cols) == 0:\n",
                "    log(\"No datetime columns detected\", level=2, type=\"ERROR\")\n",
                "# Case 2 ‚Üí exactly one column\n",
                "elif len(ts_cols) == 1:\n",
                "    ts_score += 40\n",
                "    log(f\"Unique datetime column detected: {ts_cols[ts_main_col_index]} (+40 points)\", level=2, type=\"SUCCESS\")\n",
                "# Case 3 ‚Üí multiple datetime columns\n",
                "else:\n",
                "    ts_score += 25\n",
                "    log(f\"Several datetime columns were detected: {ts_cols} (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "# Evaluate time-series structure (only if datetime column exists)\n",
                "if len(ts_cols) > 0:\n",
                "    dt_col = ts_cols[ts_main_col_index]\n",
                "    # Parse chosen datetime column (EU format)\n",
                "    serie_date_time_raw = pd.to_datetime(df_raw[dt_col], errors=\"coerce\", dayfirst=True).dropna()\n",
                "    # Evidence 2) Chronologically sorted (do NOT penalize; warn and sort later)\n",
                "    if serie_date_time_raw.is_monotonic_increasing:\n",
                "        ts_score += 20\n",
                "        log(f\"Datetime column '{dt_col}' is sorted (+20 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Datetime column '{dt_col}' is NOT sorted (we will sort before modeling)\", level=2, type=\"INFO\")\n",
                "    # Evidence 3) Detecting time-series frequency\n",
                "    serie_date_time_diff_raw = serie_date_time_raw.diff().dropna()\n",
                "    if len(serie_date_time_diff_raw) > 0:\n",
                "        most_common_delta = serie_date_time_diff_raw.mode()[0]\n",
                "        freq_ratio = (serie_date_time_diff_raw == most_common_delta).mean()\n",
                "        if freq_ratio >= freq_ratio_threshold:\n",
                "            ts_score += 20\n",
                "            log(f\"Regular frequency detected: {most_common_delta} (+20 points)\", level=2, type=\"SUCCESS\")\n",
                "            log(f\"Frequency consistency ratio: {freq_ratio:.3f}\", level=3, type=\"INFO\")\n",
                "        else:\n",
                "            log(\n",
                "                f\"No regular frequency detected (best delta: {most_common_delta}, ratio: {freq_ratio:.3f})\",\n",
                "                level=2,\n",
                "                type=\"ERROR\"\n",
                "            )\n",
                "    else:\n",
                "        log(\"Not enough data to detect time-series frequency\", level=2, type=\"ERROR\")\n",
                "    # Evidence 4) Numeric columns over time\n",
                "    numeric_cols = df_raw.select_dtypes(include=[\"number\"]).columns\n",
                "    if 1 <= len(numeric_cols) <= max_numeric_var:\n",
                "        ts_score += 10\n",
                "        log(f\"Numeric variables suitable for TS (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    elif len(numeric_cols) < 1:\n",
                "        log(\"There is not at least one numeric variable for time-series\", level=2, type=\"ERROR\")\n",
                "    else:\n",
                "        log(\"Too many numeric variables for time-series\", level=2, type=\"ERROR\")\n",
                "    # Evidence 5) Dataset length\n",
                "    if len(df_raw) >= min_rows_in_dataset:\n",
                "        ts_score += 10\n",
                "        log(f\"Enough rows for time-series (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"Dataset too short for time-series\", level=2, type=\"ERROR\")\n",
                "# Cap score at 100\n",
                "ts_score = min(ts_score, 100)\n",
                "log(f\"Score to be a TIME-SERIES Dataset: {ts_score}/100 points\", level=2, type=\"INFO\", bold=True)\n",
                "\n",
                "\n",
                "# ===============================\n",
                "# DATASET TYPE PROPOSAL\n",
                "# ===============================\n",
                "if ts_score >= nlp_score and ts_score >= min_points_ts:\n",
                "    dataset_type_auto = \"TIME-SERIES\"\n",
                "elif nlp_score >= ts_score and nlp_score >= min_points_nlp:\n",
                "    dataset_type_auto = \"NLP\"\n",
                "else:\n",
                "    dataset_type_auto = \"TABULAR\"\n",
                "print(\"\\n\")\n",
                "log(f\"Proposed dataset type: {dataset_type_auto}\", type=\"INFO\", bold=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 1 - EXPLORE DATAFRAME"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_type = \"TIME-SERIES\" # Confirm dataset type (TIME-SERIES, NLP or TABULAR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S1 = df_raw.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 1) EXPLORE DATAFRAME - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    log(\"Dataset detected as NLP (long-text dataset)\", custom_icon=\"üìù\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    log(\"Sample of raw text entries:\", type=\"INFO\")\n",
                "    display(df_S1[text_cols].head(5))\n",
                "    # Print text length stats\n",
                "    df_S1_lengths = df_S1[text_cols[0]].astype(str).str.len()\n",
                "    log(\"Text length statistics:\", type=\"INFO\")\n",
                "    log(f\"Average lengths: {df_S1_lengths.mean():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Median lengths: {df_S1_lengths.median():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Max lengths: {df_S1_lengths.max():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    # Compute word statistics\n",
                "    df_S1_words = df_S1[text_cols[0]].astype(str).str.split().str.len()\n",
                "    log(\"Word count statistics:\", type=\"INFO\")\n",
                "    log(f\"Average words: {df_S1_words.mean():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Median words: {df_S1_words.median():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "    log(f\"Max words: {df_S1_words.max():.1f} chars\", level=2, custom_icon=\"üìä\")\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    log(\"Dataset detected as TIME-SERIES\", custom_icon=\"‚è±Ô∏è\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    # Try to parse every column\n",
                "    df_S1_ts_cols = []\n",
                "    for col in df_S1.columns:\n",
                "        try:\n",
                "            pd.to_datetime(df_S1[col], errors=\"raise\")\n",
                "            df_S1_ts_cols.append(col)\n",
                "        except:\n",
                "            pass\n",
                "    # Time column\n",
                "    df_S1_ts_main_col = df_S1_ts_cols[ts_main_col_index]\n",
                "    serie_date_time_S1 = pd.to_datetime(df_S1[df_S1_ts_main_col], errors=\"coerce\")\n",
                "    log(\"Time index information:\", type=\"INFO\")\n",
                "    log(f\"Detected time column: '{df_S1_ts_main_col}'\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"Start date: {serie_date_time_S1.min()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"End date: {serie_date_time_S1.max()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    log(f\"Total duration: {serie_date_time_S1.max() - serie_date_time_S1.min()}\", level=2, custom_icon=\"üìÖ\")\n",
                "    # Estimate frequency and granularity\n",
                "    serie_date_time_diff_S1 = serie_date_time_S1.diff().dropna()\n",
                "    if len(serie_date_time_diff_S1) > 0:\n",
                "        df_S1_most_common_delta = serie_date_time_diff_S1.mode()[0] # Most common interval\n",
                "        df_S1_smallest_delta = serie_date_time_diff_S1.min() # Minimal interval\n",
                "        df_S1_freq_ratio = (serie_date_time_diff_S1 == df_S1_most_common_delta).mean()\n",
                "        # Determine granularity\n",
                "        df_S1_seconds = df_S1_most_common_delta.total_seconds()\n",
                "        granularity = determine_granularity(df_S1_seconds)\n",
                "        log(f\"Most common interval: {df_S1_most_common_delta} (granularity: {granularity})\", level=2, custom_icon=\"üìÖ\")\n",
                "        log(f\"Smallest interval: {df_S1_smallest_delta}\", level=2, custom_icon=\"üìÖ\")\n",
                "        log(f\"Frequency consistency ratio: {df_S1_freq_ratio:.3f}\", level=2, custom_icon=\"üìÖ\")\n",
                "    else:\n",
                "        log(\"Not enough data points to estimate frequency\", type=\"WARNING\")\n",
                "    # Missing or irregular timestamps\n",
                "    missing_ratio = 1 - (serie_date_time_diff_S1 == most_common_delta).mean() if len(serie_date_time_diff_S1) > 0 else None\n",
                "    if missing_ratio is not None and missing_ratio > 0.10:\n",
                "        log(\"Irregular timestamps detected (missing or uneven intervals)\", type=\"WARNING\")\n",
                "        log(f\"Irregularity ratio: {missing_ratio:.2f}\", level=2, custom_icon=\"‚ö†Ô∏è\")\n",
                "    # Numeric metrics\n",
                "    numeric_cols = df_S1.select_dtypes(include=[\"number\"]).columns\n",
                "    log(\"Numeric metrics detected:\", type=\"INFO\")\n",
                "    for col in numeric_cols:\n",
                "        log(f\"{col}\", level=2, custom_icon=\"üìà\")\n",
                "    # Statistics for each metric\n",
                "    log(\"Basic statistics per numeric variable:\", type=\"INFO\")\n",
                "    display(df_S1[numeric_cols].describe().T)\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    log(\"Dataset detected as TABULAR\", custom_icon=\"üßÆ\")\n",
                "    # Print info\n",
                "    log(f\"Shape of the DataFrame: {df_S1.shape}\", type=\"INFO\")\n",
                "    log(\"Content of the DataFrame:\", type=\"INFO\")\n",
                "    display(df_S1.head(5))\n",
                "    log(\"Info of the DataFrame (dataType and non-null values):\", type=\"INFO\")\n",
                "    df_S1.info(verbose=True, show_counts=True)\n",
                "    # Ordered info (fewest non-null first)\n",
                "    ordered_info = pd.DataFrame({\n",
                "        \"Column\": df_S1.columns,\n",
                "        \"Non-Null Count\": df_S1.notnull().sum(),\n",
                "        \"Null Count\": df_S1.isnull().sum(),\n",
                "        \"Dtype\": df_S1.dtypes.astype(str)\n",
                "    }).sort_values(by=\"Non-Null Count\", ascending=True)\n",
                "    log(\"Ordered info by number of non-null values:\", type=\"INFO\")\n",
                "    display(ordered_info)\n",
                "    # Count unique attributes (unsorted)\n",
                "    df_S1_summary = pd.DataFrame({\n",
                "        \"Column\": df_S1.columns,\n",
                "        \"Unique_Count\": df_S1.nunique().values\n",
                "    })\n",
                "    log(\"DataFrame unique attributes (unsorted):\", type=\"INFO\")\n",
                "    display(df_S1_summary)\n",
                "    # Ordered summary (fewest unique first)\n",
                "    df_S1_summary_ordered = df_S1_summary.sort_values(by=\"Unique_Count\", ascending=True)\n",
                "    log(\"Ordered unique attributes (fewest unique first):\", type=\"INFO\")\n",
                "    display(df_S1_summary_ordered)\n",
                "    # Automatic Warning for high-uniqueness columns\n",
                "    unique_counts = df_S1.nunique()\n",
                "    high_unique_cols = unique_counts[unique_counts == len(df_S1)].index.tolist()\n",
                "    if len(high_unique_cols) > 0:\n",
                "        log(\"Consider dropping the following columns for having UNIQUE values for EVERY row:\", type=\"WARNING\")\n",
                "        for col in high_unique_cols:\n",
                "            log(f\"{col}\", level=2, custom_icon=\"üóëÔ∏è\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cols_to_drop = ['open', 'max', 'min', 'vol']  # List of column names to drop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S2 = df_S1.copy()\n",
                "df_S2_ts_main_col = df_S1_ts_main_col if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 2) IDENTIFY TEXT COLUMN & SELECT RELEVANT ATTRIBUTES - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 2) IDENTIFY TIME COLUMN & BUILD TIME-SERIES - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 2) SELECT RELEVANT ATTRIBUTES - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # Drop non-relevant attributes\n",
                "    if cols_to_drop:\n",
                "        df_S2=df_S2.drop(labels=cols_to_drop, axis =1)\n",
                "    # Print results\n",
                "    log(\"Non-Relevant attributes have been dropped\", type=\"SUCCESS\")\n",
                "    log(f\"Previous df's columns: {len(df_S1.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current df's columns: {len(df_S2.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Final DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "    # Identify main text column\n",
                "    main_text_col_S2 = text_cols[0]\n",
                "    log(f\"Main text column selected: '{main_text_col_S2}'\", type=\"INFO\")\n",
                "    # Check for additional object columns\n",
                "    extra_object_cols = [c for c in df_S2.columns if c not in text_cols]\n",
                "    if len(extra_object_cols) > 0:\n",
                "        log(\"Additional non-text object columns detected:\", type=\"INFO\")\n",
                "        for col in extra_object_cols:\n",
                "            log(f\"Column: '{col}'\", level=2, custom_icon=\"üìÑ\")\n",
                "    else:\n",
                "        log(\"No additional metadata columns detected\", type=\"INFO\")\n",
                "    # Warn if more than one text-like column exists\n",
                "    if len(text_cols) > 1:\n",
                "        log(\"Multiple text-like columns detected. Consider selecting only one for preprocessing\", type=\"WARNING\")\n",
                "        for col in text_cols:\n",
                "            log(f\"Column: '{col}'\", level=2, custom_icon=\"üìù\")\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# =============================== \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Identify the temporal column\n",
                "    if len(df_S2_ts_main_col) > 0:\n",
                "        log(f\"Detected temporal column: '{df_S2_ts_main_col}'\", type=\"FOUND\")\n",
                "    else:\n",
                "        log(\"No temporal column found ‚Üí cannot build time index\", type=\"ERROR\")\n",
                "        df_S2_ts_main_col = None\n",
                "    # Stop if no datetime column exists\n",
                "    if df_S2_ts_main_col is None:\n",
                "        raise ValueError(\"No datetime column found ‚Üí cannot build time index.\")\n",
                "    # Drop the column if still present as normal column\n",
                "    if df_S2_ts_main_col in df_S2.columns:\n",
                "        df_S2 = df_S2.drop(columns=[df_S2_ts_main_col])\n",
                "    # Copy previous time-series\n",
                "    serie_date_time_S2 = serie_date_time_S1.copy()\n",
                "    # Sort by datetime just in case\n",
                "    serie_date_time_S2 = serie_date_time_S2.sort_values()\n",
                "    # Assign the datetime index\n",
                "    df_S2.index = serie_date_time_S2\n",
                "    # Make sure the index has a name\n",
                "    df_S2.index.name = df_S2_ts_main_col\n",
                "    # Show preview\n",
                "    log(f\"Indexed DataFrame by '{df_S2_ts_main_col}'\", type=\"INFO\")\n",
                "    log(\"Preview of time-indexed DataFrame:\", type=\"INFO\")\n",
                "    display(df_S2.head(5))\n",
                "    # Extract numeric target series\n",
                "    df_S2_numeric_cols = df_S2.select_dtypes(include=[\"number\"]).columns\n",
                "    if len(df_S2_numeric_cols) > 0:\n",
                "        # Pick the first numeric column as the time-series\n",
                "        df_S2_numeric_target_col = df_S2_numeric_cols[0]\n",
                "        df_timeseries_S2 = df_S2[df_S2_numeric_target_col].dropna()\n",
                "        log(f\"Extracted target time-series '{df_S2_numeric_target_col}'\", type=\"SUCCESS\")\n",
                "        display(df_timeseries_S2.head(5))\n",
                "        fig, axis = plt.subplots(figsize = (figWidth_unit, figHeight_unit))\n",
                "        sns.lineplot(data = df_timeseries_S2)\n",
                "        plt.grid(True)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    else:\n",
                "        log(\"No numeric metrics detected to extract as the main time-series\", type=\"ERROR\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Drop non-relevant attributes\n",
                "    if cols_to_drop:\n",
                "        df_S2=df_S2.drop(labels=cols_to_drop, axis =1)\n",
                "    # Print results\n",
                "    log(\"Non-Relevant attributes have been dropped\", type=\"SUCCESS\")\n",
                "    log(f\"Previous df's columns: {len(df_S1.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current df's columns: {len(df_S2.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Final DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "    display(df_S2.head())\n",
                "    # Count attributes\n",
                "    df_S2_summary = pd.DataFrame({\n",
                "        \"Column\": df_S2.columns,\n",
                "        \"Unique_Count\": df_S2.nunique().values\n",
                "    })\n",
                "    log(\"Final DataFrame unique attributes:\", level=2, type=\"INFO\")\n",
                "    display(df_S2_summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Threshold to determine seasonal component type (multiplicative or additive)\n",
                "seasonal_component_type_threshold = 0.3 # if amplitude/abs(mean_val) > threshold -> \"multiplicative\"\n",
                "# Thresholds to detect strong seasonal (both need to be higher than thresholds)\n",
                "strong_seasonal_threshold_for_acf = 0.6 # ACF at the seasonal period\n",
                "strong_seasonal_threshold_for_var_ratio = 0.5 # Variance ratio: Var(seasonal) / Var(original)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S3 = df_S2.copy()\n",
                "df_timeseries_S3 = df_timeseries_S2.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "df_S3_freq_ratio = df_S1_freq_ratio if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 3) REMOVE DUPLICATES - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 3) DECOMPOSING - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 3) REMOVE DUPLICATES - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    num_duplicates=df_S3.duplicated().sum()\n",
                "    if num_duplicates == 0:\n",
                "        df_S3=df_S3\n",
                "        log(\"Previous DataFrame does not contain duplicates:\", type=\"SUCCESS\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        df_S3_duplicates=df_S3[df_S3.duplicated()]\n",
                "        df_S3=df_S3.drop_duplicates()\n",
                "        log(f\"Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\", type=\"WARNING\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "        log(\"These are the dropped duplicates:\", level=2, type=\"INFO\")\n",
                "        display(df_S3_duplicates)\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================  \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # 1) Validate series regularity before decomposition\n",
                "    if df_S3_freq_ratio < freq_ratio_threshold:\n",
                "        raise ValueError(f\"Decomposition skipped due to Low frequency regularity (freq_ratio={df_S3_freq_ratio:.3f})\")\n",
                "    log(f\"Timestamp regularity OK (freq_ratio={df_S3_freq_ratio:.3f})\", level=1, type=\"SUCCESS\")\n",
                "    # 2) Detect period using ACF (primary robust method)\n",
                "    period_acf = infer_period_from_acf(df_timeseries_S3)\n",
                "    if period_acf is not None:\n",
                "        period_S3 = period_acf\n",
                "        log(f\"Seasonality detected via ACF ‚Üí period = {period_acf}\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"No significant seasonality found via ACF\", level=1, type=\"WARNING\")\n",
                "        # 3) Fallback based on granularity (if ACF failed)\n",
                "        period_fallback = infer_period_from_granularity(granularity)\n",
                "        if period_fallback is None:\n",
                "            raise ValueError(\"Unable to infer any valid period. Decomposition skipped because it is impossible to infer any valid period\")\n",
                "        log(f\"Fallback period inferred: {period_fallback} (granularity={granularity})\", level=1, type=\"SUCCESS\")\n",
                "        period_S3 = period_fallback\n",
                "    # 4) Determine model: additive or multiplicative\n",
                "    seasonal_component_type_S3 = infer_seasonal_component_type(df_timeseries_S3, seasonal_component_type_threshold)\n",
                "    log(f\"Type of seasonal component selected: {seasonal_component_type_S3}\", level=1, type=\"SUCCESS\")\n",
                "    # 5) Perform decomposition\n",
                "    try:\n",
                "        decomposition_S3 = seasonal_decompose(x=df_timeseries_S3, model=seasonal_component_type_S3, period=period_S3)\n",
                "        trend_S3 = decomposition_S3.trend\n",
                "        seasonal_S3 = decomposition_S3.seasonal\n",
                "        residual_S3 = decomposition_S3.resid\n",
                "        log(\"Decomposition completed successfully\", level=1, type=\"SUCCESS\")\n",
                "    except Exception as e:\n",
                "        raise ValueError(f\"Decomposition failed: {e}\")\n",
                "    # 6) Compute seasonality strength metrics\n",
                "    strong_seasonality_S3, seasonality_metrics_S3 = assess_seasonality_strength(\n",
                "        original        = df_timeseries_S3,\n",
                "        seasonal        = seasonal_S3,\n",
                "        period          = period_S3,\n",
                "        acf_threshold   = strong_seasonal_threshold_for_acf,\n",
                "        var_ratio       = strong_seasonal_threshold_for_var_ratio\n",
                "    )\n",
                "    if strong_seasonality_S3:\n",
                "        log(f\"Strong seasonality detected (var_ratio={seasonality_metrics_S3[\"seasonal_var_ratio\"]:.3f}, acf={seasonality_metrics_S3[\"acf_at_period\"]:.3f})\", level = 1, type  = \"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Weak or no seasonality (var_ratio={seasonality_metrics_S3[\"seasonal_var_ratio\"]:.3f}, acf={seasonality_metrics_S3[\"acf_at_period\"]:.3f})\", level = 1, type  = \"WARNING\")\n",
                "    # 7) Plot decomposition\n",
                "    fig, axis = plt.subplots(figsize = (figWidth_unit, figHeight_unit))\n",
                "    sns.lineplot(data = df_timeseries_S3, color = \"blue\", label = \"Original Time-series\")\n",
                "    sns.lineplot(data = trend_S3, color = \"orange\", label = \"Trend\", linestyle = \"--\")\n",
                "    sns.lineplot(data = residual_S3, color = \"red\", label = \"Residual\")\n",
                "    sns.lineplot(data = seasonal_S3, color = \"green\", label = \"Seasonal\", linestyle = \"--\")\n",
                "    plt.grid(True)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    num_duplicates=df_S3.duplicated().sum()\n",
                "    if num_duplicates == 0:\n",
                "        df_S3=df_S3\n",
                "        log(\"Previous DataFrame does not contain duplicates:\", type=\"SUCCESS\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        df_S3_duplicates=df_S3[df_S3.duplicated()]\n",
                "        df_S3=df_S3.drop_duplicates()\n",
                "        log(f\"Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\", type=\"WARNING\")\n",
                "        log(f\"Previous DataFrame shape: {df_S2.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current DataFrame shape: {df_S3.shape}\", level=2, type=\"INFO\")\n",
                "        log(\"These are the dropped duplicates:\", level=2, type=\"INFO\")\n",
                "        display(df_S3_duplicates)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "var_type_proposal_threshold = 0.500      # [%] Under this percentage of unique values, the variable is proposed as CATEGORIC"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_num_classes = 50                    # Recommended: 50 classes ‚Üí max accepted number of different classes for CATEGORIC target variable"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accepted_alpha_dickey_fuller = 0.05     # Accepted error in the hypothesis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S4 = df_S3.copy()\n",
                "main_text_col_S4 = main_text_col_S2 if dataset_type == \"NLP\" else None\n",
                "df_timeseries_S4 = df_timeseries_S3.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 4) PROPOSE TARGET VARIABLE  - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 4) STACIONARY ANAYSIS - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 4) PROPOSE TARGET VARIABLE AND ATTRIBUTE TYPE - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # UNSUPERVISED NLP ‚Üí skip target detection\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED MODE detected ‚Üí skipping target proposal\", type=\"WARNING\", bold=True)\n",
                "        y_var_auto = None\n",
                "        # Still show attribute classification for later steps\n",
                "        log(f\"Main text column assumed as: '{main_text_col_S4}'\", type=\"INFO\")\n",
                "        non_text_cols = [c for c in df_S4.columns if c != main_text_col_S4]\n",
                "        log(f\"Detected metadata columns: {non_text_cols}\", type=\"INFO\")\n",
                "    else:\n",
                "        # Safety check: make sure we have at least one text column\n",
                "        if len(text_cols) == 0:\n",
                "            log(\"No text-like columns were previously detected: cannot propose a target variable for NLP\", type=\"ERROR\")\n",
                "            y_var_auto = None\n",
                "        else:\n",
                "            # Main text column\n",
                "            log(f\"Main text column assumed as: '{main_text_col_S4}'\", type=\"INFO\")\n",
                "            # List of non-text columns (potential candidates for target variable)\n",
                "            non_text_cols = []\n",
                "            for col in df_S4.columns:\n",
                "                if col != main_text_col_S4:\n",
                "                    non_text_cols.append(col)\n",
                "            if len(non_text_cols) == 0:\n",
                "                log(\"No additional columns apart from the main text: this is likely an unsupervised NLP task\", type=\"WARNING\")\n",
                "                y_var_auto = None\n",
                "            else:\n",
                "                log(\"Columns to be proposed as target variable:\", type=\"INFO\")\n",
                "                candidate_scores = {}   # Store total score per candidate\n",
                "                # Iterate through candidate columns\n",
                "                for col in non_text_cols:\n",
                "                    log(f\"Column '{col}':\", level=2, type=\"FOUND\")\n",
                "                    # Skip empty columns\n",
                "                    if len(df_S4[col].dropna()) == 0:\n",
                "                        continue\n",
                "                    n_unique = df_S4[col].dropna().nunique()\n",
                "                    avg_len = df_S4[col].dropna().astype(str).str.len().mean()\n",
                "                    class_distribution = df_S4[col].dropna().value_counts(normalize=True)\n",
                "                    score = 0\n",
                "                    # Evidence 1) Reasonable number of unique classes\n",
                "                    if n_unique <= max_num_classes:\n",
                "                        score += 35\n",
                "                        log(f\"Acceptable number of classes: {n_unique} (+35 points)\", level=3, type=\"SUCCESS\")\n",
                "                    else:\n",
                "                        log(f\"Too many unique classes: {n_unique}\", level=3, type=\"ERROR\")\n",
                "                    # Evidence 2) Short label length (typical for target columns)\n",
                "                    if avg_len <= 20:\n",
                "                        score += 30\n",
                "                        log(f\"Short label length (avg {avg_len:.1f} chars) (+30 points)\", level=3, type=\"SUCCESS\")\n",
                "                    else:\n",
                "                        log(f\"Labels are too long on average (avg {avg_len:.1f} chars)\", level=3, type=\"ERROR\")\n",
                "                    # Evidence 3) Categorical / label-like data type\n",
                "                    if str(df_S4[col].dtype) in [\"object\", \"category\"]:\n",
                "                        score += 20\n",
                "                        log(f\"Object/Categorical dtype (+20 points)\", level=3, type=\"SUCCESS\")\n",
                "                    else:\n",
                "                        log(f\"Non-categorical dtype ({str(df_S4[col].dtype)})\", level=3, type=\"ERROR\")\n",
                "                    # Evidence 4) Class balance\n",
                "                    if class_distribution.min() >= 0.01:\n",
                "                        score += 15\n",
                "                        log(f\"Reasonable class balance (min class ratio ‚â• 1%) (+15 points)\", level=3, type=\"SUCCESS\")\n",
                "                    else:\n",
                "                        log(f\"Some classes have less than 1% of samples\", level=3, type=\"ERROR\")\n",
                "                    # Store results\n",
                "                    candidate_scores[col] = score\n",
                "                    log(f\"Total score: {candidate_scores[col]}/100 points\", level=3, custom_icon=\"üìù\", bold=True)\n",
                "                # Select best candidate\n",
                "                if len(candidate_scores) == 0:\n",
                "                    print(\"\\n\")\n",
                "                    log(\"No valid non-text columns found to be used as target: this may be an unsupervised NLP task\", type=\"WARNING\", bold=True)\n",
                "                    y_var_auto = None\n",
                "                else:\n",
                "                    y_var_auto = max(candidate_scores, key=candidate_scores.get)\n",
                "                    best_score = candidate_scores[y_var_auto]\n",
                "                    print(\"\\n\")\n",
                "                    log(f\"Proposed target variable: '{y_var_auto}' ({best_score}/100 points)\", type=\"INFO\", bold=True)\n",
                "        \n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================     \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Perform Dickey-Fuller test to check for stacionarity\n",
                "    series_Dickey_Fuller_results = test_stationarity(series=df_timeseries_S4)\n",
                "\n",
                "    if series_Dickey_Fuller_results[\"p-value\"] >= accepted_alpha_dickey_fuller:\n",
                "        log(f\"Dickey-Fuller test's results:\\n{series_Dickey_Fuller_results}\\n\", type=\"INFO\")\n",
                "        log(f\"Hyphotesis rejected:Time-series IS NOT stationary, recursive differenciation is carried out\\n\", level=1, type=\"WARNING\", bold = True)\n",
                "        # Peform recursively Dickley-Fuller test until the time-series becomes stacionary\n",
                "        df_stationary_timeseries_S4, diff_count_S4, series_recursive_Dickey_Fuller_results = make_stationary_recursive(\n",
                "            series=df_timeseries_S4,\n",
                "            alpha=accepted_alpha_dickey_fuller\n",
                "            )\n",
                "        log(f\"Recursive differenciation ({diff_count_S4} step/s) -> Dickey-Fuller test's results:\\n{series_recursive_Dickey_Fuller_results}\\n\", level=1, type=\"INFO\")\n",
                "\n",
                "        if series_recursive_Dickey_Fuller_results[\"p-value\"] >= accepted_alpha_dickey_fuller:\n",
                "            log(f\"Time-series cannot become stationary (after {diff_count_S4} differencing step/s)\", level=1, type=\"WARNING\", bold = True)\n",
                "        else:\n",
                "            log(f\"Time-series can become stationary (after {diff_count_S4} differencing step/s)\", level=1, type=\"SUCCESS\", bold = True)\n",
                "    else:\n",
                "        diff_count_S4= 0\n",
                "        log(f\"Hyphotesis accepted: time-series IS stationary, no need of differenciation\", level=1, type=\"SUCCESS\", bold = True)\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # List of columns\n",
                "    columns = df_S4.columns.tolist()\n",
                "    # Classify attributes\n",
                "    category_var_auto = []\n",
                "    numeric_var_auto  = []\n",
                "    for col in df_S4.columns:\n",
                "        total_rows = len(df_S4)\n",
                "        if total_rows == 0:\n",
                "            continue\n",
                "        unique_count = df_S4[col].dropna().nunique()\n",
                "        unique_ratio = unique_count / total_rows * 100\n",
                "        col_dtype    = str(df_S4[col].dtype)\n",
                "        # Case 1: text/categorical\n",
                "        if col_dtype in [\"object\", \"category\"]:\n",
                "            category_var_auto.append(col)\n",
                "            continue\n",
                "        # Case 2: integer\n",
                "        if col_dtype.startswith(\"int\"):\n",
                "            if unique_ratio <= var_type_proposal_threshold:\n",
                "                category_var_auto.append(col)\n",
                "            else:\n",
                "                numeric_var_auto.append(col)\n",
                "            continue\n",
                "        # Case 3: float\n",
                "        if col_dtype.startswith(\"float\"):\n",
                "            if unique_ratio <= var_type_proposal_threshold:\n",
                "                category_var_auto.append(col)\n",
                "            else:\n",
                "                numeric_var_auto.append(col)\n",
                "            continue\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED MODE detected ‚Üí skipping target proposal\", type=\"WARNING\", bold=True)\n",
                "        y_var_auto = None\n",
                "        # Still return attribute typing (needed for scaling, PCA, KMeans, etc.)\n",
                "        log(f\"Proposed CATEGORY Attributes: {category_var_auto}\", type=\"INFO\", bold=True)\n",
                "        log(f\"Proposed NUMERIC Attributes:   {numeric_var_auto}\", type=\"INFO\", bold=True)\n",
                "    else:\n",
                "        # Target variable proposal\n",
                "        y_scores = {}\n",
                "        min_reg_unique_values = 10\n",
                "        penalty_too_continuous = 150\n",
                "        corr_matrix = df_S4.corr(numeric_only=True)\n",
                "        for col in df_S4.columns:\n",
                "            if col.lower() in [\"id\", \"index\", \"timestamp\", \"date\"]:\n",
                "                continue\n",
                "            series = df_S4[col].dropna()\n",
                "            if len(series) == 0:\n",
                "                continue\n",
                "            score = 0\n",
                "            log(f\"'{col}':\", type=\"INFO\")\n",
                "            # 1) Name heuristic (common target patterns)\n",
                "            if col.lower() in [\"target\", \"label\", \"class\", \"y\", \"output\"]:\n",
                "                score += 40\n",
                "                log(f\"Name looks like a target (+40 points)\", level=2, type=\"SUCCESS\")\n",
                "            # 2) CATEGORICAL target logic (60+ points possible)\n",
                "            if col in category_var_auto:\n",
                "                n_unique = series.nunique()\n",
                "                dist     = series.value_counts(normalize=True)\n",
                "                score += 30\n",
                "                log(f\"Categorical variable confirmed (+30 points)\", level=2, type=\"SUCCESS\")\n",
                "                if n_unique <= max_num_classes:\n",
                "                    score += 20\n",
                "                    log(f\"Acceptable number of classes ({n_unique}) (+20 points)\", level=2, type=\"SUCCESS\")\n",
                "                if len(category_var_auto) == 1:\n",
                "                    score += 25\n",
                "                    log(f\"Unique categorical column (+25 points)\", level=2, type=\"SUCCESS\")\n",
                "                imbalance_ratio = dist.max() / dist.min()\n",
                "                if imbalance_ratio <= 20:\n",
                "                    score += 10\n",
                "                    log(f\"Balanced classes (imbalance ratio {imbalance_ratio:.1f}) (+10 points)\", level=2, type=\"SUCCESS\")\n",
                "                if corr_matrix is not None and col in corr_matrix.columns:\n",
                "                    corrs = corr_matrix[col].abs()\n",
                "                    mean_corr = corrs.mean()\n",
                "                    if mean_corr >= 0.05:\n",
                "                        score += 5\n",
                "                        log(f\"Moderate correlation with numeric features (+5 points)\", level=2, type=\"SUCCESS\")\n",
                "            # 3) NUMERIC regression logic (60+ points possible)\n",
                "            if col in numeric_var_auto:\n",
                "                unique_count = series.nunique()\n",
                "                score += 30\n",
                "                log(f\"Numeric variable confirmed (+30 points)\", level=2, type=\"SUCCESS\")\n",
                "                if unique_count >= min_reg_unique_values:\n",
                "                    score += 30\n",
                "                    log(f\"Sufficient variability ({unique_count} unique values) (+30 points)\", level=2, type=\"SUCCESS\")\n",
                "                if unique_count > 300:\n",
                "                    score -= penalty_too_continuous\n",
                "                    log(f\"Too continuous to be a target ({unique_count} unique values) (-{penalty_too_continuous} points)\", level=2, type=\"WARNING\")\n",
                "                if corr_matrix is not None and col in corr_matrix.columns:\n",
                "                    corrs = corr_matrix[col].abs()\n",
                "                    if corrs.max() > 0.90:\n",
                "                        score -= 40\n",
                "                        log(f\"Excessively high correlation with another feature (>{0.90}) (-40 points)\", level=2, type=\"WARNING\")\n",
                "                    elif corrs.mean() > 0.05:\n",
                "                        score += 5\n",
                "                        log(f\"Moderate correlation pattern (+5 points)\", level=2, type=\"SUCCESS\")\n",
                "            y_scores[col] = score\n",
                "            log(f\"Total target score: {score}/100 points\", level=2, custom_icon=\"üìù\")\n",
                "        # Select best candidate\n",
                "        if len(y_scores) > 0:\n",
                "            y_var_auto = max(y_scores, key=y_scores.get)\n",
                "            best_score = y_scores[y_var_auto]\n",
                "            print(\"\\n\")\n",
                "            log(f\"Proposed target variable: '{y_var_auto}' ({best_score}/100 points)\", type=\"INFO\", bold=True)\n",
                "        else:\n",
                "            y_var_auto = None\n",
                "            log(\"No suitable target variable could be proposed automatically\", type=\"WARNING\", bold=True)\n",
                "        # Clean logs for CATEGORY / NUMERIC Attributes\n",
                "        if y_var_auto is not None:\n",
                "            category_log_list = []\n",
                "            for col in category_var_auto:\n",
                "                if col != y_var_auto:\n",
                "                    category_log_list.append(col)\n",
                "            numeric_log_list = []\n",
                "            for col in numeric_var_auto:\n",
                "                if col != y_var_auto:\n",
                "                    numeric_log_list.append(col)\n",
                "        else:\n",
                "            category_log_list = category_var_auto\n",
                "            numeric_log_list  = numeric_var_auto\n",
                "        # Print proposed types\n",
                "        log(f\"Proposed CATEGORY Attributes: {category_log_list}\", type=\"INFO\", bold=True)\n",
                "        log(f\"Proposed NUMERIC Attributes: {numeric_log_list}\", type=\"INFO\", bold=True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_var = \"close\"  # Confirm target variable (Set as None if UNSUPERVISED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_plots_UNIVARIANT = True # Draw UNIVARIANT ANALYSIS plots?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if_target_is_binary_treat_as_categoric = True   # Confirm treatment for target variable"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:lightgreen\">Inputs for TABULAR (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "y_var_highlighting_color = \"green\" # Color to highlight target variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S5 = df_S4.copy()\n",
                "residual_S5 = residual_S3 if dataset_type == \"TIME-SERIES\" else None\n",
                "seasonal_component_type_S5 = seasonal_component_type_S3 if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 5) TRANSFORMATION OF TARGET VARIABLE - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 5) VARIABILITY ANALYSIS - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 5 - UNIVARIABLE ANALYSIS - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# TARGET VARIABLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "print(\"TARGET VARIABLE\")\n",
                "print(\"-------------------------------\")\n",
                "float_discrete_threshold = min(30, round(0.02 * len(df_S4)))    # Dynamic threshold for FLOAT to be considered DISCRETE\n",
                "if supervised_learning:\n",
                "    # Basic stats\n",
                "    y_unique_values = df_S5[y_var].nunique()\n",
                "    y_unique_ratio = y_unique_values / len(df_S5) * 100\n",
                "    y_dtype_kind = df_S5[y_var].dtype.kind\n",
                "    y_var_type = None\n",
                "    requires_formating_nlp = False\n",
                "    # ----------------------------------------------\n",
                "    # BASE TYPE DETECTION (dtype-driven)\n",
                "    # ----------------------------------------------\n",
                "    # Case 1: text-based or boolean columns ‚Üí categorical\n",
                "    if y_dtype_kind in ['O','b']:\n",
                "        y_var_type = \"CATEGORIC\"\n",
                "    # Case 2: numeric columns (int, uint, float) ‚Üí decide later by cardinality\n",
                "    elif y_dtype_kind in ['i','u','f']:\n",
                "        y_var_type = \"NUMERIC\"\n",
                "    # ----------------------------------------------\n",
                "    # CARDINALITY RULE (only for numeric dtypes)\n",
                "    # ----------------------------------------------\n",
                "    if y_var_type == \"NUMERIC\":\n",
                "        if y_unique_ratio <= var_type_proposal_threshold:\n",
                "            y_var_type = \"CATEGORIC\"\n",
                "    # ----------------------------------------------\n",
                "    # OPTIONAL RULE ‚Äî binary numeric treated as categorical\n",
                "    # ----------------------------------------------\n",
                "    if y_var_type == \"NUMERIC\" and y_unique_values == 2 and if_target_is_binary_treat_as_categoric:\n",
                "        y_var_type = \"CATEGORIC\"\n",
                "    # ----------------------------------------------\n",
                "    # ASSIGN SUBTYPE\n",
                "    # ----------------------------------------------\n",
                "    if y_var_type == \"CATEGORIC\":\n",
                "        requires_formating_nlp = True\n",
                "        if y_unique_values == 2:\n",
                "            y_var_subtype = \"BINARY\"\n",
                "        elif y_unique_values > 2:\n",
                "            y_var_subtype = \"MULTICLASS\"\n",
                "        else:\n",
                "            y_var_subtype = \"CONSTANT\"\n",
                "        log(\"Confirmed TARGET Variable: \" + y_var + \" -> \" + y_var_type + \" and \" + y_var_subtype, type=\"INFO\", bold=True)\n",
                "    else:\n",
                "        if y_dtype_kind in ['i','u']:\n",
                "            y_var_subtype = \"DISCRETE\"\n",
                "        elif y_dtype_kind == 'f' and y_unique_values < float_discrete_threshold:\n",
                "            y_var_subtype = \"DISCRETE\"\n",
                "        else:\n",
                "            y_var_subtype = \"CONTINUOUS\"\n",
                "        log(\"Confirmed TARGET Variable: \" + y_var + \" -> NUMERIC and \" + y_var_subtype, type=\"INFO\", bold=True)\n",
                "else:\n",
                "    # UNSUPERVISED MODE: No target ‚Üí skip entire target logic\n",
                "    y_var = None\n",
                "    y_var_type = None\n",
                "    y_var_subtype = None\n",
                "    requires_formating_nlp = False\n",
                "    log(\"UNSUPERVISED MODE detected ‚Üí No target variable is used in this pipeline\", type=\"WARNING\", bold=True)\n",
                "print(\"\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED MODE ‚Üí target transformation skipped\", type=\"WARNING\")\n",
                "    else:\n",
                "        # Transform y_var from CATEGORIC to NUMERIC format\n",
                "        if requires_formating_nlp:\n",
                "            # Instance encoder\n",
                "            y_var_encoder = LabelEncoder()\n",
                "            # Train encoder                       \n",
                "            y_var_encoder.fit(df_S5[y_var])\n",
                "            # Apply encoder                     \n",
                "            df_S5[y_var] = y_var_encoder.transform(df_S5[y_var])    \n",
                "            log(f\"Target variable '{y_var}' has been transformed with LabelEncoder():\", type=\"WARNING\")\n",
                "            for i in range(len(y_var_encoder.classes_)):\n",
                "                log(f\"{y_var_encoder.classes_[i]} -> {i}\", level=2, type=\"INFO\")\n",
                "        else:\n",
                "            log(f\"Target variable '{y_var}' does not need to be transformed with LabelEncoder():\", type=\"INFO\")\n",
                "        \n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================     \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Drop NaN values in residuals\n",
                "    residual_S5 = residual_S5.dropna()\n",
                "    # -------------------------------------------\n",
                "    # RULE A: Check for visible trend in residuals\n",
                "    # -------------------------------------------\n",
                "    # Compute simple linear regression on residual vs time index\n",
                "    x_index = np.arange(len(residual_S5))\n",
                "    # Fit linear regression slope\n",
                "    slope, intercept = np.polyfit(x_index, residual_S5.values, 1)\n",
                "    # Compute residual standard deviation\n",
                "    residual_std = np.std(residual_S5)\n",
                "    # If residuals are almost constant ‚Üí no trend by definition\n",
                "    if residual_std < 1e-8:\n",
                "        log(\"Residual's slope analysis: Residuals are almost constant ‚Üí no visible trend (good).\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Define a tolerance based on std and a minimum absolute tolerance\n",
                "        if abs(slope) < max(residual_std * 0.01, 1e-6):\n",
                "            log(\"Residual's slope analysis: No visible trend detected (good).\", type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's slope analysis: Trend detected in residuals (bad).\", type=\"WARNING\", bold=True)\n",
                "    # -------------------------------------------\n",
                "    # RULE B: Check periodicity using ACF\n",
                "    # -------------------------------------------\n",
                "    # If residuals are almost constant ‚Üí ACF cannot detect periodicity, assume GOOD\n",
                "    if residual_std < 1e-8:\n",
                "        log(\"Residual's ACF analysis: Residuals are almost constant ‚Üí no periodicity possible (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Compute ACF up to 40 lags safely\n",
                "        nlags = min(40, len(residual_S5) - 2)\n",
                "        acf_res = acf(residual_S5, nlags=nlags, fft=True, missing=\"drop\")\n",
                "        # Detect highest non-zero lag correlation\n",
                "        acf_res_no0 = acf_res[1:]\n",
                "        max_acf_lag = np.argmax(np.abs(acf_res_no0)) + 1\n",
                "        max_acf_value = acf_res[max_acf_lag]\n",
                "        # Log ACF\n",
                "        log(f\"Residual ACF strongest lag={max_acf_lag}, value={max_acf_value:.3f}\", level=1, type=\"INFO\")\n",
                "        # If ACF is NaN ‚Üí cannot infer periodicity ‚Üí assume GOOD\n",
                "        if np.isnan(max_acf_value):\n",
                "            log(\"Residual's ACF analysis: ACF cannot be computed reliably (likely constant residuals) ‚Üí no periodicity detected (good).\", level=1, type=\"SUCCESS\")\n",
                "        # Periodicity rule: if max ACF < 0.3 ‚Üí no meaningful periodicity\n",
                "        elif abs(max_acf_value) < 0.3:\n",
                "            log(\"Residual's ACF analysis: No periodic patterns detected (good).\", level=1, type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's ACF analysis: Residuals show periodic patterns (bad).\", level=1, type=\"WARNING\", bold=True)\n",
                "    # -------------------------------------------\n",
                "    # 4) RULE C: Check that residuals are centered\n",
                "    # -------------------------------------------\n",
                "    # Set theoretical center depending on decomposition model\n",
                "    residual_center = 1.0 if seasonal_component_type_S5 == \"multiplicative\" else 0.0\n",
                "    # Compute mean and standard deviation of cleaned residuals\n",
                "    mean_res = residual_S5.mean()\n",
                "    sd_res = residual_S5.std()\n",
                "    # Define relative and absolute tolerances\n",
                "    threshold_center = max(sd_res * 0.05, 1e-6)\n",
                "    # Center rule: mean close to the expected center (0 additive, 1 multiplicative)\n",
                "    if abs(mean_res - residual_center) < threshold_center:\n",
                "        log(f\"Residual's center analysis: Residuals centered around expected center ({residual_center}) (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(f\"Residual's center analysis: Residuals not centered around expected center ({residual_center}) (bad).\", level=1, type=\"WARNING\")\n",
                "    # -------------------------------------------\n",
                "    # 5) RULE D: Check for randomness using Ljung‚ÄìBox test\n",
                "    # -------------------------------------------\n",
                "    # Center residuals for randomness tests\n",
                "    residual_S5_centered = residual_S5 - residual_center\n",
                "    # Compute standard deviation\n",
                "    sd_res_centered = residual_S5_centered.std()\n",
                "    # If residuals are almost constant ‚Üí cannot test randomness, but constant noise = GOOD\n",
                "    if sd_res_centered < 1e-8:\n",
                "        log(\"Residual's randomness analysis: Residuals are almost constant ‚Üí randomness cannot be tested, assumed random (good).\", level=1, type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Define safe number of lags\n",
                "        safe_lag = min(10, len(residual_S5_centered) - 2)\n",
                "        # Compute Ljung‚ÄìBox p-value\n",
                "        ljung_box_p = acorr_ljungbox(residual_S5_centered, lags=[safe_lag], return_df=True)[\"lb_pvalue\"].iloc[0]\n",
                "        # Log p-value\n",
                "        log(f\"Ljung‚ÄìBox p-value (lag {safe_lag}) = {ljung_box_p:.4f}\", type=\"INFO\")\n",
                "        # Randomness rule: if p > 0.05 ‚Üí residuals behave like white noise\n",
                "        if ljung_box_p > 0.05:\n",
                "            log(\"Residual's randomness analysis: Residuals behave as random noise (good).\", type=\"SUCCESS\")\n",
                "        else:\n",
                "            log(\"Residual's randomness analysis: Residuals show correlation ‚Üí not white noise (bad).\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    if not supervised_learning:\n",
                "        # Just classify attributes normally\n",
                "        category_att = category_var_auto.copy()\n",
                "        numeric_att  = numeric_var_auto.copy()\n",
                "        # CATEGORY subtyping\n",
                "        binary_att = []\n",
                "        multiclass_att = []\n",
                "        constant_att = []\n",
                "        for att in category_att:\n",
                "            att_unique_values = df_S4[att].nunique()\n",
                "            if att_unique_values == 2:\n",
                "                binary_att.append(att)\n",
                "            elif att_unique_values > 2:\n",
                "                multiclass_att.append(att)\n",
                "            else:\n",
                "                constant_att.append(att)\n",
                "        # NUMERIC subtyping\n",
                "        discrete_att = []\n",
                "        continuos_att = []\n",
                "        for att in numeric_att:\n",
                "            att_dtype = df_S4[att].dtype.kind\n",
                "            unique_count = df_S4[att].nunique()\n",
                "            if att_dtype in ['i','u']:\n",
                "                discrete_att.append(att)\n",
                "            elif att_dtype == 'f' and unique_count < float_discrete_threshold:\n",
                "                discrete_att.append(att)\n",
                "            else:\n",
                "                continuos_att.append(att)\n",
                "        # Print results\n",
                "        log(\"Confirmed CATEGORY Attributes:\", type=\"INFO\")\n",
                "        log(f\"BINARY: {binary_att}\",      level=2, type=\"INFO\")\n",
                "        log(f\"MULTICLASS: {multiclass_att}\", level=2, type=\"INFO\")\n",
                "        log(f\"CONSTANT: {constant_att}\",  level=2, type=\"INFO\")\n",
                "        log(\"Confirmed NUMERIC Attributes:\", type=\"INFO\")\n",
                "        log(f\"DISCRETE: {discrete_att}\",  level=2, type=\"INFO\")\n",
                "        log(f\"CONTINUOUS: {continuos_att}\", level=2, type=\"INFO\")\n",
                "        if not make_plots_UNIVARIANT:\n",
                "            log(\"UNIVARIABLE ANALYSIS is not printed, set make_plots_UNIVARIANT = True\", type=\"WARNING\")\n",
                "        else:\n",
                "            # Same plotting logic, but WITHOUT highlighting the target\n",
                "            print(\"üè∑Ô∏è CATEGORY VARIABLES\")\n",
                "            if not category_att:\n",
                "                log(\"This type of plot is non applicable because there are not CATEGORIC variables in the DataFrame\", type=\"WARNING\")\n",
                "            else:    \n",
                "                var_to_plot = category_att.copy()\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "                fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                axes = axes.flatten()\n",
                "                idx = 0\n",
                "                for col in var_to_plot:\n",
                "                    unique_count = df_S5[col].nunique()\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
                "                    else:\n",
                "                        order = df_S5[col].value_counts().index\n",
                "                    sns.countplot(ax=axes[idx], data=df_S5, x=col, hue=col, palette=plot_palette, order=order, legend=False)\n",
                "                    axes[idx].tick_params(axis='x', rotation=90, labelsize=plot_tick_font_size)\n",
                "                    axes[idx].set_title(col, fontdict={\"fontsize\": plot_title_font_size})\n",
                "                    axes[idx].set_xlabel(\"\")\n",
                "                    idx += 1\n",
                "                for j in range(idx, len(axes)):\n",
                "                    axes[j].set_visible(False)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "            print(\"üî¢ NUMERIC VARIABLES\")\n",
                "            if not numeric_att:\n",
                "                log(\"This type of plot is non applicable because there are not NUMERIC variables in the DataFrame\", type=\"WARNING\")\n",
                "            else:\n",
                "                var_to_plot = numeric_att.copy()\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "                fig, axes = plt.subplots(nrows=num_rows * 2, ncols=num_cols,\n",
                "                                         figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
                "                                         gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
                "                var_idx = 0\n",
                "                for row in range(num_rows):\n",
                "                    for col in range(num_cols):\n",
                "                        if var_idx >= len(var_to_plot):\n",
                "                            axes[row * 2, col].set_visible(False)\n",
                "                            axes[row * 2 + 1, col].set_visible(False)\n",
                "                            continue\n",
                "                        colname = var_to_plot[var_idx]\n",
                "                        sns.histplot(ax=axes[row * 2, col], data=df_S5, x=colname, bins=num_bins)\n",
                "                        sns.boxplot(ax=axes[row * 2 + 1, col], data=df_S5, x=colname)\n",
                "                        axes[row * 2, col].set_title(colname, fontdict={\"fontsize\": plot_title_font_size})\n",
                "                        axes[row * 2, col].set_xlabel(\"\")\n",
                "                        axes[row * 2 + 1, col].set_xlabel(\"\")\n",
                "                        var_idx += 1\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "    else:\n",
                "        # Confirm attribute types\n",
                "        category_att = []\n",
                "        numeric_att = []\n",
                "        for att in category_var_auto:\n",
                "            if att != y_var:\n",
                "                category_att.append(att)\n",
                "        for att in numeric_var_auto:\n",
                "            if att != y_var:\n",
                "                numeric_att.append(att)\n",
                "        # Checking CATEGORY attributes\n",
                "        binary_att = []\n",
                "        multiclass_att = []\n",
                "        constant_att = []\n",
                "        for att in category_att:\n",
                "            att_unique_values = df_S4[att].nunique()\n",
                "            if att_unique_values == 2:\n",
                "                binary_att.append(att)\n",
                "            elif att_unique_values > 2:\n",
                "                multiclass_att.append(att)\n",
                "            else:\n",
                "                constant_att.append(att)\n",
                "        # Checking NUMERIC attributes\n",
                "        discrete_att = []\n",
                "        continuos_att = []\n",
                "        for att in numeric_att:\n",
                "            att_dtype = df_S4[att].dtype.kind\n",
                "            unique_count = df_S4[att].nunique()\n",
                "            if att_dtype in ['i', 'u']:\n",
                "                discrete_att.append(att)\n",
                "            elif att_dtype == 'f' and unique_count < float_discrete_threshold:\n",
                "                discrete_att.append(att)\n",
                "            else:\n",
                "                continuos_att.append(att)\n",
                "        # Print results\n",
                "        log(\"Confirmed CATEGORY Attributes:\", type=\"INFO\")\n",
                "        log(f\"BINARY: {binary_att}\", level=2, type=\"INFO\")\n",
                "        log(f\"MULTICLASS: {multiclass_att}\", level=2, type=\"INFO\")\n",
                "        log(f\"CONSTANT: {constant_att}\", level=2, type=\"INFO\")\n",
                "        log(\"Confirmed NUMERIC Attributes:\", type=\"INFO\")\n",
                "        log(f\"DISCRETE: {discrete_att}\", level=2, type=\"INFO\")\n",
                "        log(f\"CONTINUOUS: {continuos_att}\", level=2, type=\"INFO\")\n",
                "\n",
                "        if not make_plots_UNIVARIANT:\n",
                "            log(\"UNIVARIABLE ANALYSIS is not printed, set make_plots_UNIVARIANT = True\", type=\"WARNING\")\n",
                "        else:\n",
                "            # -------------------------------------------\n",
                "            # CATEGORY VARIABLES (including target if categorical)\n",
                "            # -------------------------------------------\n",
                "            print(\"üè∑Ô∏è CATEGORY VARIABLES\")\n",
                "            if not category_att and y_var_type == \"NUMERIC\":\n",
                "                log(\"This type of plot is non applicable because there are not CATEGORIC variables in the DataFrame\", type=\"WARNING\")\n",
                "            else:    \n",
                "                var_to_plot = category_att.copy()\n",
                "                if y_var_type == \"CATEGORIC\" and y_var not in var_to_plot:\n",
                "                    var_to_plot.insert(0, y_var)\n",
                "                # Figure\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "                fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                axes = axes.flatten()\n",
                "                idx = 0\n",
                "                for col in var_to_plot:\n",
                "                    unique_count = df_S5[col].nunique()\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
                "                    else:\n",
                "                        order = df_S5[col].value_counts().index\n",
                "                    # Countplot\n",
                "                    sns.countplot(ax=axes[idx], data=df_S5, x=col, hue=col, palette=plot_palette, order=order, legend=False)\n",
                "                    axes[idx].tick_params(axis='x', rotation=90, labelsize=plot_tick_font_size)\n",
                "                    axes[idx].set_xlabel(\"\")\n",
                "                    # Highlight target\n",
                "                    target_box_style = dict(facecolor='none', edgecolor=y_var_highlighting_color, linewidth=5)\n",
                "                    target_title_style = dict(fontsize= plot_title_font_size, color=y_var_highlighting_color, fontweight='bold')\n",
                "                    if col == y_var:\n",
                "                        axes[idx].set_title(col, **target_title_style)\n",
                "                        axes[idx].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[idx].transAxes, **target_box_style))\n",
                "                    else:\n",
                "                        axes[idx].set_title(col, fontdict = {\"fontsize\": plot_title_font_size})\n",
                "                    # Add truncated info\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        msg = f\"There are {unique_count} values,\\nbut only {num_values_to_plot} have been plotted\"\n",
                "                        axes[idx].text(0.5, 0.9, msg, transform=axes[idx].transAxes, fontsize=plot_text_font_size, color=\"red\", ha=\"center\", va=\"top\", bbox=dict(facecolor=\"grey\", alpha=0.25, edgecolor=\"red\"))\n",
                "                    idx += 1\n",
                "                # Hide unused axes\n",
                "                for j in range(idx, len(axes)):\n",
                "                    axes[j].set_visible(False)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "            # -------------------------------------------\n",
                "            # NUMERIC VARIABLES (including target if numeric)\n",
                "            # -------------------------------------------\n",
                "            print(\"üî¢ NUMERIC VARIABLES\")\n",
                "            if not numeric_att and y_var_type == \"CATEGORIC\":\n",
                "                log(\"This type of plot is non applicable because there are not NUMERIC variables in the DataFrame\", type=\"WARNING\")\n",
                "            else: \n",
                "                var_to_plot = numeric_att.copy()\n",
                "                if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
                "                    var_to_plot.insert(0, y_var)\n",
                "                # Figure\n",
                "                num_cols = 2 \n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
                "                fig, axes = plt.subplots(nrows=num_rows * 2, ncols=num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows), gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
                "                var_idx = 0\n",
                "                for row in range(num_rows):\n",
                "                    for col in range(num_cols):\n",
                "                        if var_idx >= len(var_to_plot):\n",
                "                            axes[row * 2, col].set_visible(False)\n",
                "                            axes[row * 2 + 1, col].set_visible(False)\n",
                "                            continue\n",
                "                        colname = var_to_plot[var_idx]\n",
                "                        # Histogram\n",
                "                        sns.histplot(ax=axes[row * 2, col], data=df_S5, x=colname, bins=num_bins)\n",
                "                        axes[row * 2, col].set_xlabel(\"\")\n",
                "                        # Boxplot\n",
                "                        sns.boxplot(ax=axes[row * 2 + 1, col], data=df_S5, x=colname)\n",
                "                        axes[row * 2 + 1, col].set_xlabel(\"\")\n",
                "                        # Highlight target\n",
                "                        if colname == y_var:\n",
                "                            axes[row * 2, col].set_title(colname, **target_title_style)\n",
                "                            axes[row * 2 + 1, col].set_title(colname, **target_title_style)\n",
                "                            axes[row * 2, col].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2, col].transAxes, **target_box_style))\n",
                "                            axes[row * 2 + 1, col].add_patch(plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2 + 1, col].transAxes, **target_box_style))\n",
                "                        else:\n",
                "                            axes[row * 2, col].set_title(colname, fontdict = {\"fontsize\": plot_title_font_size})\n",
                "                        var_idx += 1\n",
                "                plt.tight_layout()\n",
                "                plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 6"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for NLP (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Default settings for text preprocessing:\n",
                "lowercase_text = True                   # Convert all text to lowercase\n",
                "remove_urls = True                     # Remove http/https/www links\n",
                "remove_emails = True                   # Remove email addresses\n",
                "remove_html_tags = True                # Remove HTML tags\n",
                "remove_non_letters = True              # Remove digits, punctuation, symbols, emojis, etc.\n",
                "remove_single_char_tokens = True       # Remove isolated 1-letter tokens (e.g., \"a\", \"b\")\n",
                "reduce_whitespace = True                # Replace multiple spaces with a single one\n",
                "tokenize_output = True                  # Output is a list of tokens after cleaning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_plots_MULTIVARIANT_ATTRIBUTES = True\n",
                "make_plots_GLOBAL_SCATTER = True\n",
                "make_plots_GLOBAL_PCA = True\n",
                "compute_GLOBAL_GEOMETRY_METRICS = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:lightgreen\">Inputs for TABULAR (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_plots_MULTIVARIANT_TARGET = True\n",
                "category_combi_att = \"\"                # Combination attribute for multivariant analysis (must be a CATEGORIC attribute)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S6 = df_S5.copy()\n",
                "main_text_col_S6 = main_text_col_S4 if dataset_type == \"NLP\" else None\n",
                "df_timeseries_S6 = df_timeseries_S3.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "period_S6 = period_S3 if dataset_type == \"TIME-SERIES\" else None\n",
                "suggested_d = diff_count_S4 if dataset_type == \"TIME-SERIES\" else None\n",
                "df_stacionary_timeseries_for_pacf_S6 = df_stationary_timeseries_S4.copy() if dataset_type == \"TIME-SERIES\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 6) TEXT PROCESSING - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 6) AUTOCORRELATION ANALYSIS - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 6 - MULTIVARIANT ANALYSIS - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # 1) Store original text before any modification\n",
                "    df_S6[main_text_col_S6 + \"_tmp\"] = df_S6[main_text_col_S6].astype(str)\n",
                "    # 2) Compute clean tokens from preprocess_text\n",
                "    df_S6[main_text_col_S6 + \"_clean\"] = df_S6[main_text_col_S6 + \"_tmp\"].apply(\n",
                "        lambda txt: preprocess_text(\n",
                "            txt,\n",
                "            mode=\"auto\",\n",
                "            lowercase_text=lowercase_text,\n",
                "            remove_urls=remove_urls,\n",
                "            remove_emails=remove_emails,\n",
                "            remove_html_tags=remove_html_tags,\n",
                "            remove_non_letters=remove_non_letters,\n",
                "            remove_single_char_tokens=remove_single_char_tokens,\n",
                "            reduce_whitespace=reduce_whitespace,\n",
                "            tokenize_output=tokenize_output\n",
                "        )[\"output\"])\n",
                "    # 3) Compute processing mode (url / text)\n",
                "    df_S6[\"processing_mode\"] = df_S6[main_text_col_S6 + \"_tmp\"].apply(\n",
                "        lambda txt: preprocess_text(\n",
                "            txt,\n",
                "            mode=\"auto\",\n",
                "            lowercase_text=lowercase_text,\n",
                "            remove_urls=remove_urls,\n",
                "            remove_emails=remove_emails,\n",
                "            remove_html_tags=remove_html_tags,\n",
                "            remove_non_letters=remove_non_letters,\n",
                "            remove_single_char_tokens=remove_single_char_tokens,\n",
                "            reduce_whitespace=reduce_whitespace,\n",
                "            tokenize_output=tokenize_output\n",
                "        )[\"mode_used\"])\n",
                "    # 4) Build the inspection DataFrame\n",
                "    df_inspection_S6 = pd.DataFrame({\n",
                "        main_text_col_S6 : df_S6[main_text_col_S6 + \"_tmp\"],\n",
                "        \"processing_mode_used\": df_S6[\"processing_mode\"]\n",
                "    })\n",
                "    # 5) Clean df_S6 for downstream pipeline\n",
                "    df_S6=df_S6.drop(labels=[main_text_col_S6], axis =1)\n",
                "    df_S6 = df_S6.rename(columns={main_text_col_S6 + \"_clean\": main_text_col_S6})\n",
                "    if supervised_learning:\n",
                "        # Keep clean text + target\n",
                "        df_S6 = df_S6[[main_text_col_S6, y_var]]\n",
                "    else:\n",
                "        # Keep ONLY the clean text column\n",
                "        df_S6 = df_S6[[main_text_col_S6]]\n",
                "\n",
                "    log(\"Text preprocessing applied successfully:\", type=\"SUCCESS\")\n",
                "    log(\"Processing mode used:\", type=\"INFO\")\n",
                "    display(df_inspection_S6)\n",
                "    log(\"Current DataFrame content:\", type=\"INFO\")\n",
                "    display(df_S6)\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================       \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Set frequency for ARIMA based on granularity\n",
                "    freq = get_freq_from_granularity(granularity)\n",
                "    if freq is not None:\n",
                "        try:\n",
                "            df_timeseries_S6 = df_timeseries_S6.asfreq(freq)\n",
                "            log(f\"Applied pandas frequency '{freq}' to series for ARIMA modeling.\", level=1, type=\"INFO\")\n",
                "        except Exception as e:\n",
                "            log(f\"Could not apply frequency '{freq}': {e}\", level=1, type=\"WARNING\")\n",
                "    else:\n",
                "        log(f\"No valid pandas freq for granularity='{granularity}', ARIMA frequency skipped.\", level=1, type=\"WARNING\")\n",
                "    # If series is almost constant ‚Üí no meaningful ACF/PACF\n",
                "    if df_timeseries_S6.std() < 1e-8:\n",
                "        log(\"ACF/PACF analysis: Time-series is almost constant ‚Üí no meaningful autocorrelation.\", level=1, type=\"WARNING\")\n",
                "    else:\n",
                "        # Get recommended lag based on granularity\n",
                "        recommended_lag = get_recommended_lag(granularity)\n",
                "        # Limit by available data\n",
                "        safe_lag = min(recommended_lag, len(df_timeseries_S6) - 2)\n",
                "        # Log chosen lag\n",
                "        log(f\"Used safe_lag = {safe_lag} (recommended={recommended_lag}, granularity={granularity})\", level=1, type=\"INFO\")\n",
                "        # Compute confidence (95%) limit for significance bands\n",
                "        conf_limit = 1.96 / np.sqrt(len(df_timeseries_S6))\n",
                "\n",
                "        # -------------------------------------------\n",
                "        # ACF BEHAVIOUR\n",
                "        # -------------------------------------------\n",
                "        # Compute numerical ACF values\n",
                "        acf_vals = acf(df_timeseries_S6, nlags=safe_lag, fft=True, missing=\"drop\")\n",
                "        # Build list of significant ACF lags\n",
                "        significant_acf_lags = []\n",
                "        seasonal_peaks_S6 = []\n",
                "        for lag in range(1, len(acf_vals)):\n",
                "            val = acf_vals[lag]\n",
                "            if abs(val) > conf_limit:\n",
                "                significant_acf_lags.append(lag)\n",
                "        if len(significant_acf_lags) == 0:\n",
                "            log(\"ACF analysis: No significant autocorrelation detected ‚Üí series close to white noise.\", level=1, type=\"INFO\")\n",
                "        else:\n",
                "            log(f\"ACF analysis: Significant autocorrelation at lags {significant_acf_lags}.\", level=1, type=\"INFO\")\n",
                "            # Check short-lag ACF persistence (trend indicator)\n",
                "            short_lags = []\n",
                "            for lag in significant_acf_lags:\n",
                "                if lag <= min(get_short_lag_cutoff(granularity), safe_lag):\n",
                "                    short_lags.append(lag)\n",
                "            if len(short_lags) > 0:\n",
                "                log(\"ACF analysis: High short-lag autocorrelation ‚Üí possible trend or strong persistence.\", level=1, type=\"INFO\")\n",
                "            # Check for seasonal multiples\n",
                "            if (period_S6 is not None) and (period_S6 <= safe_lag):\n",
                "                seasonal_peaks = []\n",
                "                max_k = safe_lag // period_S6\n",
                "                for k in range(1, max_k + 1):\n",
                "                    lag = k * period_S6\n",
                "                    if lag in significant_acf_lags:\n",
                "                        seasonal_peaks.append(lag)\n",
                "                seasonal_peaks_clean_print = []        \n",
                "                for l in seasonal_peaks:\n",
                "                    seasonal_peaks_clean_print.append(int(l))\n",
                "                if len(seasonal_peaks) > 0:\n",
                "                    seasonal_peaks_S6 = seasonal_peaks.copy()\n",
                "                    log(f\"ACF analysis: Significant seasonal peaks at lags {seasonal_peaks_clean_print} ‚Üí strong seasonality.\", level=1, type=\"SUCCESS\")\n",
                "\n",
                "        # -------------------------------------------\n",
                "        # PACF BEHAVIOUR\n",
                "        # -------------------------------------------\n",
                "        # Compute numerical PACF values\n",
                "        pacf_vals = pacf(df_stacionary_timeseries_for_pacf_S6, nlags=safe_lag, method=\"ywm\")\n",
                "        # Build list of significant PACF lags\n",
                "        significant_pacf_lags = []\n",
                "        for lag in range(1, len(pacf_vals)):\n",
                "            val = pacf_vals[lag]\n",
                "            if abs(val) > conf_limit:\n",
                "                significant_pacf_lags.append(lag)\n",
                "        if len(significant_pacf_lags) == 0:\n",
                "            suggested_p = 0\n",
                "            log(\"PACF analysis: No significant partial autocorrelation detected.\", level=1, type=\"INFO\")\n",
                "        else:\n",
                "            log(f\"PACF analysis: Significant PACF lags detected {significant_pacf_lags}.\", level=1, type=\"INFO\")\n",
                "            # Keep non-seasonal PACF lags\n",
                "            non_seasonal_pacf = []\n",
                "            for lag in significant_pacf_lags:\n",
                "                if (period_S6 is None) or (lag % period_S6 != 0):\n",
                "                    non_seasonal_pacf.append(lag)\n",
                "            if len(non_seasonal_pacf) > 0:\n",
                "                suggested_p = non_seasonal_pacf[0]\n",
                "                log(f\"PACF analysis: First significant non-seasonal lag = {suggested_p} ‚Üí candidate AR order p ‚âà {suggested_p}.\", level=1, type=\"INFO\")\n",
                "            else:\n",
                "                suggested_p = 0\n",
                "        # -------------------------------------------\n",
                "        # MODEL ORDER SUGGESTION (AR / MA)\n",
                "        # -------------------------------------------\n",
                "        if len(significant_acf_lags) > 0:\n",
                "            suggested_q = significant_acf_lags[0]\n",
                "        else:\n",
                "            suggested_q = 0\n",
                "        # Start from suggested values\n",
                "        candidate_orders = [(suggested_p, suggested_d, suggested_q)]\n",
                "        # If suggested_p = 0, also try p = 1 as alternative\n",
                "        if suggested_p == 0:\n",
                "            candidate_orders.append((1, suggested_d, suggested_q))\n",
                "        best_aic = np.inf\n",
                "        best_order = None\n",
                "        for (p_try, d_try, q_try) in candidate_orders:\n",
                "            try:\n",
                "                model_try = ARIMA(df_timeseries_S6, order=(p_try, d_try, q_try))\n",
                "                result_try = model_try.fit()\n",
                "                if result_try.aic < best_aic:\n",
                "                    best_aic = result_try.aic\n",
                "                    best_order = (p_try, d_try, q_try)\n",
                "            except Exception as e:\n",
                "                log(f\"ARIMA({p_try},{d_try},{q_try}) could not be fitted: {e}\", type=\"WARNING\")\n",
                "\n",
                "        if best_order is not None:\n",
                "            suggested_p, suggested_d, suggested_q = best_order\n",
                "            log(f\"Final ARIMA order suggestion: (p,d,q)=({suggested_p},{suggested_d},{suggested_q}) after AIC-checked refinement (best AIC={best_aic:.2f}).\", level=1, type=\"INFO\", bold=True)\n",
                "\n",
                "        # -------------------------------------------\n",
                "        # PLOT ACF\n",
                "        # -------------------------------------------\n",
                "        fig_acf, ax_acf = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "        plot_acf(df_timeseries_S6, lags=safe_lag, ax=ax_acf)\n",
                "        ax_acf.set_title(label=\"Autocorrelation Function (ACF)\", fontsize=plot_title_font_size)\n",
                "        ax_acf.set_xlabel(xlabel=\"Lag\", fontsize=plot_label_font_size)\n",
                "        ax_acf.set_ylabel(ylabel=\"Autocorrelation\", fontsize=plot_label_font_size)\n",
                "        ax_acf.tick_params(labelsize=plot_tick_font_size)\n",
                "        ax_acf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "        # Build manual legend to avoid statsmodels overwriting handles\n",
                "        handles, labels = [], []\n",
                "        # Highlight short-lag zone (trend indicator)\n",
                "        short_cutoff = min(get_short_lag_cutoff(granularity), safe_lag)\n",
                "        ax_acf.axvspan(\n",
                "            xmin=1,\n",
                "            xmax=short_cutoff,\n",
                "            color=\"lightblue\",\n",
                "            alpha=0.35,\n",
                "            label=f\"Short-lag zone (‚â§ {short_cutoff})\"\n",
                "        )\n",
                "        handles.append(plt.Line2D([0], [0], color=\"lightblue\", linewidth=10, alpha=0.35))\n",
                "        labels.append(f\"Short-lag zone (‚â§ {short_cutoff})\")\n",
                "        # Mark seasonal period (if applicable)\n",
                "        if (period_S6 is not None) and (period_S6 <= safe_lag):\n",
                "            ax_acf.axvline(\n",
                "                x=period_S6,\n",
                "                color=\"orange\",\n",
                "                linestyle=\"-\",\n",
                "                linewidth=4.0,\n",
                "                alpha=0.8,\n",
                "                label=f\"Seasonal period (lag={period_S6})\"\n",
                "            )\n",
                "            handles.append(plt.Line2D([0], [0], color=\"orange\", linewidth=4))\n",
                "            labels.append(f\"Seasonal period (lag={period_S6})\")\n",
                "        # Mark significant ACF lags (points)\n",
                "        if len(significant_acf_lags) > 0:\n",
                "            ax_acf.scatter(\n",
                "                significant_acf_lags,\n",
                "                [acf_vals[lag] for lag in significant_acf_lags],\n",
                "                color=\"blue\",\n",
                "                s=100,\n",
                "                label=\"Significant lags\"\n",
                "            )\n",
                "            handles.append(plt.Line2D([0], [0], marker=\"o\", color=\"blue\", linestyle=\"None\"))\n",
                "            labels.append(\"Significant lags\")\n",
                "        # Info text box\n",
                "        ax_acf.text(\n",
                "            0.98, 0.02,\n",
                "            f\"safe_lag = {safe_lag}\\n\"\n",
                "            f\"granularity = {granularity}\",\n",
                "            ha='right',\n",
                "            va='bottom',\n",
                "            transform=ax_acf.transAxes,\n",
                "            fontsize=plot_text_font_size,\n",
                "            bbox=dict(boxstyle=\"round\", fc=\"white\", alpha=0.6)\n",
                "        )\n",
                "        # Show legend and plot\n",
                "        ax_acf.legend(handles, labels, loc=\"upper right\", fontsize=plot_text_font_size)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "\n",
                "        # -------------------------------------------\n",
                "        # PLOT PACF\n",
                "        # -------------------------------------------\n",
                "        fig_pacf, ax_pacf = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "        plot_pacf(df_timeseries_S6, lags=safe_lag, ax=ax_pacf)\n",
                "        ax_pacf.set_title(label=\"Partial Autocorrelation Function (PACF)\", fontsize=plot_title_font_size)\n",
                "        ax_pacf.set_xlabel(xlabel=\"Lag\", fontsize=plot_label_font_size)\n",
                "        ax_pacf.set_ylabel(ylabel=\"Partial autocorrelation\", fontsize=plot_label_font_size)\n",
                "        ax_pacf.tick_params(labelsize=plot_tick_font_size)\n",
                "        ax_pacf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "        # Build manual legend to avoid statsmodels overwriting handles\n",
                "        handles, labels = [], []\n",
                "        # Mark significant PACF lags (points)\n",
                "        if len(significant_pacf_lags) > 0:\n",
                "            ax_pacf.scatter(\n",
                "                significant_pacf_lags,\n",
                "                [pacf_vals[lag] for lag in significant_pacf_lags],\n",
                "                color=\"blue\",\n",
                "                s=100,\n",
                "                label=\"Significant lags\"\n",
                "            )\n",
                "            handles.append(plt.Line2D([0], [0], marker=\"o\", color=\"blue\", linestyle=\"None\"))\n",
                "            labels.append(\"Significant lags\")\n",
                "        # Info text box\n",
                "        ax_pacf.text(\n",
                "            0.98, 0.02,\n",
                "            f\"safe_lag = {safe_lag}\",\n",
                "            ha='right',\n",
                "            va='bottom',\n",
                "            transform=ax_pacf.transAxes,\n",
                "            fontsize=plot_text_font_size,\n",
                "            bbox=dict(boxstyle=\"round\", fc=\"white\", alpha=0.6)\n",
                "        )\n",
                "        # Show legend and plot\n",
                "        ax_pacf.legend(handles, labels, loc=\"upper right\", fontsize=plot_text_font_size)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Prepare aligned NUMERIC matrix for global plots / metrics\n",
                "    X_numeric_S6 = None\n",
                "    y_aligned_S6 = None\n",
                "    if supervised_learning and (len(numeric_att) > 0) and (y_var in df_S6.columns):\n",
                "        X_numeric_S6 = df_S6[numeric_att].dropna()\n",
                "        if len(X_numeric_S6) > 0:\n",
                "            y_aligned_S6 = df_S6.loc[X_numeric_S6.index, y_var]\n",
                "\n",
                "    # -------------------------------------------\n",
                "    # GLOBAL 2D / 3D SCATTER (KNN intuition)\n",
                "    # -------------------------------------------\n",
                "    if make_plots_GLOBAL_SCATTER:\n",
                "        if not supervised_learning:\n",
                "            # Unsupervised version ‚Üí no hue=y_var\n",
                "            if (len(numeric_att) >= 2):\n",
                "                print(\"\\n üîç GLOBAL 2D / 3D SCATTER (UNSUPERVISED)\")\n",
                "                x_col = numeric_att[0]\n",
                "                y_col = numeric_att[1]\n",
                "                fig, ax = plt.subplots(figsize=(figWidth_unit, figHeight_unit))\n",
                "                ax.scatter(df_S6[x_col], df_S6[y_col], alpha=0.7)\n",
                "                ax.set_xlabel(x_col)\n",
                "                ax.set_ylabel(y_col)\n",
                "                ax.set_title(\"GLOBAL 2D scatter (unsupervised)\")\n",
                "                ax.grid(True)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "        else:\n",
                "            if (X_numeric_S6 is not None) and (X_numeric_S6.shape[1] >= 2):\n",
                "                print(\"\\n üîç GLOBAL 2D / 3D SCATTER (NUMERIC attributes)\")\n",
                "                num_features_scatter = X_numeric_S6.shape[1]\n",
                "                feature_names_scatter = X_numeric_S6.columns.tolist()\n",
                "\n",
                "                # 2D scatter (first two NUMERIC attributes)\n",
                "                x_col = feature_names_scatter[0]\n",
                "                y_col = feature_names_scatter[1]\n",
                "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(figWidth_unit, figHeight_unit))\n",
                "                if y_var_type == \"CATEGORIC\":\n",
                "                    sns.scatterplot(ax=ax, data=df_S6, x=x_col, y=y_col, hue=y_var, palette=plot_palette, alpha=0.7)\n",
                "                else:\n",
                "                    scatter = ax.scatter(df_S6[x_col], df_S6[y_col], c=df_S6[y_var], alpha=0.7)\n",
                "                    cbar = plt.colorbar(scatter, ax=ax)\n",
                "                    cbar.set_label(y_var)\n",
                "                ax.set_xlabel(x_col, fontsize=plot_label_font_size)\n",
                "                ax.set_ylabel(y_col, fontsize=plot_label_font_size)\n",
                "                ax.set_title(\"GLOBAL 2D scatter (first 2 NUMERIC attributes)\", fontsize=plot_title_font_size)\n",
                "                ax.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "                # 3D scatter (first three NUMERIC attributes)\n",
                "                if num_features_scatter >= 3:\n",
                "                    z_col = feature_names_scatter[2]\n",
                "                    fig = plt.figure(figsize=(figWidth_unit * 1.5, figHeight_unit * 1.5))\n",
                "                    ax3d = fig.add_subplot(111, projection=\"3d\")\n",
                "                    if y_var_type == \"CATEGORIC\":\n",
                "                        unique_classes = df_S6[y_var].dropna().unique().tolist()\n",
                "                        for cls in unique_classes:\n",
                "                            mask = df_S6[y_var] == cls\n",
                "                            ax3d.scatter(df_S6.loc[mask, x_col], df_S6.loc[mask, y_col], df_S6.loc[mask, z_col], label=str(cls), alpha=0.7)\n",
                "                        ax3d.legend(title=y_var)\n",
                "                    else:\n",
                "                        scatter3d = ax3d.scatter(df_S6[x_col], df_S6[y_col], df_S6[z_col], c=df_S6[y_var], alpha=0.7)\n",
                "                        cbar = plt.colorbar(scatter3d, ax=ax3d, pad=0.1)\n",
                "                        cbar.set_label(y_var)\n",
                "                    ax3d.set_xlabel(x_col)\n",
                "                    ax3d.set_ylabel(y_col)\n",
                "                    ax3d.set_zlabel(z_col)\n",
                "                    ax3d.set_title(\"GLOBAL 3D scatter (first 3 NUMERIC attributes)\")\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "    # -------------------------------------------\n",
                "    # GLOBAL PCA 2D / 3D (KNN intuition)\n",
                "    # -------------------------------------------\n",
                "    if make_plots_GLOBAL_PCA:\n",
                "        if not supervised_learning:\n",
                "            # Unsupervised PCA\n",
                "            if len(numeric_att) >= 2:\n",
                "                print(\"\\n üîç PCA 2D (UNSUPERVISED)\")\n",
                "                scaler = StandardScaler()\n",
                "                X_scaled = scaler.fit_transform(df_S6[numeric_att])\n",
                "                pca_2d = PCA(n_components=2, random_state=random_seed)\n",
                "                X_pca = pca_2d.fit_transform(X_scaled)\n",
                "                fig, ax = plt.subplots(figsize=(figWidth_unit, figHeight_unit))\n",
                "                ax.scatter(X_pca[:,0], X_pca[:,1], alpha=0.7)\n",
                "                ax.set_title(\"PCA 2D projection (unsupervised)\")\n",
                "                ax.set_xlabel(\"PC1\")\n",
                "                ax.set_ylabel(\"PC2\")\n",
                "                ax.grid(True)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "        else:\n",
                "            if (X_numeric_S6 is not None) and (X_numeric_S6.shape[1] >= 2):\n",
                "                print(\"\\n üîç PCA 2D / 3D (NUMERIC attributes)\")\n",
                "                scaler = StandardScaler()\n",
                "                X_scaled = scaler.fit_transform(X_numeric_S6)\n",
                "\n",
                "                # PCA 2D\n",
                "                pca_2d = PCA(n_components=2, random_state=random_seed)\n",
                "                X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
                "                df_pca_2d = pd.DataFrame(X_pca_2d, columns=[\"PC1\", \"PC2\"])\n",
                "                df_pca_2d[y_var] = y_aligned_S6.values\n",
                "\n",
                "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(figWidth_unit, figHeight_unit))\n",
                "                if y_var_type == \"CATEGORIC\":\n",
                "                    sns.scatterplot(ax=ax, data=df_pca_2d, x=\"PC1\", y=\"PC2\", hue=y_var, palette=plot_palette, alpha=0.7)\n",
                "                else:\n",
                "                    scatter = ax.scatter(df_pca_2d[\"PC1\"], df_pca_2d[\"PC2\"], c=df_pca_2d[y_var], alpha=0.7)\n",
                "                    cbar = plt.colorbar(scatter, ax=ax)\n",
                "                    cbar.set_label(y_var)\n",
                "                ax.set_xlabel(\"PC1\", fontsize=plot_label_font_size)\n",
                "                ax.set_ylabel(\"PC2\", fontsize=plot_label_font_size)\n",
                "                ax.set_title(\"PCA 2D projection (NUMERIC attributes)\", fontsize=plot_title_font_size)\n",
                "                ax.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "                # PCA 3D\n",
                "                if X_numeric_S6.shape[1] >= 3:\n",
                "                    pca_3d = PCA(n_components=3, random_state=random_seed)\n",
                "                    X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
                "                    df_pca_3d = pd.DataFrame(X_pca_3d, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
                "                    df_pca_3d[y_var] = y_aligned_S6.values\n",
                "\n",
                "                    fig = plt.figure(figsize=(figWidth_unit * 1.5, figHeight_unit * 1.5))\n",
                "                    ax3d = fig.add_subplot(111, projection=\"3d\")\n",
                "                    if y_var_type == \"CATEGORIC\":\n",
                "                        unique_classes = df_pca_3d[y_var].dropna().unique().tolist()\n",
                "                        for cls in unique_classes:\n",
                "                            mask = df_pca_3d[y_var] == cls\n",
                "                            ax3d.scatter(df_pca_3d.loc[mask, \"PC1\"], df_pca_3d.loc[mask, \"PC2\"], df_pca_3d.loc[mask, \"PC3\"], label=str(cls), alpha=0.7)\n",
                "                        ax3d.legend(title=y_var)\n",
                "                    else:\n",
                "                        scatter3d = ax3d.scatter(df_pca_3d[\"PC1\"], df_pca_3d[\"PC2\"], df_pca_3d[\"PC3\"], c=df_pca_3d[y_var], alpha=0.7)\n",
                "                        cbar = plt.colorbar(scatter3d, ax=ax3d, pad=0.1)\n",
                "                        cbar.set_label(y_var)\n",
                "                    ax3d.set_xlabel(\"PC1\")\n",
                "                    ax3d.set_ylabel(\"PC2\")\n",
                "                    ax3d.set_zlabel(\"PC3\")\n",
                "                    ax3d.set_title(\"PCA 3D projection (NUMERIC attributes)\")\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "    # -------------------------------------------\n",
                "    # GEOMETRIC SEPARABILITY METRICS\n",
                "    # (Fisher score + Silhouette in PCA 2D)\n",
                "    # -------------------------------------------\n",
                "    if compute_GLOBAL_GEOMETRY_METRICS and (X_numeric_S6 is not None) and (y_var_type == \"CATEGORIC\") and (y_aligned_S6 is not None):\n",
                "        print(\"\\n üìè GEOMETRIC SEPARABILITY METRICS (TABULAR)\")\n",
                "\n",
                "        # Fisher scores (one per NUMERIC attribute)\n",
                "        def compute_fisher_scores(X_df, y_series):\n",
                "            scores = {}\n",
                "            overall_means = X_df.mean(axis=0)\n",
                "            classes = y_series.dropna().unique().tolist()\n",
                "            for col in X_df.columns:\n",
                "                num = 0.0\n",
                "                den = 0.0\n",
                "                for cls in classes:\n",
                "                    mask = y_series == cls\n",
                "                    x_c = X_df.loc[mask, col]\n",
                "                    if len(x_c) == 0:\n",
                "                        continue\n",
                "                    n_c = len(x_c)\n",
                "                    mean_c = x_c.mean()\n",
                "                    var_c = x_c.var()\n",
                "                    num += n_c * (mean_c - overall_means[col])**2\n",
                "                    den += n_c * var_c\n",
                "                if den == 0:\n",
                "                    scores[col] = 0.0\n",
                "                else:\n",
                "                    scores[col] = num / den\n",
                "            return pd.Series(scores).sort_values(ascending=False)\n",
                "\n",
                "        fisher_scores_S6 = compute_fisher_scores(X_numeric_S6, y_aligned_S6)\n",
                "        log(\"Fisher scores per NUMERIC attribute (higher = better class separability):\", type=\"INFO\")\n",
                "        display(fisher_scores_S6.to_frame(name=\"Fisher_score\"))\n",
                "\n",
                "        # Silhouette score on PCA(2D)\n",
                "        if X_numeric_S6.shape[1] >= 2 and y_aligned_S6.nunique() >= 2:\n",
                "            scaler_sil = StandardScaler()\n",
                "            X_scaled_sil = scaler_sil.fit_transform(X_numeric_S6)\n",
                "            pca_sil = PCA(n_components=2, random_state=random_seed)\n",
                "            X_pca_sil = pca_sil.fit_transform(X_scaled_sil)\n",
                "            sil_value_S6 = silhouette_score(X_pca_sil, y_aligned_S6)\n",
                "            log(f\"Silhouette score on PCA(2) space: {sil_value_S6:.3f} (‚âà quality of geometric class separability)\", type=\"INFO\", bold=True)\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED MODE ‚Üí Target-based multivariant plots skipped\", type=\"WARNING\")\n",
                "    else:\n",
                "        if not make_plots_MULTIVARIANT_TARGET:\n",
                "            log(\"MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET is not printed, set make_plots_MULTIVARIANT_TARGET = True\", type=\"WARNING\")\n",
                "        else:\n",
                "            # -------------------------------------------\n",
                "            # NUMERIC Attributes VS CATEGORY Target\n",
                "            # -------------------------------------------\n",
                "            print(\"\\n üî¢ NUMERIC Attributes VS üè∑Ô∏è CATEGORY Target\")\n",
                "            if not numeric_att:\n",
                "                log(\"This type of plot is non applicable because there are not NUMERIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "            elif y_var_type == \"NUMERIC\":\n",
                "                log(\"This type of plot is non applicable because Target variable is NUMERIC\", type=\"WARNING\")\n",
                "            else:\n",
                "                # Set plotting variables\n",
                "                var_to_plot = numeric_att\n",
                "                # Figure\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
                "                fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols*2, figsize=(figWidth_unit*num_cols, figHeight_unit*num_rows), gridspec_kw={'width_ratios': [3, 1] * num_cols})\n",
                "                var_idx = 0\n",
                "                for row in range(num_rows):\n",
                "                    for col in range(num_cols):\n",
                "                        if var_idx >= len(var_to_plot):\n",
                "                            # Hide unused subplots\n",
                "                            axes[row, col * 2].set_visible(False)\n",
                "                            axes[row, col * 2 + 1].set_visible(False)\n",
                "                            continue\n",
                "                        # Stripplot (left)\n",
                "                        sns.stripplot(ax=axes[row, col*2], data=df_S6, x=y_var, y=var_to_plot[var_idx], hue=y_var, alpha=0.3, legend=False)\n",
                "                        axes[row, col * 2].set_ylabel(var_to_plot[var_idx], fontdict={\"fontsize\": plot_label_font_size})\n",
                "                        axes[row, col * 2].grid(True)\n",
                "                        # Boxplot (right)\n",
                "                        sns.boxplot(ax=axes[row, col*2 + 1], data=df_S6, x=y_var, y=var_to_plot[var_idx], hue=y_var, palette=plot_palette, legend=False)\n",
                "                        axes[row, col * 2 + 1].set_ylabel(\"\")\n",
                "                        axes[row, col * 2 + 1].grid(True)\n",
                "                        axes[row, col * 2 + 1].set_yticklabels([])\n",
                "                        var_idx += 1\n",
                "                # Adjust layout\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "            # -------------------------------------------\n",
                "            # NUMERIC Attributes VS NUMERIC Target\n",
                "            # -------------------------------------------    \n",
                "            print(\"\\n üî¢ NUMERIC Attributes VS üî¢ NUMERIC Target\")\n",
                "            if not numeric_att:\n",
                "                log(\"This type of plot is non applicable because there are not NUMERIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "            elif y_var_type == \"CATEGORIC\":\n",
                "                log(\"This type of plot is non applicable because Target variable is CATEGORIC\", type=\"WARNING\")\n",
                "            else:\n",
                "                # Set plotting variables\n",
                "                var_to_plot = numeric_att\n",
                "                # Figure\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
                "                fig, axes = plt.subplots(nrows=num_rows*2, ncols=num_cols, figsize=(figWidth_unit*num_cols, figHeight_unit*num_rows), gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
                "                var_idx = 0\n",
                "                for row in range(num_rows):\n",
                "                    for col in range(num_cols):\n",
                "                        if var_idx >= len(var_to_plot):\n",
                "                            # Hide unused subplots\n",
                "                            axes[row * 2, col].set_visible(False)\n",
                "                            axes[row * 2 + 1, col].set_visible(False)\n",
                "                            continue\n",
                "                        # Regplot (top)\n",
                "                        sns.regplot(ax=axes[row*2, col], data=df_S6, x=var_to_plot[var_idx], y=y_var, scatter_kws={'s': plot_text_font_size, 'alpha': 0.6}, line_kws={'color': 'red'})\n",
                "                        # Boxplot (bottom)\n",
                "                        sns.heatmap(ax=axes[row*2 + 1, col], data=df_S6[[var_to_plot[var_idx], y_var]].corr(), annot=True, fmt=\".2f\", cbar=False)\n",
                "                        var_idx += 1\n",
                "                # Adjust layout\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "            # -------------------------------------------\n",
                "            # CATEGORY Attributes VS NUMERIC Target\n",
                "            # -------------------------------------------  \n",
                "            print(\"\\n üè∑Ô∏è CATEGORY Attributes VS üî¢ NUMERIC Target\")\n",
                "            if not category_att:\n",
                "                log(\"This type of plot is non applicable because there are not CATEGORIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "            elif y_var_type == \"CATEGORIC\":\n",
                "                log(\"This type of plot is non applicable because Target variable is CATEGORIC\", type=\"WARNING\")\n",
                "            else:        \n",
                "                # Set plotting variables\n",
                "                var_to_plot = category_att\n",
                "                # Figure\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
                "                fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(figWidth_unit*num_cols, figHeight_unit*num_rows))\n",
                "                axes = axes.flatten()\n",
                "                idx = 0\n",
                "                for col in var_to_plot:\n",
                "                    # Count unique values\n",
                "                    unique_count = df_S6[col].nunique()\n",
                "                    # Limit the number of plotted categories if there are more than num_values_to_plot\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
                "                    else:\n",
                "                        order = df_S6[col].value_counts().index\n",
                "                    # Barplot\n",
                "                    sns.barplot(ax=axes[idx], data=df_S6, x=col, y=y_var, hue=category_combi_att, order=order)\n",
                "                    axes[idx].tick_params(axis='x', rotation=90, labelsize=10)\n",
                "                    # Add text box if truncated\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
                "                        axes[idx].text(0.5, 0.9, s=msg, transform=axes[idx].transAxes, fontsize=plot_text_font_size, color='red', ha='center', va='top', bbox=dict(facecolor='grey', alpha=0.5, edgecolor='red'))\n",
                "                    idx += 1\n",
                "                # Turn off unused axes if there are any\n",
                "                for j in range(idx, len(axes)):\n",
                "                    axes[j].set_visible(False)\n",
                "                # Adjust layout and display\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "            # -------------------------------------------\n",
                "            # CATEGORY Attributes VS Combined CATEGORY Target\n",
                "            # -------------------------------------------  \n",
                "            print(\"\\n üè∑Ô∏è CATEGORY Attributes with üè∑Ô∏è Combined CATEGORY Target\")\n",
                "            if not category_att:\n",
                "                log(\"This type of plot is non applicable because there are not CATEGORIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "            elif y_var_type == \"NUMERIC\":\n",
                "                log(\"This type of plot is non applicable because Target variable is NUMERIC\", type=\"WARNING\")\n",
                "            else:\n",
                "                # Set plotting variables\n",
                "                var_to_plot = category_att\n",
                "                # Determine hue order dynamically\n",
                "                hue_order = sorted(df_S6[y_var].dropna().unique().tolist()) \n",
                "                # Figure\n",
                "                num_cols = 2\n",
                "                num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
                "                fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(figWidth_unit*num_cols, figHeight_unit*num_rows))\n",
                "                axes = axes.flatten()\n",
                "                idx = 0\n",
                "                for col in var_to_plot:\n",
                "                    # Count unique values\n",
                "                    unique_count = df_S6[col].nunique()\n",
                "                    # Limit the number of plotted categories if there are more than num_values_to_plot\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
                "                    else:\n",
                "                        order = df_S6[col].value_counts().index\n",
                "                    # Countplot\n",
                "                    sns.countplot(ax=axes[idx], data=df_S6, x=col, hue=y_var, hue_order=hue_order, palette=plot_palette, order=order)\n",
                "                    axes[idx].tick_params(axis='x', rotation=90, labelsize=plot_tick_font_size)\n",
                "                    # Add text box if truncated\n",
                "                    if unique_count > num_values_to_plot:\n",
                "                        msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
                "                        axes[idx].text(0.5, 0.9, s=msg, transform=axes[idx].transAxes, fontsize=plot_text_font_size, color='red', ha='center', va='top', bbox=dict(facecolor='grey', alpha=0.25, edgecolor='red'))\n",
                "                    idx += 1\n",
                "                # Turn off unused axes if there are any\n",
                "                for j in range(idx, len(axes)):\n",
                "                    axes[j].set_visible(False)\n",
                "                # Adjust layout and display\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "\n",
                "    if not make_plots_MULTIVARIANT_ATTRIBUTES:\n",
                "        log(\"MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES is not printed, set make_plots_MULTIVARIANT_ATTRIBUTES = True\", type=\"WARNING\")\n",
                "    else:\n",
                "        # -------------------------------------------\n",
                "        # NUMERIC Attributes VS NUMERIC Attributes\n",
                "        # -------------------------------------------  \n",
                "        print(\"\\n üî¢ NUMERIC Attributes VS üî¢ NUMERIC Attributes\")\n",
                "        if not category_att:\n",
                "            log(\"This type of plot is non applicable because there are not CATEGORIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "        else:\n",
                "            # Set plotting variables\n",
                "            var_to_plot = numeric_att\n",
                "            # Figure\n",
                "            num_cols = 2\n",
                "            num_rows = len(var_to_plot) - 1  # Number of rows (one less than number of variables)\n",
                "            fig, axes = plt.subplots(nrows=num_rows*2, ncols=len(var_to_plot) - 1, figsize=(figWidth_unit*(len(var_to_plot) - 1), figHeight_unit*num_rows), gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
                "            axes = np.array(axes)\n",
                "            # Track subplot usage\n",
                "            for row in range(num_rows):\n",
                "                n_cols = len(var_to_plot) - row - 1  # Decreasing number of columns each row\n",
                "                for col in range(n_cols):\n",
                "                    # Regplot (top)\n",
                "                    sns.regplot(ax=axes[row*2, col], data=df_S6, x=var_to_plot[row + col + 1], y=var_to_plot[row], scatter_kws={'s': plot_text_font_size, 'alpha': 0.6}, line_kws={'color': 'red'})\n",
                "                    axes[row * 2, col].set_xlabel(var_to_plot[row + col + 1], fontsize=20)\n",
                "                    # Show Y label only for first plot in row\n",
                "                    if col == 0:\n",
                "                        axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=plot_label_font_size)\n",
                "                    else:\n",
                "                        axes[row * 2, col].set_ylabel(\"\")\n",
                "                    # Heatmap (bottom)\n",
                "                    sns.heatmap(ax=axes[row*2 + 1, col], data=df_S6[[var_to_plot[row + col + 1], var_to_plot[row]]].corr(), annot=True, fmt=\".2f\", cbar=False, annot_kws={\"size\": 20})\n",
                "                # Hide unused subplots on the right for this row\n",
                "                for col in range(n_cols, len(var_to_plot) - 1):\n",
                "                    axes[row * 2, col].set_visible(False)\n",
                "                    axes[row * 2 + 1, col].set_visible(False)\n",
                "            # Adjust layout and show\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "\n",
                "        print(\"\\n üè∑Ô∏èüî¢ ALL Attributes VS üè∑Ô∏èüî¢ ALL Attributes\")\n",
                "        # Encode categorical variables using the Series.factorize() method\n",
                "        for col in category_att:\n",
                "            codes, uniques = df_S6[col].factorize()\n",
                "            df_S6[col] = codes  # replace column with integer codes\n",
                "        # Heatmap (CATEGORIC Attributes)\n",
                "        if len(category_att) > 1:\n",
                "            corr_cat = df_S6[category_att].corr()\n",
                "            fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
                "            plt.title(\"ONLY CATEGORIC ATTRIBUTES\", fontsize=plot_title_font_size, fontweight=\"bold\")\n",
                "            sns.heatmap(data=corr_cat, annot=True, vmin=-1, vmax=1, fmt=\".2f\", annot_kws={\"size\": plot_text_font_size})\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "        # Heatmap (NUMERIC Attributes)\n",
                "        if len(numeric_att) > 1:\n",
                "            corr_num = df_S6[numeric_att].corr()\n",
                "            fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
                "            plt.title(\"ONLY NUMERIC ATTRIBUTES\", fontsize=plot_title_font_size + 2, fontweight=\"bold\")\n",
                "            sns.heatmap(data=corr_num, annot=True, vmin=-1, vmax=1, fmt=\".2f\", annot_kws={\"size\": plot_text_font_size})\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "        # Heatmap (CATEGORIC + NUMERIC Attributes)\n",
                "        corr_matrix = df_S6[numeric_att + category_att].corr()\n",
                "        corr_order = corr_matrix.mean().sort_values(ascending=False).index\n",
                "        if len(category_att) > 1 and len(numeric_att) > 1:\n",
                "            corr_matrix = corr_matrix.loc[corr_order, corr_order]\n",
                "            fig = plt.figure(figsize=(2 * figWidth_unit, 2 * figHeight_unit))\n",
                "            plt.title(\"CATEGORIC AND NUMERIC ATTRIBUTES\", fontsize=plot_title_font_size + 2, fontweight=\"bold\")\n",
                "            sns.heatmap(data=corr_matrix, annot=True, vmin=-1, vmax=1, fmt=\".2f\", annot_kws={\"size\": plot_text_font_size})\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "        # Pairplot (sorted by correlation order)\n",
                "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
                "        sns.pairplot(data=df_S6[corr_order])\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 7"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for NLP (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text lemmatization\n",
                "apply_stopword_removal = True                   # Remove stopwords during lemmatization\n",
                "stopword_language = \"english\"                   # Language for stopword list\n",
                "case_sensitive_stopwords = False                # If False ‚Üí stopwords compared in lowercase\n",
                "apply_pos_tagging = True                       # If True ‚Üí use NLTK POS tagging (more accurate but slower)\n",
                "exclude_digits = True                           # Remove tokens composed only by digits\n",
                "min_word_length = 3                             # Minimum length of tokens after lemmatization\n",
                "# Wordcloud plot\n",
                "mask_path = \"../data/raw/comment.png\"           # Folder where .png mask (locates words in space) is saved"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "zero_to_nan = []                    # List of attributes where zero should be considered missing\n",
                "filling_threshold = 5.0             # [%] If missing perc > filling_threshold ‚Üí fill values, otherwise drop rows\n",
                "grouping_max_unique = 6             # Max number of unique values for a categorical attribute to be usable as keys for grouped median\n",
                "make_missing_values_plots = True    # Make plots?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S7 = df_S6.copy()\n",
                "main_text_col_S7 = main_text_col_S6 if dataset_type == \"NLP\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 7) TEXT LEMMATIZATION - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 7 - MISSING VALUES - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 7 - MISSING VALUES - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # Remove missing values from target variable\n",
                "    if supervised_learning:\n",
                "        missing_y = df_S7[y_var].isnull().sum()\n",
                "        if missing_y > 0:\n",
                "            log(f\"Target variable '{y_var}' contains {missing_y} missing values ‚Üí rows will be dropped\", type=\"WARNING\")\n",
                "            df_S7 = df_S7.dropna(subset=[y_var])\n",
                "        else:\n",
                "            log(f\"Target variable '{y_var}' has no missing values\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"UNSUPERVISED MODE ‚Üí No target-variable cleaning applied\", type=\"INFO\")\n",
                "    # Apply lemmatization to each list of tokens\n",
                "    df_S7[main_text_col_S7] = df_S7[main_text_col_S7].apply(\n",
                "        lambda token_list: lemmatize_tokens(\n",
                "            token_list,\n",
                "            apply_stopword_removal=apply_stopword_removal,\n",
                "            stopword_language=stopword_language,\n",
                "            case_sensitive_stopwords=case_sensitive_stopwords,\n",
                "            apply_pos_tagging=apply_pos_tagging,\n",
                "            exclude_digits=exclude_digits,\n",
                "            min_word_length=min_word_length))\n",
                "    log(\"Lemmatization completed successfully:\", type=\"SUCCESS\")\n",
                "    log(f\"Processed column: '{main_text_col_S7}'\", type=\"INFO\")\n",
                "    display(df_S7.head(10))\n",
                "    # ---------------------------\n",
                "    # WordCloud plotting\n",
                "    # ---------------------------\n",
                "    # Build full text corpus\n",
                "    all_words = []\n",
                "    for row in df_S7[main_text_col_S7]:\n",
                "        for tok in row:\n",
                "            all_words.append(tok)\n",
                "    if len(all_words) == 0:\n",
                "        log(\"WordCloud skipped: no tokens available after preprocessing\", type=\"WARNING\")\n",
                "    else:\n",
                "        text_for_wc = \" \".join(all_words)\n",
                "        # Load mask\n",
                "        wordcloud_mask = np.array(Image.open(mask_path))\n",
                "        # Generate WordCloud\n",
                "        wordcloud = WordCloud(\n",
                "            width = figWidth_unit*500,\n",
                "            height = figHeight_unit*500,\n",
                "            mask = wordcloud_mask,\n",
                "            colormap = \"viridis\",\n",
                "            background_color = \"black\",\n",
                "            mode = \"RGBA\",\n",
                "            max_words = 1000,\n",
                "            random_state = random_seed\n",
                "        ).generate(text_for_wc)\n",
                "        # Plot with grayscale recoloring\n",
                "        fig = plt.figure(figsize=(figWidth_unit*2, figHeight_unit*2), facecolor=\"black\")\n",
                "        # Inline grayscale color function\n",
                "        grey_lambda = lambda *args, **kwargs: \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
                "        plt.imshow(wordcloud.recolor(color_func=grey_lambda, random_state=random_seed), interpolation=\"bilinear\")\n",
                "        plt.show()\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================      \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Remove missing values from target variable\n",
                "    if supervised_learning:\n",
                "        missing_y = df_S7[y_var].isnull().sum()\n",
                "        if missing_y > 0:\n",
                "            log(f\"Target variable '{y_var}' contains {missing_y} missing values ‚Üí rows will be dropped\", type=\"WARNING\")\n",
                "            df_S7 = df_S7.dropna(subset=[y_var])\n",
                "        else:\n",
                "            log(f\"Target variable '{y_var}' has no missing values\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"UNSUPERVISED MODE ‚Üí No target-variable cleaning applied\", type=\"INFO\")\n",
                "    # Print results\n",
                "    log(f\"Previous df's rows: {len(df_S6)}\", type=\"INFO\")\n",
                "    log(f\"Current df's rows: {len(df_S7)}\", type=\"INFO\")\n",
                "    log(f\"Current df's shape: {df_S7.shape}\", type=\"INFO\")\n",
                "    log(f\"Remaining missing values per column:\\n{df_S7.isnull().sum()}\", type=\"INFO\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Remove missing values from target variable\n",
                "    if supervised_learning:\n",
                "        missing_y = df_S7[y_var].isnull().sum()\n",
                "        if missing_y > 0:\n",
                "            log(f\"Target variable '{y_var}' contains {missing_y} missing values ‚Üí rows will be dropped\", type=\"WARNING\")\n",
                "            df_S7 = df_S7.dropna(subset=[y_var])\n",
                "        else:\n",
                "            log(f\"Target variable '{y_var}' has no missing values\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        log(\"UNSUPERVISED MODE ‚Üí no target-variable cleaning applied\", type=\"INFO\")\n",
                "    # Replace zeros by NaN for selected columns\n",
                "    for col in zero_to_nan:\n",
                "        if col in df_S7.columns:\n",
                "            df_S7[col] = df_S7[col].replace(0, np.nan)\n",
                "            log(f\"Values equal to 0 in '{col}' have been replaced by NaN\", type=\"WARNING\")\n",
                "    # Identify categorical variables usable as grouping keys for numeric imputation\n",
                "    group_vars = []\n",
                "    if supervised_learning:\n",
                "        # Normal categorical attributes\n",
                "        for col in category_att:\n",
                "            if df_S7[col].nunique() <= grouping_max_unique:\n",
                "                group_vars.append(col)\n",
                "        # Add target as grouping variable if it is CATEGORICAL and has few unique values\n",
                "        if y_var_type == \"CATEGORIC\":\n",
                "            if df_S7[y_var].nunique() <= grouping_max_unique:\n",
                "                group_vars.append(y_var)\n",
                "                log(f\"Target variable '{y_var}' added to grouping keys for numeric imputation\", type=\"INFO\")\n",
                "    else:\n",
                "        # Unsupervised ‚Üí only attribute-based grouping\n",
                "        for col in category_att:\n",
                "            if df_S7[col].nunique() <= grouping_max_unique:\n",
                "                group_vars.append(col)\n",
                "    # Calculate missing percentages per column\n",
                "    missing_pct = (df_S7.isnull().sum() / len(df_S7)) * 100\n",
                "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
                "    if len(missing_pct) == 0:\n",
                "        log(\"DataFrame has no missing values at all (excluding target variable already handled)\", type=\"SUCCESS\")\n",
                "    else:\n",
                "        # Process each column with missing values\n",
                "        for col in missing_pct.index:\n",
                "            pct = missing_pct[col]\n",
                "            log(f\"Column: {col} ‚Üí {pct:.2f}% missing\", type=\"WARNING\")\n",
                "            # CASE 1: NUMERIC ATTRIBUTE\n",
                "            if col in numeric_att:\n",
                "                # CASE 1A: grouped median\n",
                "                if pct > filling_threshold and len(group_vars) > 0:\n",
                "                    medians = df_S7.groupby(group_vars)[col].median().reset_index()\n",
                "                    medians = medians.rename(columns={col: f\"median_{col}\"})\n",
                "                    df_S7 = pd.merge(df_S7, medians, on=group_vars, how=\"left\")\n",
                "                    df_S7[col] = df_S7[col].fillna(df_S7[f\"median_{col}\"])\n",
                "                    df_S7 = df_S7.drop(columns=[f\"median_{col}\"])\n",
                "                    log(f\"FILLED missing numeric values in {col} using grouped median by {group_vars}\", level=2, type=\"WARNING\")\n",
                "                # CASE 1B: global median\n",
                "                elif pct > filling_threshold and len(group_vars) == 0:\n",
                "                    df_S7[col] = df_S7[col].fillna(df_S7[col].median())\n",
                "                    log(f\"FILLED missing numeric values in {col} using global median (no grouping columns)\", level=2, type=\"WARNING\")\n",
                "                # CASE 1C: drop rows\n",
                "                elif pct <= filling_threshold:\n",
                "                    df_S7 = df_S7.dropna(subset=[col])\n",
                "                    log(f\"DROPPED rows with missing values in {col} ({pct:.2f}% ‚â§ {filling_threshold}%)\", level=2, type=\"WARNING\")\n",
                "            # CASE 2: CATEGORICAL ATTRIBUTE ‚Üí mode imputation\n",
                "            elif col in category_att:\n",
                "                mode_value = df_S7[col].mode().iloc[0]\n",
                "                df_S7[col] = df_S7[col].fillna(mode_value)\n",
                "                log(f\"FILLED missing categorical values in {col} using mode (most frequent value)\", level=2, type=\"WARNING\")\n",
                "            # CASE 3: unsupported\n",
                "            else:\n",
                "                df_S7 = df_S7.dropna(subset=[col])\n",
                "                log(f\"DROPPED rows for missing values in {col} because it has unsupported type for imputation\", level=2, type=\"INFO\")\n",
                "    # Print results\n",
                "    log(f\"Previous df's rows: {len(df_S6)}\", type=\"INFO\")\n",
                "    log(f\"Current df's rows: {len(df_S7)}\", type=\"INFO\")\n",
                "    log(f\"Current df's shape: {df_S7.shape}\", type=\"INFO\")\n",
                "    log(f\"Remaining missing values per column:\\n{df_S7.isnull().sum()}\", type=\"INFO\")\n",
                "    if make_missing_values_plots:\n",
                "        # BEFORE vs AFTER missing values handling\n",
                "        print(\"\\nüìä VISUAL CHECK - BEFORE vs AFTER missing values handling\")\n",
                "        df_S7_before = df_S6.copy()   # Before missing-value handling\n",
                "        df_S7_after = df_S7.copy()    # After missing-value handling\n",
                "        if not numeric_att:\n",
                "            log(\"This type of plot is non applicable because there are not NUMERIC variables in the DataFrame\", type=\"WARNING\")\n",
                "        else:\n",
                "            # Set plotting variables\n",
                "            var_to_plot = numeric_att.copy()\n",
                "            if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
                "                var_to_plot.insert(0, y_var)\n",
                "            # Figure\n",
                "            num_cols = 2\n",
                "            num_rows = len(var_to_plot)\n",
                "            fig, axes = plt.subplots(nrows = num_rows * 2, ncols = num_cols, figsize = (figWidth_unit * num_cols, figHeight_unit * num_rows), gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
                "            for i, colname in enumerate(var_to_plot):\n",
                "                # Row indices for histogram and boxplot of this variable\n",
                "                hist_row  = i * 2\n",
                "                box_row   = i * 2 + 1\n",
                "                # Common bins (syncronize BEFORE and AFTER)\n",
                "                xmin = min(df_S7_before[colname].min(), df_S7_after[colname].min())\n",
                "                xmax = max(df_S7_before[colname].max(), df_S7_after[colname].max())\n",
                "                common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
                "                # ================\n",
                "                # BEFORE PLOTS\n",
                "                # ================\n",
                "                before_hist_ax = axes[hist_row, 0]\n",
                "                before_box_ax  = axes[box_row, 0]\n",
                "                sns.histplot(ax = before_hist_ax, data = df_S7_before, x = colname, bins = num_bins, color = \"gray\", alpha = 0.35)\n",
                "                before_hist_ax.set_title(colname + \" - BEFORE\")\n",
                "                before_hist_ax.set_xlabel(\"\")\n",
                "                sns.boxplot(ax = before_box_ax, data = df_S7_before, x = colname, color = \"lightgray\")\n",
                "                before_box_ax.set_xlabel(\"\")\n",
                "                # Store BEFORE axis limits\n",
                "                xlim_hist_before = before_hist_ax.get_xlim()\n",
                "                ylim_hist_before = before_hist_ax.get_ylim()\n",
                "                xlim_box_before  = before_box_ax.get_xlim()\n",
                "                # ================\n",
                "                # AFTER PLOTS\n",
                "                # ================\n",
                "                after_hist_ax = axes[hist_row, 1]\n",
                "                after_box_ax  = axes[box_row, 1]\n",
                "                sns.histplot( ax = after_hist_ax, data = df_S7_after, x = colname, bins = common_bins)\n",
                "                after_hist_ax.set_title(colname + \" - AFTER\")\n",
                "                after_hist_ax.set_xlabel(\"\")\n",
                "                sns.boxplot(ax = after_box_ax, data = df_S7_after, x = colname)\n",
                "                after_box_ax.set_xlabel(\"\")\n",
                "                # Syncronize axes limits\n",
                "                after_hist_ax.set_xlim(xlim_hist_before)\n",
                "                after_hist_ax.set_ylim(ylim_hist_before)\n",
                "                after_box_ax.set_xlim(xlim_box_before)\n",
                "            plt.tight_layout()\n",
                "            plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 8"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "removal_type = \"EXTREME OUTLIERS\"   # Removal logic type (NORMAL or EXTREME outliers)\n",
                "low_outliers_threshold = 2        # [%] Max percentage of lower outliers allowed to remove\n",
                "up_outliers_threshold = 2         # [% ]Max percentage of upper outliers allowed to remove\n",
                "make_outliers_plots = True          # Make plots?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S8 = df_S7.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 8 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 8 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 8 - CLEAN OUTLIERS - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP and TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\" or dataset_type == \"TIME-SERIES\":\n",
                "    log(f\"This step is non-applicable for {dataset_type} datasets\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Print info\n",
                "    display(df_S8.describe())\n",
                "    # Outliers detection is applied over numeric attributes\n",
                "    cols = []\n",
                "    for col in numeric_att:\n",
                "        if col in df_S8.columns.to_list():\n",
                "            cols.append(col)\n",
                "    # Initialize containers\n",
                "    lower_limits = []\n",
                "    upper_limits = []\n",
                "    n_outliers_lower = []\n",
                "    n_outliers_upper = []\n",
                "    pct_outliers_lower = []\n",
                "    pct_outliers_upper = []\n",
                "    extreme_lower_limits = []\n",
                "    extreme_upper_limits = []\n",
                "    n_extreme_outliers_lower = []\n",
                "    n_extreme_outliers_upper = []\n",
                "    pct_extreme_outliers_lower = []\n",
                "    pct_extreme_outliers_upper = []\n",
                "    for col in cols:\n",
                "        Q1 = df_S8[col].quantile(0.25)\n",
                "        Q3 = df_S8[col].quantile(0.75)\n",
                "        IQR = Q3 - Q1\n",
                "        # Calulate limits\n",
                "        lower = Q1 - 1.5 * IQR\n",
                "        upper = Q3 + 1.5 * IQR\n",
                "        extreme_lower = Q1 - 3 * IQR\n",
                "        extreme_upper = Q3 + 3 * IQR\n",
                "        # Count num of outliers\n",
                "        n_low = (df_S8[col] < lower).sum()\n",
                "        n_high = (df_S8[col] > upper).sum()\n",
                "        n_extreme_low = (df_S8[col] < extreme_lower).sum()\n",
                "        n_extreme_high = (df_S8[col] > extreme_upper).sum()\n",
                "        # Percentages of outliers\n",
                "        pct_low = (n_low / len(df_S8.index)) * 100\n",
                "        pct_high = (n_high / len(df_S8.index)) * 100\n",
                "        pct_extreme_low = (n_extreme_low / len(df_S8.index)) * 100\n",
                "        pct_extreme_high = (n_extreme_high / len(df_S8.index)) * 100\n",
                "        # Save limits\n",
                "        lower_limits.append(lower)\n",
                "        upper_limits.append(upper)\n",
                "        extreme_lower_limits.append(extreme_lower)\n",
                "        extreme_upper_limits.append(extreme_upper)\n",
                "        # Save num of outliers\n",
                "        n_outliers_lower.append(n_low)\n",
                "        n_outliers_upper.append(n_high)\n",
                "        n_extreme_outliers_lower.append(n_extreme_low)\n",
                "        n_extreme_outliers_upper.append(n_extreme_high)\n",
                "        # Save percentages of outliers\n",
                "        pct_outliers_lower.append(pct_low)\n",
                "        pct_outliers_upper.append(pct_high)\n",
                "        pct_extreme_outliers_lower.append(pct_extreme_low)\n",
                "        pct_extreme_outliers_upper.append(pct_extreme_high)\n",
                "    # Build DataFrame with all results\n",
                "    df_limits = pd.DataFrame([\n",
                "        lower_limits, upper_limits, n_outliers_lower, n_outliers_upper, pct_outliers_lower, pct_outliers_upper],\n",
                "        columns=cols,\n",
                "        index=[\"lower_limit\", \"upper_limit\", \"n_outliers_lower\", \"n_outliers_upper\", \"pct_outliers_lower\", \"pct_outliers_upper\"])\n",
                "    df_extreme_limits = pd.DataFrame([\n",
                "        extreme_lower_limits, extreme_upper_limits, n_extreme_outliers_lower, n_extreme_outliers_upper, pct_extreme_outliers_lower, pct_extreme_outliers_upper],\n",
                "        columns=cols,\n",
                "        index=[\"extreme_lower_limit\", \"extreme_upper_limit\", \"n_extreme_outliers_lower\", \"n_extreme_outliers_upper\", \"pct_extreme_outliers_lower\", \"pct_extreme_outliers_upper\"])\n",
                "    # Display results\n",
                "    display(df_limits)\n",
                "    display(df_extreme_limits)\n",
                "    # Outliers detection is applied over numeric attributes\n",
                "    cols = []\n",
                "    for col in numeric_att:\n",
                "        if col in df_S8.columns.to_list():\n",
                "            cols.append(col)\n",
                "    if removal_type == \"NORMAL OUTLIERS\":\n",
                "        # Loop through each column and apply filtering rules\n",
                "        for col in cols:\n",
                "            low_limit = df_limits.loc[\"lower_limit\", col]\n",
                "            high_limit = df_limits.loc[\"upper_limit\", col]\n",
                "            pct_low = df_limits.loc[\"pct_outliers_lower\", col]\n",
                "            pct_high = df_limits.loc[\"pct_outliers_upper\", col]\n",
                "            # Remove low outliers if below threshold\n",
                "            print(\"\\n\")\n",
                "            log(f\"Attribute '{col}':\", type=\"INFO\")\n",
                "            if pct_low == 0:\n",
                "                log(\"None lower outliers detected\", level=2, type=\"INFO\")\n",
                "            elif pct_low <= low_outliers_threshold:\n",
                "                df_S8 = df_S8[df_S8[col] >= low_limit]\n",
                "                log(f\"REMOVED lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\", level=2, type=\"WARNING\")\n",
                "            else:\n",
                "                log(f\"KEPT lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\", level=2, type=\"SUCCESS\")\n",
                "            # Remove high outliers if below threshold\n",
                "            if pct_high == 0:\n",
                "                log(\"None upper outliers detected\", level=2, type=\"INFO\")\n",
                "            elif pct_high <= up_outliers_threshold:\n",
                "                df_S8 = df_S8[df_S8[col] <= high_limit]\n",
                "                log(f\"REMOVED upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\", level=2, type=\"WARNING\")\n",
                "            else:\n",
                "                log(f\"KEPT upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\", level=2, type=\"SUCCESS\")\n",
                "    elif removal_type == \"EXTREME OUTLIERS\":\n",
                "        # Loop through each column and apply filtering rules\n",
                "        for col in cols:\n",
                "            low_limit = df_extreme_limits.loc[\"extreme_lower_limit\", col]\n",
                "            high_limit = df_extreme_limits.loc[\"extreme_upper_limit\", col]\n",
                "            pct_low = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", col]\n",
                "            pct_high = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", col]\n",
                "            # Remove low outliers if below threshold\n",
                "            print(\"\\n\")\n",
                "            log(f\"Attribute '{col}':\", type=\"INFO\")\n",
                "            if pct_low == 0:\n",
                "                log(\"None extreme lower outliers detected\", level=2, type=\"INFO\")\n",
                "            elif pct_low <= low_outliers_threshold:\n",
                "                df_S8 = df_S8[df_S8[col] >= low_limit]\n",
                "                log(f\"REMOVED extreme lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\", level=2, type=\"WARNING\")\n",
                "            else:\n",
                "                log(f\"KEPT extreme lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\", level=2, type=\"SUCCESS\")\n",
                "            # Remove high outliers if below threshold\n",
                "            if pct_high == 0:\n",
                "                log(\"None extreme upper outliers detected\", level=2, type=\"INFO\")\n",
                "            elif pct_high <= up_outliers_threshold:\n",
                "                df_S8 = df_S8[df_S8[col] <= high_limit]\n",
                "                log(f\"REMOVED extreme upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\", level=2, type=\"WARNING\")\n",
                "            else:\n",
                "                log(f\"KEPT extreme upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\", level=2, type=\"SUCCESS\")\n",
                "    # Print results\n",
                "    log(\"Outliers have been handled successfully:\", type=\"SUCCESS\")\n",
                "    log(f\"Previous df's rows: {len(df_S7)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current df's rows: {len(df_S8)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current DataFrame shape: {df_S8.shape}\", level=2, type=\"INFO\")\n",
                "    display(df_S8.describe())\n",
                "    if make_outliers_plots:\n",
                "        # BEFORE vs AFTER Outliers handling\n",
                "        print(\"\\nüìä VISUAL CHECK - BEFORE vs AFTER outliers handling\")\n",
                "        df_S8_before = df_S7.copy()   # Before missing-value handling\n",
                "        df_S8_after = df_S8.copy()    # After missing-value handling\n",
                "        if not numeric_att:\n",
                "            log(\"This type of plot is non applicable because there are not NUMERIC variables in the DataFrame\", type=\"WARNING\")\n",
                "        else:\n",
                "            # Set plotting variables\n",
                "            var_to_plot = numeric_att\n",
                "            # Figure\n",
                "            num_cols = 2\n",
                "            num_rows = len(var_to_plot)\n",
                "            fig, axes = plt.subplots(nrows = num_rows * 2, ncols = num_cols, figsize = (figWidth_unit * num_cols, figHeight_unit * num_rows), gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
                "            for i, colname in enumerate(var_to_plot):\n",
                "                # Row indices for histogram and boxplot of this variable\n",
                "                hist_row  = i * 2\n",
                "                box_row   = i * 2 + 1\n",
                "                # Set common bins (syncronize BEFORE and AFTER)\n",
                "                xmin = min(df_S8_before[colname].min(), df_S8_after[colname].min())\n",
                "                xmax = max(df_S8_before[colname].max(), df_S8_after[colname].max())\n",
                "                common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
                "                # Set colored area limits\n",
                "                normal_low = df_limits.loc[\"lower_limit\", colname]\n",
                "                normal_up  = df_limits.loc[\"upper_limit\", colname]\n",
                "                extreme_low = df_extreme_limits.loc[\"extreme_lower_limit\", colname]\n",
                "                extreme_up  = df_extreme_limits.loc[\"extreme_upper_limit\", colname]\n",
                "                # ================\n",
                "                # BEFORE PLOTS\n",
                "                # ================\n",
                "                before_hist_ax = axes[hist_row, 0]\n",
                "                before_box_ax  = axes[box_row, 0]\n",
                "                sns.histplot(ax = before_hist_ax, data = df_S8_before, x = colname, bins = num_bins, color = \"gray\", alpha = 0.35)\n",
                "                before_hist_ax.set_title(colname + \" - BEFORE\")\n",
                "                before_hist_ax.set_xlabel(\"\")\n",
                "                sns.boxplot(ax = before_box_ax, data = df_S8_before, x = colname, color = \"lightgray\")\n",
                "                before_box_ax.set_xlabel(\"\")\n",
                "                # Outlier count\n",
                "                pct_low_normal  = df_limits.loc[\"pct_outliers_lower\", colname]\n",
                "                pct_high_normal = df_limits.loc[\"pct_outliers_upper\", colname]\n",
                "                pct_low_extreme  = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", colname]\n",
                "                pct_high_extreme = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", colname]\n",
                "                # NORMAL Outliers\n",
                "                if pct_low_normal > 0:\n",
                "                    before_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
                "                    before_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
                "                if pct_high_normal > 0:\n",
                "                    before_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
                "                    before_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
                "                # EXTREME Outliers\n",
                "                if pct_low_extreme > 0:\n",
                "                    before_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
                "                    before_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
                "                if pct_high_extreme > 0:\n",
                "                    before_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
                "                    before_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
                "                # Store BEFORE limits\n",
                "                xlim_hist_before = before_hist_ax.get_xlim()\n",
                "                ylim_hist_before = before_hist_ax.get_ylim()\n",
                "                xlim_box_before  = before_box_ax.get_xlim()\n",
                "                # ================\n",
                "                # AFTER PLOTS\n",
                "                # ================\n",
                "                after_hist_ax = axes[hist_row, 1]\n",
                "                after_box_ax  = axes[box_row, 1]\n",
                "                sns.histplot(ax = after_hist_ax, data = df_S8_after, x = colname, bins = common_bins)\n",
                "                after_hist_ax.set_title(colname + \" - AFTER\")\n",
                "                after_hist_ax.set_xlabel(\"\")\n",
                "                sns.boxplot(ax = after_box_ax, data = df_S8_after, x = colname)\n",
                "                after_box_ax.set_xlabel(\"\")\n",
                "                # Check if outliers are still present in AFTER\n",
                "                normal_low_present  = (df_S8_after[colname] < normal_low).any()\n",
                "                normal_up_present   = (df_S8_after[colname] > normal_up).any()\n",
                "                extreme_low_present = (df_S8_after[colname] < extreme_low).any()\n",
                "                extreme_up_present  = (df_S8_after[colname] > extreme_up).any()\n",
                "                # NORMAL Outliers\n",
                "                if normal_low_present:\n",
                "                    after_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
                "                    after_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
                "                if normal_up_present:\n",
                "                    after_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
                "                    after_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
                "                # EXTREME Outliers\n",
                "                if extreme_low_present:\n",
                "                    after_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
                "                    after_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
                "                if extreme_up_present:\n",
                "                    after_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
                "                    after_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
                "                # Legends\n",
                "                before_hist_ax.legend(\n",
                "                    handles=[\n",
                "                        plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
                "                        plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
                "                    loc=\"upper right\")\n",
                "                after_hist_ax.legend(\n",
                "                    handles=[\n",
                "                        plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
                "                        plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
                "                    loc=\"upper right\")\n",
                "                # Syncronize axes limits\n",
                "                after_hist_ax.set_xlim(xlim_hist_before)\n",
                "                after_hist_ax.set_ylim(ylim_hist_before)\n",
                "                after_box_ax.set_xlim(xlim_box_before)\n",
                "            plt.tight_layout()\n",
                "            plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 9"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corr_threshold = 0.95 # Correlation level considered as \"too high\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_S9 = df_S8.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 9 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 9 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 9 - REMOVE NOISY ATTRIBUTES - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP and TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\" or dataset_type == \"TIME-SERIES\":\n",
                "    log(f\"This step is non-applicable for {dataset_type} datasets\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    #  NUMERIC ATTRIBUTES (Pearson correlation)\n",
                "    corr_matrix = df_S9[numeric_att].corr().abs()\n",
                "    to_drop = set()\n",
                "    for i in range(len(corr_matrix.columns)):\n",
                "        for j in range(i):\n",
                "            if corr_matrix.iloc[i, j] > corr_threshold:\n",
                "                col_i = corr_matrix.columns[i]\n",
                "                col_j = corr_matrix.columns[j]\n",
                "                if col_i not in to_drop:\n",
                "                    to_drop.add(col_i)\n",
                "    if to_drop:\n",
                "        df_S9 = df_S9.drop(columns=list(to_drop), axis=1)\n",
                "        log(f\"High NUMERIC attributes correlation detected (Pearson Corr. > {corr_threshold}). Dropped:\", type=\"WARNING\")\n",
                "        for col in to_drop:\n",
                "            log(f\"Attribute '{col}'\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        log(f\"No NUMERIC attributes exceeded {corr_threshold} Pearson Correlation\", type=\"SUCCESS\")\n",
                "    #  CATEGORICAL ATTRIBUTES (Cram√©r's V)\n",
                "    to_drop_cat = set()\n",
                "    if len(category_att) > 1:\n",
                "        for i in range(len(category_att)):\n",
                "            for j in range(i):\n",
                "                v = cramers_v(df_S9[category_att[i]], df_S9[category_att[j]])\n",
                "                if v > corr_threshold:\n",
                "                    col_i = category_att[i]\n",
                "                    col_j = category_att[j]\n",
                "                    if col_i not in to_drop_cat:\n",
                "                        to_drop_cat.add(col_i)\n",
                "    if to_drop_cat:\n",
                "        df_S9 = df_S9.drop(columns=list(to_drop_cat), axis=1)\n",
                "        log(f\"High CATEGORICAL attributes correlation association detected (Cramer‚Äôs V > {corr_threshold}). Dropped:\", type=\"WARNING\")\n",
                "        for col in to_drop_cat:\n",
                "            log(f\"Attribute '{col}'\", level=2, type=\"INFO\")\n",
                "    else:\n",
                "        log(f\"No CATEGORICAL attributes exceeded {corr_threshold} Cramer‚Äôs V\", type=\"SUCCESS\")\n",
                "    # Update numeric_att\n",
                "    updated_numeric = []\n",
                "    for col in numeric_att:\n",
                "        if col in df_S9.columns:\n",
                "            updated_numeric.append(col)\n",
                "    numeric_att = updated_numeric\n",
                "    # Update category_att\n",
                "    updated_category = []\n",
                "    for col in category_att:\n",
                "        if col in df_S9.columns:\n",
                "            updated_category.append(col)\n",
                "    category_att = updated_category\n",
                "    # Update binary_att\n",
                "    updated_binary = []\n",
                "    for col in binary_att:\n",
                "        if col in df_S9.columns:\n",
                "            updated_binary.append(col)\n",
                "    binary_att = updated_binary\n",
                "    # Update multiclass_att\n",
                "    updated_multiclass = []\n",
                "    for col in multiclass_att:\n",
                "        if col in df_S9.columns:\n",
                "            updated_multiclass.append(col)\n",
                "    multiclass_att = updated_multiclass\n",
                "    # Print results\n",
                "    log(\"Noisy attributes have been handled successfully:\", type=\"SUCCESS\")\n",
                "    log(f\"Previous df's columns: {len(df_S8.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current df's columns: {len(df_S9.columns)}\", level=2, type=\"INFO\")\n",
                "    log(f\"Current DataFrame shape: {df_S9.shape}\", level=2, type=\"INFO\")\n",
                "    display(df_S9.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 10 - SPLIT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "df_10 = df_S9.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 10 - SPLIT - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 10 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 10 - SPLIT - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP and TABULAR DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\" or dataset_type == \"TABULAR\":\n",
                "    if supervised_learning:\n",
                "        # Separate attributes from target variable\n",
                "        X = df_10.drop(labels = y_var, axis = 1)\n",
                "        y = df_10[y_var]\n",
                "        # Make split between Train and Test\n",
                "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_seed)\n",
                "        # Print results\n",
                "        log(\"Shape of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "        log(f\"X_train: {X_train.shape}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"X_test: {X_test.shape}\", level=2, custom_icon=\"üß™\")\n",
                "        log(f\"y_train: {y_train.shape}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"y_test: {y_test.shape}\", level=2, custom_icon=\"üß™\")\n",
                "        print(\"\\n\")\n",
                "        log(\"Content of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "        log(f\"X_train: {X_train.head(5)}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"X_test: {X_test.head(5)}\", level=2, custom_icon=\"üß™\")\n",
                "        log(f\"y_train: {y_train.head(5)}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"y_test: {y_test.head(5)}\", level=2, custom_icon=\"üß™\")\n",
                "        print(\"\\n\")\n",
                "        log(\"Info of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "        log(f\"X_train: {X_train.info()}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"X_test: {X_test.info()}\", level=2, custom_icon=\"üß™\")\n",
                "        log(f\"y_train: {y_train.info()}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"y_test: {y_test.info()}\", level=2, custom_icon=\"üß™\")\n",
                "    else:\n",
                "        # In unsupervised NLP, we keep all available columns as features\n",
                "        X = df_10.copy()\n",
                "        # Make split between Train and Test (only X)\n",
                "        X_train, X_test = train_test_split(X, test_size=test_size, random_state=random_seed)\n",
                "        # Print results\n",
                "        log(\"Shape of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "        log(f\"X_train: {X_train.shape}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"X_test:  {X_test.shape}\", level=2, custom_icon=\"üß™\")\n",
                "        print(\"\\n\")\n",
                "        log(\"Content of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "        log(f\"X_train:\\n{X_train.head(5)}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "        log(f\"X_test:\\n{X_test.head(5)}\", level=2, custom_icon=\"üß™\")\n",
                "    \n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================       \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # NOTE: In time-series we must keep chronological order (no random split)\n",
                "    df_ts = df_10.copy()\n",
                "    # Ensure data is sorted from past -> present (prefer datetime index if available)\n",
                "    try:\n",
                "        df_ts = df_ts.sort_index()\n",
                "    except:\n",
                "        pass\n",
                "    # Separate attributes from target variable\n",
                "    X = df_ts.drop(labels = y_var, axis = 1)\n",
                "    y = df_ts[y_var]\n",
                "    # Cut split (test = most recent observations)\n",
                "    split_idx = int(len(df_ts) * (1 - test_size))\n",
                "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
                "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
                "    # Print results\n",
                "    log(\"Shape of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "    log(f\"X_train: {X_train.shape}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"X_test: {X_test.shape}\", level=2, custom_icon=\"üß™\")\n",
                "    log(f\"y_train: {y_train.shape}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"y_test: {y_test.shape}\", level=2, custom_icon=\"üß™\")\n",
                "    print(\"\\n\")\n",
                "    log(\"Content of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "    log(f\"X_train: {X_train.head(5)}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"X_test: {X_test.head(5)}\", level=2, custom_icon=\"üß™\")\n",
                "    log(f\"y_train: {y_train.head(5)}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"y_test: {y_test.head(5)}\", level=2, custom_icon=\"üß™\")\n",
                "    print(\"\\n\")\n",
                "    log(\"Info of DataFrames after SPLIT:\", type=\"INFO\")\n",
                "    log(f\"X_train: {X_train.info()}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"X_test: {X_test.info()}\", level=2, custom_icon=\"üß™\")\n",
                "    log(f\"y_train: {y_train.info()}\", level=2, custom_icon=\"üèãÔ∏è\")\n",
                "    log(f\"y_test: {y_test.info()}\", level=2, custom_icon=\"üß™\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 11"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_scalling = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "X_train_es = X_train.copy()\n",
                "X_test_es  = X_test.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 11 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 11 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 11 - SCALLING - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP and TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\" or dataset_type == \"TIME-SERIES\":\n",
                "    log(f\"This step is non-applicable for {dataset_type} datasets\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Automatic scaler decision for each numeric attribute\n",
                "    scaler_dic = {}\n",
                "    if not numeric_att:\n",
                "        data_has_been_scaled = False\n",
                "        X_train_es = X_train_es[numeric_att]\n",
                "        X_test_es  = X_test_es[numeric_att]\n",
                "        log(\"SCALLING is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "    elif not make_scalling:\n",
                "        data_has_been_scaled = False\n",
                "        X_train_es = X_train_es[numeric_att]\n",
                "        X_test_es  = X_test_es[numeric_att]\n",
                "        log(\"SCALLING is not carried out, set make_scalling = True\", type=\"WARNING\")\n",
                "        display(X_train_es.head())\n",
                "    else:\n",
                "        data_has_been_scaled = True\n",
                "        log(\"Automatic scaler selection per NUMERIC attribute:\", type=\"INFO\", bold=True)\n",
                "        # -------------------------------\n",
                "        # 1) Decide scaler for each column\n",
                "        # -------------------------------\n",
                "        for col in numeric_att:\n",
                "            series = X_train[col].dropna()\n",
                "            col_min = series.min()\n",
                "            col_max = series.max()\n",
                "            col_std = series.std()\n",
                "            col_skew = abs(series.skew())\n",
                "            # Avoid division by zero\n",
                "            range_ratio = (col_max - col_min) / max(col_std, 1e-8)\n",
                "            # MinMaxScaler if variable has large range or is skewed\n",
                "            if (range_ratio > 20) or (col_skew > 1.5):\n",
                "                scaler_dic[col] = MinMaxScaler()\n",
                "                chosen_scaler = \"MinMaxScaler\"\n",
                "            else:\n",
                "                scaler_dic[col] = StandardScaler()\n",
                "                chosen_scaler = \"StandardScaler\"\n",
                "            log(f\"'{col}' ‚Üí range_ratio={range_ratio:.1f}, skew={col_skew:.2f} ‚Üí selected {chosen_scaler}\", level=2, type=\"INFO\")\n",
                "        # -------------------------------\n",
                "        # 2) Fit scalers ONLY on train data\n",
                "        # -------------------------------\n",
                "        for col in numeric_att:\n",
                "            scaler_dic[col].fit(X_train_es[[col]])\n",
                "        log(\"All Scalers have been trained successfully\", type=\"SUCCESS\")\n",
                "        # -------------------------------\n",
                "        # 3) Apply scaling, create scaled columns\n",
                "        # -------------------------------\n",
                "        scaled_cols = []\n",
                "        for col in numeric_att:\n",
                "            scaler_name = scaler_dic[col].__class__.__name__\n",
                "            if scaler_name == \"StandardScaler\":\n",
                "                suffix = \"_SS\"\n",
                "            elif scaler_name == \"MinMaxScaler\":\n",
                "                suffix = \"_MM\"\n",
                "            else:\n",
                "                suffix = \"_Scaled\"\n",
                "            X_train_es[col + suffix] = scaler_dic[col].transform(X_train_es[[col]])\n",
                "            X_test_es[col + suffix]  = scaler_dic[col].transform(X_test_es[[col]])\n",
                "            scaled_cols.append(col + suffix)\n",
                "        log(\"All NUMERIC attributes has been scaled successfully\", type=\"SUCCESS\")\n",
                "        # -------------------------------\n",
                "        # 4) Keep only scaled attributes\n",
                "        # -------------------------------\n",
                "        X_train_es = X_train_es[scaled_cols]\n",
                "        X_test_es  = X_test_es[scaled_cols]\n",
                "        print(\"\\n\")\n",
                "        log(\"Final scaled datasets created successfully!\", type=\"SUCCESS\", bold=True)\n",
                "        display(X_train_es.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 12"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for NLP (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_vectorization_plots = True        # Draw PCA comparison plots between COUNT and TF-IDF\n",
                "max_features_manual = None             # Set a number manually, or None for auto selection\n",
                "min_df_threshold = 1                   # Ignore tokens that appear in fewer than N documents\n",
                "rare_word_cutoff = 3                   # Words appearing < rare_word_cutoff considered \"rare\"\n",
                "long_text_threshold = 40               # If avg words per doc > threshold ‚Üí increase vocab"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "make_encoding = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "X_train_en = X_train.copy()\n",
                "X_test_en  = X_test.copy()\n",
                "if supervised_learning:\n",
                "    y_train_en = y_train.copy()\n",
                "else:\n",
                "    y_train_en = None\n",
                "main_text_col_12 = main_text_col_S7 if dataset_type == \"NLP\" else None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 12 - TEXT VECTORIZATION - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 12 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 12 - ENCODING - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # Reconstruct text from tokens\n",
                "    text_series = X_train_en[main_text_col_12].apply(lambda token_list: \" \".join(token_list))\n",
                "    # Number of documents\n",
                "    num_texts = len(text_series)\n",
                "    # Build vocabulary\n",
                "    vocab = set()\n",
                "    for doc_tokens in X_train_en[main_text_col_12]:\n",
                "        for word in doc_tokens:\n",
                "            vocab.add(word)\n",
                "    vocab_size = len(vocab)\n",
                "    # Compute avg words per document\n",
                "    word_lengths = []\n",
                "    for doc_tokens in X_train_en[main_text_col_12]:\n",
                "        word_lengths.append(len(doc_tokens))\n",
                "    avg_words_per_text = np.mean(word_lengths)\n",
                "    # Count word frequencies\n",
                "    word_counts = {}\n",
                "    for doc_tokens in X_train_en[main_text_col_12]:\n",
                "        for word in doc_tokens:\n",
                "            if word not in word_counts:\n",
                "                word_counts[word] = 1\n",
                "            else:\n",
                "                word_counts[word] += 1\n",
                "    # Identify rare words\n",
                "    rare_words = []\n",
                "    for word, count in word_counts.items():\n",
                "        if count < rare_word_cutoff:\n",
                "            rare_words.append(word)\n",
                "    rare_ratio = len(rare_words) / vocab_size if vocab_size > 0 else 0.0\n",
                "    log(f\"Total vocabulary size: {vocab_size}\", type=\"INFO\")\n",
                "    log(f\"Average words per document: {avg_words_per_text:.1f}\", type=\"INFO\")\n",
                "    log(f\"Rare-word ratio: {rare_ratio:.2f}\", type=\"INFO\")\n",
                "    # 3) Auto-determine max_features\n",
                "    if max_features_manual is not None:\n",
                "        max_features_auto = max_features_manual\n",
                "        log(f\"Using manual max_features = {max_features_manual}\", type=\"INFO\")\n",
                "    else:\n",
                "        # Base rule by dataset size\n",
                "        if num_texts < 3000:\n",
                "            max_features_auto = 2000\n",
                "        elif num_texts <= 20000:\n",
                "            max_features_auto = 5000\n",
                "        else:\n",
                "            max_features_auto = 7000\n",
                "        # Ensure vocabulary is not smaller\n",
                "        if vocab_size < max_features_auto:\n",
                "            max_features_auto = vocab_size\n",
                "        # If too many rare words, expand vocabulary allowance\n",
                "        if rare_ratio > 0.50:\n",
                "            max_features_auto = int(max_features_auto * 1.5)\n",
                "        # If documents are long, increase vocabulary\n",
                "        if avg_words_per_text > long_text_threshold:\n",
                "            max_features_auto = int(max_features_auto * 1.5)\n",
                "        log(f\"Auto-selected max_features = {max_features_auto}\", type=\"INFO\")\n",
                "    # Isntance vectorizers\n",
                "    count_vec = CountVectorizer(max_features=max_features_auto, min_df=min_df_threshold)\n",
                "    tfidf_vec = TfidfVectorizer(max_features=max_features_auto, min_df=min_df_threshold)\n",
                "    # Fit vectorizers\n",
                "    count_X = count_vec.fit_transform(text_series).toarray()\n",
                "    tfidf_X = tfidf_vec.fit_transform(text_series).toarray()\n",
                "    # PCA 2D Visualization\n",
                "    if make_vectorization_plots:\n",
                "        print(\"\\nüìä PCA 2D VISUALIZATION\")\n",
                "        pca = PCA(n_components=2)\n",
                "        count_2D = pca.fit_transform(count_X)\n",
                "        tfidf_2D = pca.fit_transform(tfidf_X)\n",
                "        # Prepare plot\n",
                "        fig, axes = plt.subplots(1, 2, figsize=(figWidth_unit * 2, figHeight_unit * 1))\n",
                "        # SUPERVISED ‚Üí colorear por y\n",
                "        if supervised_learning and (y_train_en is not None):\n",
                "            classes = sorted(y_train_en.unique())\n",
                "            colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
                "            # COUNT plot\n",
                "            axes[0].scatter(count_2D[:, 0], count_2D[:, 1], c=y_train_en, cmap=\"viridis\", alpha=0.8)\n",
                "            axes[0].set_title(\"PCA (2D) ‚Äî CountVectorizer\", fontsize=plot_title_font_size)\n",
                "            # Legend\n",
                "            handles_count = []\n",
                "            for i, cls in enumerate(classes):\n",
                "                handles_count.append(\n",
                "                    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i],\n",
                "                               markersize=8, label=str(cls))\n",
                "                )\n",
                "            axes[0].legend(handles=handles_count, title=\"Target\")\n",
                "            # TF-IDF plot\n",
                "            axes[1].scatter(tfidf_2D[:, 0], tfidf_2D[:, 1], c=y_train_en, cmap=\"viridis\", alpha=0.8)\n",
                "            axes[1].set_title(\"PCA (2D) ‚Äî TF-IDF\", fontsize=plot_title_font_size)\n",
                "            handles_tfidf = []\n",
                "            for i, cls in enumerate(classes):\n",
                "                handles_tfidf.append(\n",
                "                    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i],\n",
                "                               markersize=8, label=str(cls))\n",
                "                )\n",
                "            axes[1].legend(handles=handles_tfidf, title=\"Target\")\n",
                "        else:\n",
                "            axes[0].scatter(count_2D[:, 0], count_2D[:, 1], alpha=0.8)\n",
                "            axes[0].set_title(\"PCA (2D) ‚Äî CountVectorizer\", fontsize=plot_title_font_size)\n",
                "            axes[1].scatter(tfidf_2D[:, 0], tfidf_2D[:, 1], alpha=0.8)\n",
                "            axes[1].set_title(\"PCA (2D) ‚Äî TF-IDF\", fontsize=plot_title_font_size)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    # Auto-select best Vectorizer\n",
                "    best_vectorizer_name_S8 = \"TFIDF\"\n",
                "    if supervised_learning and (y_train_en is not None) and y_var_subtype in [\"BINARY\", \"MULTICLASS\"]:\n",
                "        try:\n",
                "            score_count = silhouette_score(count_2D, y_train_en)\n",
                "            score_tfidf = silhouette_score(tfidf_2D, y_train_en)\n",
                "            if score_count > score_tfidf:\n",
                "                best_vectorizer_name_S8 = \"COUNT\"\n",
                "        except:\n",
                "            pass\n",
                "    print(\"\\n\")\n",
                "    log(f\"Final selected vectorizer: {best_vectorizer_name_S8}\", type=\"SUCCESS\", bold=True)\n",
                "     # Instance selected vectorizer\n",
                "    if best_vectorizer_name_S8 == \"COUNT\":\n",
                "        vectorizer = CountVectorizer()\n",
                "        vectorizer_name = \"CountVectorizer()\"\n",
                "    else:\n",
                "        vectorizer = TfidfVectorizer()\n",
                "        vectorizer_name = \"TfidfVectorizer()\"\n",
                "    log(f\"Auto-selected Vectorizer ({best_vectorizer_name_S8}) instanced successfully\", type=\"SUCCESS\")\n",
                "    # Train vectorizer wiht TRAIN data and transform TRAIN + TEST\n",
                "    X_train_en = vectorizer.fit_transform(X_train_en[main_text_col_12].astype(str)).toarray()\n",
                "    X_test_en = vectorizer.transform(X_test_en[main_text_col_12].astype(str)).toarray()\n",
                "    # Get feature names\n",
                "    feature_names = vectorizer.get_feature_names_out()\n",
                "    # Convert array to DataFrame\n",
                "    X_train_en = pd.DataFrame(X_train_en, index=X_train.index, columns=feature_names)\n",
                "    X_test_en = pd.DataFrame(X_test_en, index=X_test.index, columns=feature_names)\n",
                "    log(\"Final vectorized datasets created successfully\", type=\"SUCCESS\")\n",
                "    display(X_train_en)\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================    \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    log(f\"This step is non-applicable for {dataset_type} datasets\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    if not category_att:\n",
                "        X_train_en = X_train_en[category_att]\n",
                "        X_test_en = X_test_en[category_att]\n",
                "        log(\"ENCODING is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\", type=\"WARNING\")\n",
                "    elif not make_encoding:\n",
                "        X_train_en = X_train_en[category_att]\n",
                "        X_test_en = X_test_en[category_att]\n",
                "        log(\"ENCODING is is not carried out, set make_encoding = True\", type=\"WARNING\")\n",
                "        display(X_train_en.head())\n",
                "    else:\n",
                "        # List of columns\n",
                "        columns = X_train_en.columns.tolist()\n",
                "        # Create encoder instance for each categorical attribute\n",
                "        encoder_dic = {}\n",
                "        for col in category_att:\n",
                "            if col in binary_att:\n",
                "                encoder_dic[col] = LabelEncoder()\n",
                "                log(f\"Encoder instanced successfully for {col}: LabelEncoder()\", type=\"SUCCESS\")\n",
                "            elif col in multiclass_att:\n",
                "                encoder_dic[col] = OneHotEncoder(sparse_output=False)\n",
                "                log(f\"Encoder instanced successfully for {col}: OneHotEncoder()\", type=\"SUCCESS\")\n",
                "        # Train encoders with TRAIN data only\n",
                "        for col in category_att:\n",
                "            encoder = encoder_dic[col]\n",
                "            if isinstance(encoder, LabelEncoder):\n",
                "                encoder.fit(X_train_en[col])        # LabelEncoder needs 1D\n",
                "                log(f\"Encoder trained successfully with {col} from Train: LabelEncoder()\", type=\"SUCCESS\")\n",
                "            elif isinstance(encoder, OneHotEncoder):\n",
                "                encoder.fit(X_train_en[[col]])      # OHE needs 2D\n",
                "                log(f\"Encoder trained successfully with {col} from Train: OneHotEncoder()\", type=\"SUCCESS\")\n",
                "        # Apply encoders to TRAIN + TEST\n",
                "        for col in category_att:\n",
                "            encoder = encoder_dic[col]\n",
                "            if isinstance(encoder, LabelEncoder):\n",
                "                X_train_en[col + \"_LE\"] = encoder.transform(X_train_en[col])\n",
                "                X_test_en[col + \"_LE\"] = encoder.transform(X_test_en[col])\n",
                "                log(f\"Train/Test encoded for: {col} using LabelEncoder()\", type=\"SUCCESS\")\n",
                "            elif isinstance(encoder, OneHotEncoder):\n",
                "                # Transform train and test\n",
                "                train_encoded = encoder.transform(X_train_en[[col]])\n",
                "                test_encoded = encoder.transform(X_test_en[[col]])\n",
                "                # New names\n",
                "                ohe_colnames = encoder.get_feature_names_out([col])\n",
                "                ohe_colnames = [name + \"_OHE\" for name in ohe_colnames]\n",
                "                # Convert to DataFrames\n",
                "                train_ohe_df = pd.DataFrame(train_encoded, index=X_train_en.index, columns=ohe_colnames)\n",
                "                test_ohe_df = pd.DataFrame(test_encoded, index=X_test_en.index, columns=ohe_colnames)\n",
                "                # Concatenate new cols\n",
                "                X_train_en = pd.concat([X_train_en, train_ohe_df], axis=1)\n",
                "                X_test_en = pd.concat([X_test_en, test_ohe_df], axis=1)\n",
                "                log(f\"Train/Test encoded for: {col} using OneHotEncoder()\", type=\"SUCCESS\")\n",
                "        # Keep only encoded columns\n",
                "        encoded_cols = []\n",
                "        for col in category_att:\n",
                "            encoder = encoder_dic[col]\n",
                "            if isinstance(encoder, LabelEncoder):\n",
                "                encoded_cols.append(col + \"_LE\")\n",
                "            elif isinstance(encoder, OneHotEncoder):\n",
                "                ohe_colnames = encoder.get_feature_names_out([col])\n",
                "                for name in ohe_colnames:\n",
                "                    encoded_cols.append(name + \"_OHE\")\n",
                "        X_train_en = X_train_en[encoded_cols]\n",
                "        X_test_en = X_test_en[encoded_cols]\n",
                "        log(\"Final encoded datasets created successfully\", type=\"SUCCESS\")\n",
                "        display(X_test_en)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 13 - ASSEMBLY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "X_train_S13 = X_train.copy()\n",
                "X_test_S13 = X_test.copy()\n",
                "X_train_es_S13 = X_train_es.copy()\n",
                "X_test_es_S13 = X_test_es.copy()\n",
                "X_train_en_S13 = X_train_en.copy()\n",
                "X_test_en_S13 = X_test_en.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "print(\"STEP 13 - ASSEMBLY - \",learning_type, dataset_type)\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # Concatenate vectorized var\n",
                "    X_train_assembled_S13 = X_train_en_S13\n",
                "    X_test_assembled_S13 = X_test_en_S13\n",
                "    # Print results\n",
                "    log(f\"X_train_assembled {X_train_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_train_assembled_S13)\n",
                "    print(\"\\n\")\n",
                "    log(f\"X_test_assembled {X_test_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_test_assembled_S13)\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================      \n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    # Concatenate the whole dataset how it was\n",
                "    X_train_assembled_S13 = X_train_S13\n",
                "    X_test_assembled_S13 = X_test_S13\n",
                "    # Print results\n",
                "    log(f\"X_train_assembled {X_train_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_train_assembled_S13)\n",
                "    print(\"\\n\")\n",
                "    log(f\"X_test_assembled {X_test_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_test_assembled_S13)\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # Concatenate NUMERIC_var_scaled with CATEGORY_var_encoded\n",
                "    X_train_assembled_S13 = pd.concat([X_train_es_S13, X_train_en_S13], axis=1)\n",
                "    X_test_assembled_S13  = pd.concat([X_test_es_S13,  X_test_en_S13],  axis=1)\n",
                "    # Print results\n",
                "    log(f\"X_train_assembled {X_train_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_train_assembled_S13)\n",
                "    print(\"\\n\")\n",
                "    log(f\"X_test_assembled {X_test_assembled_S13.shape}\", custom_icon=\"üßÆ\")\n",
                "    display(X_test_assembled_S13)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 14 - FEATURE SELECTION"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_keeping_threshold = 100   # [%] Percentaje of features (tokens) to keep\n",
                "min_features_for_selection = 20  # If num features < this ‚Üí skip selection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP (UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# VarianceThreshold percentile for UNSUPERVISED feature selection\n",
                "variance_threshold_percentile = 10   # Drop lowest variance_threshold_percentile % variance features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "X_train_S14 = X_train_assembled_S13.copy()\n",
                "X_test_S14 = X_test_assembled_S13.copy()\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"===============================\")\n",
                "if dataset_type == \"NLP\":\n",
                "    print(\"STEP 14 - FEATURE SELECTION - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    print(\"STEP 14 - Non applicable - \",learning_type, dataset_type)\n",
                "if dataset_type == \"TABULAR\":\n",
                "    print(\"STEP 14 - FEATURE SELECTION - \",learning_type, dataset_type)\n",
                "print(\"===============================\\n\")\n",
                "\n",
                "# ===============================\n",
                "# NLP DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"NLP\":\n",
                "    # -------------------------------------------\n",
                "    # UNSUPERVISED LEARNING ‚Üí VarianceThreshold\n",
                "    # -------------------------------------------\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED feature selection using VarianceThreshold\", type=\"INFO\", bold=True)\n",
                "        # Compute variances on TRAIN data only\n",
                "        feature_variances = X_train_S14.var(axis=0)\n",
                "        # Automatic threshold: low variance percentile\n",
                "        variance_threshold_value = np.percentile(feature_variances.values, variance_threshold_percentile)\n",
                "        log(f\"Automatic variance threshold set to {variance_threshold_value:.6f} (percentile={variance_threshold_percentile})\", type=\"INFO\")\n",
                "        # SAFETY CHECK: threshold cannot exceed max variance\n",
                "        max_var = feature_variances.max()\n",
                "        eps = 1e-12\n",
                "        if variance_threshold_value >= max_var:\n",
                "            log(f\"VarianceThreshold value ({variance_threshold_value:.6f}) is >= max variance ({max_var:.6f}) ‚Üí threshold will be capped\", type=\"WARNING\")\n",
                "            variance_threshold_value = max(max_var - eps, 0.0)\n",
                "            log(f\"Final capped variance threshold: {variance_threshold_value:.6f}\", type=\"INFO\")\n",
                "        # Instance selector\n",
                "        variance_selector = VarianceThreshold(threshold=variance_threshold_value)\n",
                "        log(\"VarianceThreshold selector instanced successfully\", type=\"SUCCESS\")\n",
                "        try:\n",
                "            # Fit selector on TRAIN only\n",
                "            variance_selector.fit(X_train_S14)\n",
                "            log(\"VarianceThreshold selector trained successfully\", type=\"SUCCESS\")\n",
                "            # Apply mask\n",
                "            variance_mask = variance_selector.get_support()\n",
                "            variance_selected_cols = X_train_S14.columns[variance_mask]\n",
                "            # Transform TRAIN and TEST\n",
                "            X_train_S14 = pd.DataFrame(\n",
                "                variance_selector.transform(X_train_S14),\n",
                "                columns=variance_selected_cols,\n",
                "                index=X_train_S14.index\n",
                "            )\n",
                "            X_test_S14 = pd.DataFrame(\n",
                "                variance_selector.transform(X_test_S14),\n",
                "                columns=variance_selected_cols,\n",
                "                index=X_test_S14.index\n",
                "            )\n",
                "        except ValueError as e:\n",
                "            log(f\"VarianceThreshold skipped: {e}\", type=\"WARNING\")\n",
                "            X_train_S14 = X_train_S14\n",
                "            X_test_S14  = X_test_S14\n",
                "            variance_selector = None\n",
                "        # Print results\n",
                "        log(f\"Previous X_train shape: {X_train_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Previous X_test shape: {X_test_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current X_train shape: {X_train_S14.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current X_test shape: {X_test_S14.shape}\", level=2, type=\"INFO\")\n",
                "        display(X_train_S14)\n",
                "    # -------------------------------------------\n",
                "    # SUPERVISED LEARNING ‚Üí SelectKBest\n",
                "    # -------------------------------------------\n",
                "    else:\n",
                "        log(\"UNSUPERVISED feature selection using SelectKBest\", type=\"INFO\", bold=True)\n",
                "        # Count features (tokens)\n",
                "        num_features_nlp = X_train_S14.shape[1]\n",
                "        log(f\"Number of features before selection: {num_features_nlp}\", type=\"INFO\")\n",
                "        # Check if feature selection makes sense\n",
                "        if num_features_nlp < min_features_for_selection:\n",
                "            log(f\"Feature selection skipped: number of features ({num_features_nlp}) is below defined threshold ({min_features_for_selection})\", type=\"INFO\")\n",
                "        else:\n",
                "            # Compute number of features to keep\n",
                "            num_keep_nlp = round(feature_keeping_threshold / 100 * num_features_nlp)\n",
                "            num_keep_nlp = max(1, min(num_keep_nlp, num_features_nlp))\n",
                "            # Select chi2 for categorical targets, f_regression for numeric\n",
                "            if y_var_type == \"CATEGORIC\":\n",
                "                score_func_nlp = chi2\n",
                "                log(\"Used chi2 for feature selection (CATEGORIC target)\", type=\"INFO\")\n",
                "                if num_keep_nlp == num_features_nlp:\n",
                "                    log(f\"Feature selection will keep all original {num_features_nlp} features, because keeping threshold is set as {feature_keeping_threshold} %\", type=\"INFO\")\n",
                "                else:\n",
                "                    log(f\"Feature selection will keep {num_keep_nlp} out of {num_features_nlp} features\", type=\"WARNING\")\n",
                "            else:\n",
                "                score_func_nlp = f_regression\n",
                "                log(\"Used f_regression for feature selection (NUMERIC target)\", type=\"INFO\")\n",
                "                if num_keep_nlp == num_features_nlp:\n",
                "                    log(f\"Feature selection will keep all original {num_features_nlp} features, because keeping threshold is set as {feature_keeping_threshold} %\", type=\"INFO\")\n",
                "                else:\n",
                "                    log(f\"Feature selection will keep {num_keep_nlp} out of {num_features_nlp} features\", type=\"WARNING\")\n",
                "            # Instance selector\n",
                "            selector_nlp = SelectKBest(score_func=score_func_nlp, k=num_keep_nlp)\n",
                "            log(\"Selector has been instanced successfully\", type=\"SUCCESS\")\n",
                "            # Train selector using ONLY TRAIN data\n",
                "            selector_nlp.fit(X_train_S14, y_train)\n",
                "            log(\"Selector has been trained successfully\", type=\"SUCCESS\")\n",
                "            # Apply mask\n",
                "            mask_nlp = selector_nlp.get_support()\n",
                "            selected_cols_nlp = X_train_S14.columns[mask_nlp]\n",
                "            # Transform TRAIN and TEST\n",
                "            X_train_S14 = pd.DataFrame(selector_nlp.transform(X_train_S14),\n",
                "                                    columns=selected_cols_nlp,\n",
                "                                    index=X_train_S14.index)\n",
                "\n",
                "            X_test_S14 = pd.DataFrame(selector_nlp.transform(X_test_S14),\n",
                "                                    columns=selected_cols_nlp,\n",
                "                                    index=X_test_S14.index)\n",
                "            # Print results\n",
                "            log(f\"Previous X_train shape: {X_train_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Previous X_test shape: {X_test_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Current X_train shape: {X_train_S14.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Current X_test shape: {X_test_S14.shape}\", level=2, type=\"INFO\")\n",
                "            display(X_train_S14)\n",
                "        \n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TIME-SERIES\":\n",
                "    log(f\"This step is non-applicable for {dataset_type} datasets\", type=\"WARNING\")\n",
                "\n",
                "# ===============================\n",
                "# TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"TABULAR\":\n",
                "    # -------------------------------------------\n",
                "    # UNSUPERVISED LEARNING ‚Üí VarianceThreshold\n",
                "    # -------------------------------------------\n",
                "    if not supervised_learning:\n",
                "        log(\"UNSUPERVISED feature selection using VarianceThreshold\", type=\"INFO\", bold=True)\n",
                "        # Compute variances on TRAIN data only\n",
                "        feature_variances = X_train_S14.var(axis=0)\n",
                "        # Automatic threshold\n",
                "        variance_threshold_value = np.percentile(feature_variances.values, variance_threshold_percentile)\n",
                "        log(f\"Automatic variance threshold set to {variance_threshold_value:.6f} (percentile={variance_threshold_percentile})\", type=\"INFO\")\n",
                "        # SAFETY CHECK: threshold cannot exceed max variance\n",
                "        max_var = feature_variances.max()\n",
                "        eps = 1e-12\n",
                "        if variance_threshold_value >= max_var:\n",
                "            log(f\"VarianceThreshold value ({variance_threshold_value:.6f}) is >= max variance ({max_var:.6f}) ‚Üí threshold will be capped\", type=\"WARNING\")\n",
                "            variance_threshold_value = max(max_var - eps, 0.0)\n",
                "            log(f\"Final capped variance threshold: {variance_threshold_value:.6f}\", type=\"INFO\")\n",
                "        # Instance selector\n",
                "        variance_selector = VarianceThreshold(threshold=variance_threshold_value)\n",
                "        log(\"VarianceThreshold selector instanced successfully\", type=\"SUCCESS\")\n",
                "        try:\n",
                "            # Fit selector on TRAIN only\n",
                "            variance_selector.fit(X_train_S14)\n",
                "            log(\"VarianceThreshold selector trained successfully\", type=\"SUCCESS\")\n",
                "            # Apply mask\n",
                "            variance_mask = variance_selector.get_support()\n",
                "            variance_selected_cols = X_train_S14.columns[variance_mask]\n",
                "            # Transform TRAIN and TEST\n",
                "            X_train_S14 = pd.DataFrame(\n",
                "                variance_selector.transform(X_train_S14),\n",
                "                columns=variance_selected_cols,\n",
                "                index=X_train_S14.index\n",
                "            )\n",
                "            X_test_S14 = pd.DataFrame(\n",
                "                variance_selector.transform(X_test_S14),\n",
                "                columns=variance_selected_cols,\n",
                "                index=X_test_S14.index\n",
                "            )\n",
                "        except ValueError as e:\n",
                "            log(f\"VarianceThreshold skipped: {e}\", type=\"WARNING\")\n",
                "            X_train_S14 = X_train_S14\n",
                "            X_test_S14  = X_test_S14\n",
                "            variance_selector = None\n",
                "        # Print results\n",
                "        log(f\"Previous X_train shape: {X_train_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Previous X_test shape: {X_test_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current X_train shape: {X_train_S14.shape}\", level=2, type=\"INFO\")\n",
                "        log(f\"Current X_test shape: {X_test_S14.shape}\", level=2, type=\"INFO\")\n",
                "        display(X_train_S14)\n",
                "    # -------------------------------------------\n",
                "    # SUPERVISED LEARNING ‚Üí SelectKBest\n",
                "    # -------------------------------------------\n",
                "    else:\n",
                "        log(\"UNSUPERVISED feature selection using SelectKBest\", type=\"INFO\", bold=True)\n",
                "        # Count features (tokens)\n",
                "        num_features_tab = X_train_S14.shape[1]\n",
                "        log(f\"Number of features before selection: {num_features_tab}\", type=\"INFO\")\n",
                "        # Check if feature selection makes sense\n",
                "        if num_features_tab < min_features_for_selection:\n",
                "            log(f\"Feature selection skipped: number of features ({num_features_tab}) is below defined threshold ({min_features_for_selection})\", type=\"INFO\")\n",
                "        else:\n",
                "            # Compute number of features to keep\n",
                "            num_keep_tab = round(feature_keeping_threshold / 100 * num_features_tab)\n",
                "            num_keep_tab = max(1, min(num_keep_tab, num_features_tab))\n",
                "            # Choose score function based on target type\n",
                "            if y_var_type == \"CATEGORIC\":\n",
                "                score_func_tab = f_classif\n",
                "                log(\"Used f_classif (ANOVA F-test) for feature selection (CATEGORIC target)\", type=\"INFO\")\n",
                "                if num_keep_tab == num_features_tab:\n",
                "                    log(f\"Feature selection will keep all original {num_features_tab} features, because keeping threshold is set as {feature_keeping_threshold} %\", type=\"INFO\")\n",
                "                else:\n",
                "                    log(f\"Feature selection will keep {num_keep_tab} out of {num_features_tab} features\", type=\"WARNING\")\n",
                "            else:\n",
                "                score_func_tab = f_regression\n",
                "                log(\"Used f_regression for feature selection (CATEGORIC target)\", type=\"INFO\")\n",
                "                if num_keep_tab == num_features_tab:\n",
                "                    log(f\"Feature selection will keep all original {num_features_tab} features, because keeping threshold is set as {feature_keeping_threshold} %\", type=\"INFO\")\n",
                "                else:\n",
                "                    log(f\"Feature selection will keep {num_keep_tab} out of {num_features_tab} features\", type=\"WARNING\")\n",
                "            # Instance selector\n",
                "            selector_tab = SelectKBest(score_func=score_func_tab, k=num_keep_tab)\n",
                "            log(\"Selector has been instanced successfully\", type=\"SUCCESS\")\n",
                "            # Train selector using ONLY TRAIN data\n",
                "            selector_tab.fit(X_train_S14, y_train)\n",
                "            log(\"Selector has been trained successfully\", type=\"SUCCESS\")\n",
                "            # Apply mask\n",
                "            mask_tab = selector_tab.get_support()\n",
                "            selected_cols_tab = X_train_S14.columns[mask_tab]\n",
                "            # Transform TRAIN and TEST\n",
                "            X_train_S14 = pd.DataFrame(selector_tab.transform(X_train_S14),\n",
                "                                    columns=selected_cols_tab,\n",
                "                                    index=X_train_S14.index)\n",
                "\n",
                "            X_test_S14 = pd.DataFrame(selector_tab.transform(X_test_S14),\n",
                "                                    columns=selected_cols_tab,\n",
                "                                    index=X_test_S14.index)\n",
                "            # Print results\n",
                "            log(f\"Previous X_train shape: {X_train_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Previous X_test shape: {X_test_assembled_S13.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Current X_train shape: {X_train_S14.shape}\", level=2, type=\"INFO\")\n",
                "            log(f\"Current X_test shape: {X_test_S14.shape}\", level=2, type=\"INFO\")\n",
                "            display(X_train_S14)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 15 - SAVE PROCESSED DATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# PREVIOUS STEP DATA\n",
                "# ===============================\n",
                "X_train_S15 = X_train_S14.copy()\n",
                "X_test_S15  = X_test_S14.copy()\n",
                "if supervised_learning:\n",
                "    y_train_S15 = y_train.copy()\n",
                "    y_test_S15 = y_test.copy()\n",
                "else:\n",
                "    y_train_S15 = None\n",
                "    y_test_S15 = None\n",
                "\n",
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "print(\"STEP 15) SAVE PROCESSED DATA - \",learning_type, dataset_type)\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# ===============================\n",
                "# BUILD FILENAMES WITH REVISION NUMBER\n",
                "# ===============================\n",
                "rev_number = get_revision_number(processed_data_output_path, \"X_train_final\")\n",
                "suffix = \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number)\n",
                "output_path_X_train = processed_data_output_path + \"X_train_final\" + suffix + \".csv\"\n",
                "output_path_X_test  = processed_data_output_path + \"X_test_final\"  + suffix + \".csv\"\n",
                "if supervised_learning:\n",
                "    output_path_y_train = processed_data_output_path + \"y_train_final\" + suffix + \".csv\"\n",
                "    output_path_y_test  = processed_data_output_path + \"y_test_final\"  + suffix + \".csv\"\n",
                "\n",
                "# ===============================\n",
                "# SAVE ALL DATASETS\n",
                "# ===============================\n",
                "X_train_S15.to_csv(output_path_X_train, index=False)\n",
                "log(f\"X_train saved in {processed_data_output_path} with revision number: {rev_number}\", type=\"SUCCESS\")\n",
                "X_test_S15.to_csv(output_path_X_test, index=False)\n",
                "log(f\"X_test saved in {processed_data_output_path} with revision number: {rev_number}\", type=\"SUCCESS\")\n",
                "if supervised_learning:\n",
                "    y_train_S15.to_csv(output_path_y_train, index=False)\n",
                "    log(f\"y_train saved in {processed_data_output_path} with revision number: {rev_number}\", type=\"SUCCESS\")\n",
                "    y_test_S15.to_csv(output_path_y_test, index=False)\n",
                "    log(f\"y_test saved in {processed_data_output_path} with revision number: {rev_number}\", type=\"SUCCESS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 16 - PREDICTION MODELS SUGGESTED"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:red\">Inputs for TABULAR or NLP or TIME-SERIES (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rev_to_use = 0   # Select the desired revision number for models/data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP (SUPERVISED or UNSUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASET SIZE THRESHOLDS\n",
                "# --- BIG dataset thresholds ---\n",
                "n_samples_BIG_dataset    = 10000      # Above this ‚Üí BIG dataset (more complex models allowed, smaller grids)\n",
                "n_features_BIG_dataset   = 50         # Above this ‚Üí HIGH-DIMENSIONAL feature space\n",
                "# --- MEDIUM dataset thresholds ---\n",
                "n_samples_MEDIUM_dataset = 2000       # Between SMALL and MEDIUM ‚Üí MEDIUM dataset\n",
                "n_features_MEDIUM_dataset = 15        # Between LOW and MEDIUM dimensionality\n",
                "# --- SMALL dataset thresholds ---\n",
                "n_samples_SMALL_dataset  = 500        # Below this ‚Üí SMALL dataset (simpler models recommended)\n",
                "n_features_SMALL_dataset = 5          # Below this ‚Üí LOW-DIMENSIONAL feature space\n",
                "# --- MICRO dataset threshold (optional special-case) ---\n",
                "n_samples_MICRO_dataset  = 100        # Very small datasets ‚Äî may restrict to extremely simple models only"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP (SUPERVISED)</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_target_imbalanced_ratio_accepted = 0.8    # Threshold to warn about strong target class imbalance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "print(\"STEP 16) PREDICTION MODELS SUGGESTED - \",learning_type, dataset_type)\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    log(\"Dataset type detected: TIME-SERIES\", type=\"FOUND\", bold=True)\n",
                "    log(\"Available forecasting models: ARIMA (manual) and AUTO-ARIMA\", level=2, type=\"INFO\")\n",
                "    log(\"Suggestion: ARIMA (manual): use it when you trust the suggested (p, d, q) from ACF/PACF\", level=2, custom_icon=\"üí≠\")\n",
                "    log(\"Suggestion: AUTO-ARIMA: use it as baseline search ‚Üí detects seasonality automatically\", level=2, custom_icon=\"üí≠\")\n",
                "\n",
                "# ===============================\n",
                "# NLP and TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"NLP\" or dataset_type == \"TABULAR\":\n",
                "    # -------------------------------\n",
                "    # LOAD PROCESSED DATASET\n",
                "    # -------------------------------\n",
                "    try:\n",
                "        X_train_model = pd.read_csv(processed_data_output_path + \"X_train_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\")\n",
                "        X_test_model  = pd.read_csv(processed_data_output_path + \"X_test_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\")\n",
                "        if supervised_learning:\n",
                "            y_train_model = pd.read_csv(processed_data_output_path + \"y_train_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\").squeeze()\n",
                "            y_test_model  = pd.read_csv(processed_data_output_path + \"y_test_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\").squeeze()\n",
                "    except FileNotFoundError as e:\n",
                "        raise FileNotFoundError(\n",
                "            f\"‚ùå The selected revision rev={rev_to_use} does NOT exist.\\nMissing file: {e.filename}\")\n",
                "    # -------------------------------\n",
                "    # DATASET SHAPE HEURISTICS\n",
                "    # -------------------------------\n",
                "    n_samples  = X_train_model.shape[0]\n",
                "    n_features = X_train_model.shape[1]\n",
                "    if n_samples <= n_samples_SMALL_dataset:\n",
                "        size_label = \"SMALL\"\n",
                "    elif n_samples <= n_samples_MEDIUM_dataset:\n",
                "        size_label = \"MEDIUM\"\n",
                "    else:\n",
                "        size_label = \"BIG\"\n",
                "    if n_features <= n_features_SMALL_dataset:\n",
                "        feature_label = \"LOW-DIMENSIONAL\"\n",
                "    elif n_features <= n_features_MEDIUM_dataset:\n",
                "        feature_label = \"MEDIUM-DIMENSIONAL\"\n",
                "    else:\n",
                "        feature_label = \"HIGH-DIMENSIONAL\"\n",
                "    log(\"Scale of the dataset:\", type=\"INFO\")\n",
                "    log(f\"Dataset size category: {size_label} ({n_samples} rows)\", level=2, type=\"INFO\")\n",
                "    log(f\"Feature space category: {feature_label} ({n_features} features)\", level=2, type=\"INFO\")\n",
                "    # -------------------------------\n",
                "    # UNSUPERVISED\n",
                "    # -------------------------------\n",
                "    if not supervised_learning:\n",
                "        log(\"Learning mode detected: UNSUPERVISED\", type=\"FOUND\", bold=True)\n",
                "        # -------------------------------\n",
                "        # SAFE IMPORT OF STEP 6 METRICS (if they exist)\n",
                "        # -------------------------------\n",
                "        if \"sil_value_S6\" in locals():\n",
                "            sil_value_S16 = sil_value_S6\n",
                "        else:\n",
                "            sil_value_S16 = None\n",
                "        # -------------------------------\n",
                "        # QUICK DATA PROFILE (lightweight)\n",
                "        # -------------------------------\n",
                "        sample_n = 2000\n",
                "        if n_samples < sample_n:\n",
                "            X_profile = X_train_model.copy()\n",
                "        else:\n",
                "            X_profile = X_train_model.sample(n=sample_n, random_state=random_seed)\n",
                "        # Missing ratio (on sample)\n",
                "        missing_ratio = None\n",
                "        try:\n",
                "            missing_ratio = X_profile.isna().mean().mean()\n",
                "        except Exception:\n",
                "            missing_ratio = None\n",
                "        # Basic \"sparsity\" proxy (ratio of exact zeros, common in NLP vector spaces)\n",
                "        zero_ratio = None\n",
                "        try:\n",
                "            total_cells = X_profile.shape[0] * X_profile.shape[1]\n",
                "            if total_cells > 0:\n",
                "                zero_ratio = (X_profile.fillna(0) == 0).sum().sum() / total_cells\n",
                "        except Exception:\n",
                "            zero_ratio = None\n",
                "        # Quick scale diagnostic: do features look roughly standardized?\n",
                "        # (mean ~ 0 and std ~ 1 on average)\n",
                "        looks_scaled = None\n",
                "        try:\n",
                "            col_means = X_profile.mean(axis=0, skipna=True).abs()\n",
                "            col_stds  = X_profile.std(axis=0, skipna=True)\n",
                "            mean_of_means = float(col_means.mean())\n",
                "            mean_of_stds  = float(col_stds.replace(0, np.nan).mean())\n",
                "            if (mean_of_means < 0.2) and (mean_of_stds > 0.6) and (mean_of_stds < 1.6):\n",
                "                looks_scaled = True\n",
                "            else:\n",
                "                looks_scaled = False\n",
                "        except Exception:\n",
                "            looks_scaled = None\n",
                "        # -------------------------------\n",
                "        # DATA QUALITY WARNINGS\n",
                "        # -------------------------------\n",
                "        if missing_ratio is not None:\n",
                "            if missing_ratio > 0.01:\n",
                "                log(\"Missing values detected: consider imputation before clustering\", level=2, type=\"WARNING\")\n",
                "        if looks_scaled is not None:\n",
                "            if not looks_scaled:\n",
                "                log(\"Many clustering methods benefit from scaling: consider StandardScaler/RobustScaler\", level=2, type=\"WARNING\")\n",
                "        if (zero_ratio is not None) and (zero_ratio > 0.70) and (dataset_type == \"NLP\"):\n",
                "            log(\"High sparsity detected: prefer TruncatedSVD + MiniBatchKMeans / HDBSCAN\", level=2, type=\"WARNING\")\n",
                "        # -------------------------------\n",
                "        # REFINE RECOMMENDATIONS WITH SILHOUETTE\n",
                "        # -------------------------------\n",
                "        if sil_value_S16 is not None:\n",
                "            if sil_value_S16 >= 0.25:\n",
                "                log(f\"Clustering separability seems reasonable (silhouette={round(sil_value_S16, 3)} >= 0.25)\", level=2, type=\"SUCCESS\")\n",
                "            else:\n",
                "                log(\"Clustering separability seems weak (silhouette < 0.25): consider dim-reduction / anomaly detection\", level=2, type=\"WARNING\")\n",
                "        # -------------------------------\n",
                "        # INITIALIZE SUGGESTION LISTS (grouped)\n",
                "        # -------------------------------\n",
                "        suggested_unsupervised_models = []\n",
                "        suggested_dimred_steps         = []\n",
                "        # -------------------------------\n",
                "        # DIMENSIONALITY REDUCTION SUGGESTIONS\n",
                "        # -------------------------------\n",
                "        # High-dimensional (common in NLP / one-hot / TF-IDF): reduce before distance-based clustering\n",
                "        if feature_label == \"HIGH-DIMENSIONAL\":\n",
                "            if dataset_type == \"NLP\":\n",
                "                suggested_dimred_steps.append(\"TruncatedSVD\")\n",
                "            else:\n",
                "                suggested_dimred_steps.append(\"PCA\")\n",
                "        # Visualization-oriented\n",
                "        if size_label in [\"SMALL\", \"MEDIUM\"]:\n",
                "            suggested_dimred_steps.append(\"UMAP\")\n",
                "            suggested_dimred_steps.append(\"t-SNE\")\n",
                "        # -------------------------------\n",
                "        # CLUSTERING MODEL SUGGESTIONS\n",
                "        # -------------------------------\n",
                "        # (A) Distance-based (KMeans family)\n",
                "        if feature_label in [\"LOW-DIMENSIONAL\", \"MEDIUM-DIMENSIONAL\"]:\n",
                "            suggested_unsupervised_models.append(\"KMeans\")\n",
                "            if size_label not in [\"SMALL\", \"MEDIUM\"]:\n",
                "                suggested_unsupervised_models.append(\"MiniBatchKMeans\")\n",
                "        else:\n",
                "            # High-dimensional: still possible, but better after SVD/PCA\n",
                "            suggested_unsupervised_models.append(\"MiniBatchKMeans\")\n",
                "        # (B) Density-based\n",
                "        # DBSCAN struggles in high-D; HDBSCAN can be more flexible but still not magic in very high-D\n",
                "        if feature_label == \"LOW-DIMENSIONAL\":\n",
                "            suggested_unsupervised_models.append(\"DBSCAN\")\n",
                "            if size_label in [\"SMALL\", \"MEDIUM\"]:\n",
                "                suggested_unsupervised_models.append(\"HDBSCAN\")\n",
                "        elif feature_label == \"MEDIUM-DIMENSIONAL\":\n",
                "            suggested_unsupervised_models.append(\"HDBSCAN\")\n",
                "        else:\n",
                "            # High-dimensional: recommend density methods only after dim reduction\n",
                "            suggested_unsupervised_models.append(\"HDBSCAN\")\n",
                "        # (C) Hierarchical / graph-based\n",
                "        if size_label == \"SMALL\" and feature_label != \"HIGH-DIMENSIONAL\":\n",
                "            suggested_unsupervised_models.append(\"AgglomerativeClustering\")\n",
                "        if size_label == \"SMALL\" and feature_label == \"LOW-DIMENSIONAL\":\n",
                "            suggested_unsupervised_models.append(\"SpectralClustering\")\n",
                "        # (D) Very scalable alternative\n",
                "        if size_label in [\"MEDIUM\", \"BIG\"]:\n",
                "            suggested_unsupervised_models.append(\"Birch\")\n",
                "        # -------------------------------\n",
                "        # REMOVE DUPLICATES\n",
                "        # -------------------------------\n",
                "        suggested_dimred_steps         = list(dict.fromkeys(suggested_dimred_steps))\n",
                "        suggested_unsupervised_models  = list(dict.fromkeys(suggested_unsupervised_models))\n",
                "        # -------------------------------\n",
                "        # FINAL SUMMARY\n",
                "        # -------------------------------\n",
                "        print(\"\\n\")\n",
                "        if len(suggested_dimred_steps) > 0:\n",
                "            log(\"SUGGESTED DIMENSIONAL-REDUCTION STEPS üß©:\", custom_icon=\"üí≠\", bold=True)\n",
                "            for s in suggested_dimred_steps:\n",
                "                print(f\"     - {s}\")\n",
                "        else:\n",
                "            log(\"No DIMENSIONAL-REDUCTION üß© seems needed\", custom_icon=\"üí≠\", bold=True)\n",
                "        if len(suggested_unsupervised_models) > 0:\n",
                "            log(\"SUGGESTED CLUSTERING MODELS üß≠:\", custom_icon=\"üí≠\", bold=True)\n",
                "            for m in suggested_unsupervised_models:\n",
                "                print(f\"     - {m}\")\n",
                "        else:\n",
                "            log(\"No CLUSTERING MODEL üß≠ seems appropriate\", custom_icon=\"üí≠\", bold=True)\n",
                "    # -------------------------------\n",
                "    # SUPERVISED\n",
                "    # -------------------------------\n",
                "    else:\n",
                "        log(\"Learning mode detected: SUPERVISED\", type=\"FOUND\", bold=True)\n",
                "        # -------------------------------\n",
                "        # DETECT PROBLEM TYPE\n",
                "        # -------------------------------\n",
                "        y_unique_values = y_train_model.nunique()\n",
                "        y_unique_ratio  = y_unique_values / len(y_train_model) * 100\n",
                "\n",
                "        if (y_var_type == \"NUMERIC\") and (y_unique_ratio > var_type_proposal_threshold):\n",
                "            problem_type = \"REGRESSION\"\n",
                "        else:\n",
                "            problem_type = \"CLASSIFICATION\"\n",
                "        log(f\"Inferred problem type: {problem_type}\", type=\"FOUND\", bold=True)\n",
                "        log(f\"Target unique values (classes): {y_unique_values}\", level=2, type=\"INFO\")\n",
                "        # -------------------------------\n",
                "        # CLASS IMBALANCE CHECK\n",
                "        # -------------------------------\n",
                "        class_imbalance_ratio = None\n",
                "        if problem_type == \"CLASSIFICATION\":\n",
                "            class_imbalance_ratio = y_train_model.value_counts(normalize=True).max()\n",
                "            if class_imbalance_ratio > max_target_imbalanced_ratio_accepted:\n",
                "                log(f\"Target is highly imbalanced: {round(class_imbalance_ratio*100,1)}%\", level=2, type=\"WARNING\")\n",
                "            else:\n",
                "                log(f\"Target imbalance acceptable: {round(class_imbalance_ratio*100,1)}%\", level=2, type=\"SUCCESS\")\n",
                "        # -------------------------------\n",
                "        # INITIALIZE SUGGESTION LISTS\n",
                "        # -------------------------------\n",
                "        suggested_classification_models = []\n",
                "        suggested_regression_models     = []\n",
                "        # ==============================\n",
                "        # NLP DATASET\n",
                "        # ==============================\n",
                "        if dataset_type == \"NLP\":\n",
                "            # --------------------------\n",
                "            # CLASSIFICATION\n",
                "            # --------------------------\n",
                "            if problem_type == \"CLASSIFICATION\":\n",
                "                suggested_classification_models.append(\"LinearSVC\")\n",
                "                suggested_classification_models.append(\"LogisticRegression\")\n",
                "                if y_unique_values > 2:\n",
                "                    suggested_classification_models.append(\"MultinomialNB\")\n",
                "                if feature_label in [\"LOW-DIMENSIONAL\", \"MEDIUM-DIMENSIONAL\"]:\n",
                "                    suggested_classification_models.append(\"GaussianNB\")\n",
                "                if size_label in [\"SMALL\", \"MEDIUM\"]:\n",
                "                    suggested_classification_models.append(\"BernoulliNB\")\n",
                "                if size_label in [\"MEDIUM\", \"BIG\"]:\n",
                "                    suggested_classification_models.append(\"RandomForestClassifier\")\n",
                "                if n_features <= 50:\n",
                "                    suggested_classification_models.append(\"KNeighborsClassifier\")\n",
                "            # --------------------------\n",
                "            # REGRESSION\n",
                "            # --------------------------\n",
                "            else:\n",
                "                suggested_regression_models.append(\"LinearRegression\")\n",
                "                suggested_regression_models.append(\"RandomForestRegressor\")\n",
                "                if size_label in [\"MEDIUM\", \"BIG\"]:\n",
                "                    suggested_regression_models.append(\"GradientBoostingRegressor\")\n",
                "                if n_features <= 50:\n",
                "                    suggested_regression_models.append(\"KNeighborsRegressor\")\n",
                "        # ==============================\n",
                "        # TABULAR DATASET\n",
                "        # ==============================\n",
                "        if dataset_type == \"TABULAR\":\n",
                "            # -------------------------------\n",
                "            # SAFE IMPORT OF STEP 6 METRICS (if they exist)\n",
                "            # -------------------------------\n",
                "            if \"sil_value_S6\" in locals():\n",
                "                sil_value_S16 = sil_value_S6\n",
                "            else:\n",
                "                sil_value_S16 = None\n",
                "\n",
                "            if \"fisher_scores_S6\" in locals():\n",
                "                fisher_scores_S16 = fisher_scores_S6\n",
                "                fisher_max_S16 = fisher_scores_S16.max()\n",
                "            else:\n",
                "                fisher_scores_S16 = None\n",
                "                fisher_max_S16 = None\n",
                "            # -------------------------------\n",
                "            # CLASSIFICATION\n",
                "            # -------------------------------\n",
                "            if problem_type == \"CLASSIFICATION\":\n",
                "                suggested_classification_models.append(\"LogisticRegression\")\n",
                "                if len(numeric_att) > len(category_att):\n",
                "                    suggested_classification_models.append(\"GaussianNB\")\n",
                "                if (X_train_model.nunique() <= 2).sum() > (0.5 * n_features):\n",
                "                    suggested_classification_models.append(\"BernoulliNB\")\n",
                "                if (X_train_model.dtypes == \"int64\").sum() > 0:\n",
                "                    suggested_classification_models.append(\"MultinomialNB\")\n",
                "                # Tree models depend on dataset size\n",
                "                if size_label == \"SMALL\":\n",
                "                    suggested_classification_models.append(\"DecisionTreeClassifier\")\n",
                "                    suggested_classification_models.append(\"RandomForestClassifier\")\n",
                "                if size_label == \"MEDIUM\":\n",
                "                    suggested_classification_models.append(\"RandomForestClassifier\")\n",
                "                    suggested_classification_models.append(\"GradientBoostingClassifier\")\n",
                "                if size_label == \"BIG\":\n",
                "                    suggested_classification_models.append(\"RandomForestClassifier\")\n",
                "                    suggested_classification_models.append(\"XGBClassifier\")\n",
                "                    suggested_classification_models.append(\"LGBMClassifier\")\n",
                "                # Imbalance\n",
                "                if (class_imbalance_ratio is not None) and (class_imbalance_ratio > max_target_imbalanced_ratio_accepted):\n",
                "                    log(\"Imbalance: boosting / RF recommended.\", level=2, type=\"INFO\")\n",
                "                # -------------------------------\n",
                "                # KNN CLASSIFIER HEURISTICS\n",
                "                # -------------------------------\n",
                "                knn_ok = True\n",
                "                # Bad for very high dimensional space (curse of dimensionality)\n",
                "                if feature_label == \"HIGH-DIMENSIONAL\":\n",
                "                    knn_ok = False\n",
                "                # Bad for very large datasets (O(N) per prediction)\n",
                "                if size_label == \"BIG\":\n",
                "                    knn_ok = False\n",
                "                # Improve suggestion if geometric separability is strong\n",
                "                if sil_value_S16 is not None:\n",
                "                    if sil_value_S16 >= 0.25:\n",
                "                        knn_ok = True\n",
                "                if fisher_max_S16 is not None:\n",
                "                    if fisher_max_S16 >= 0.5:\n",
                "                        knn_ok = True\n",
                "                if knn_ok:\n",
                "                    suggested_classification_models.append(\"KNeighborsClassifier\")\n",
                "            # -------------------------------\n",
                "            # REGRESSION\n",
                "            # -------------------------------\n",
                "            else:\n",
                "                suggested_regression_models.append(\"LinearRegression\")\n",
                "                if feature_label in [\"MEDIUM-DIMENSIONAL\", \"HIGH-DIMENSIONAL\"]:\n",
                "                    suggested_regression_models.append(\"Lasso\")\n",
                "                    suggested_regression_models.append(\"Ridge\")\n",
                "                if size_label in [\"MEDIUM\", \"BIG\"]:\n",
                "                    suggested_regression_models.append(\"RandomForestRegressor\")\n",
                "                    suggested_regression_models.append(\"GradientBoostingRegressor\")\n",
                "                if size_label == \"BIG\":\n",
                "                    suggested_regression_models.append(\"XGBRegressor\")\n",
                "                    suggested_regression_models.append(\"LGBMRegressor\")\n",
                "                # -------------------------------\n",
                "                # KNN REGRESSOR HEURISTICS\n",
                "                # -------------------------------\n",
                "                knn_r_ok = True\n",
                "                # Regression KNN suffers more from high-dimensionality\n",
                "                if feature_label == \"HIGH-DIMENSIONAL\":\n",
                "                    knn_r_ok = False\n",
                "                if size_label == \"BIG\":\n",
                "                    knn_r_ok = False\n",
                "                if sil_value_S16 is not None:\n",
                "                    if sil_value_S16 >= 0.20:\n",
                "                        knn_r_ok = True\n",
                "                if fisher_max_S16 is not None:\n",
                "                    if fisher_max_S16 >= 0.4:\n",
                "                        knn_r_ok = True\n",
                "                if knn_r_ok:\n",
                "                    suggested_regression_models.append(\"KNeighborsRegressor\")\n",
                "        # ==============================\n",
                "        # REMOVE DUPLICATES\n",
                "        # ==============================\n",
                "        suggested_classification_models = list(dict.fromkeys(suggested_classification_models))\n",
                "        suggested_regression_models     = list(dict.fromkeys(suggested_regression_models))\n",
                "        # -------------------------------\n",
                "        # FINAL SUMMARY\n",
                "        # -------------------------------\n",
                "        print(\"\\n\")\n",
                "        if len(suggested_classification_models) > 0:\n",
                "            log(\"SUGGESTED CLASSIFICATION MODELS ‚öñÔ∏è:\", custom_icon=\"üí≠\", bold=True)\n",
                "            for m in suggested_classification_models:\n",
                "                print(f\"     - {m}\")\n",
                "        else:\n",
                "            log(\"No CLASSIFICATION MODEL ‚öñÔ∏è seems appropriate\", custom_icon=\"üí≠\", bold=True)\n",
                "        if len(suggested_regression_models) > 0:\n",
                "            log(\"SUGGESTED REGRESSION MODELS üìà:\", custom_icon=\"üí≠\", bold=True)\n",
                "            for m in suggested_regression_models:\n",
                "                print(f\"     - {m}\")\n",
                "        else:\n",
                "            log(\"No REGRESSION MODEL üìà seems appropriate\", custom_icon=\"üí≠\", bold=True)\n",
                "        if (len(suggested_classification_models)==0) and (len(suggested_regression_models)==0):\n",
                "            log(\"No clear suggestion could be made. Review the target variable manually.\", type=\"WARNING\", bold=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 17 - PREDICTION MODELS SELECTED"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TABULAR or NLP</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# -------------------------------\n",
                "# SUPERVISED LEARNING\n",
                "# -------------------------------\n",
                "# Optimization parameters\n",
                "grid_cross_validation = 10                  # Parameter cv for GridSearchCV\n",
                "classification_scoring_target = \"f1\"        # Choose between: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
                "regression_scoring_target = \"r2\"            # Choose between: \"neg_root_mean_squared_error\", \"r2\"\n",
                "# Models selection\n",
                "classification_models_selection = {\n",
                "    \"LogisticRegression\": False,\n",
                "    \"DecisionTreeClassifier\": True,\n",
                "    \"RandomForestClassifier\": True,\n",
                "    \"AdaBoostClassifier\": False,\n",
                "    \"GradientBoostingClassifier\": False,\n",
                "    \"XGBClassifier\": False,\n",
                "    \"LGBMClassifier\": False,\n",
                "    \"LinearSVC\": False,\n",
                "    \"MultinomialNB\": False,\n",
                "    \"GaussianNB\": False,\n",
                "    \"BernoulliNB\": False,\n",
                "    \"KNeighborsClassifier\": True}\n",
                "regression_models_selection = {\n",
                "    \"LinearRegression\": False,\n",
                "    \"DecisionTreeRegressor\": False,\n",
                "    \"RandomForestRegressor\": False,\n",
                "    \"AdaBoostRegressor\": False,\n",
                "    \"GradientBoostingRegressor\": False,\n",
                "    \"XGBRegressor\": False,\n",
                "    \"LGBMRegressor\": False,\n",
                "    \"Lasso\": False,\n",
                "    \"Ridge\": False,\n",
                "    \"KNeighborsRegressor\": False}\n",
                "# -------------------------------\n",
                "# UNSUPERVISED LEARNING\n",
                "# -------------------------------\n",
                "# Dimensionality reduction\n",
                "reduce_dimensionality   = False\n",
                "use_PCA                 = False\n",
                "use_TruncatedSVD        = False\n",
                "# Models selection\n",
                "cluster_models_selection = {\n",
                "    \"KMeans\": True,\n",
                "    \"DBSCAN\": True,\n",
                "    \"AgglomerativeClustering\": False}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### <span style=\"color:orange\">Inputs for TIME-SERIES</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prediction_horizon = 360"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "if supervised_learning:\n",
                "    print(\"STEP 17) PREDICTION MODELS SELECTED - \",learning_type, dataset_type)\n",
                "elif not supervised_learning and not pseudo_tagging:\n",
                "    print(\"STEP 17) PREDICTION MODELS SELECTED - \",learning_type, dataset_type)\n",
                "elif not supervised_learning and pseudo_tagging:\n",
                "    print(\"STEP 17) PREDICTION MODELS SELECTED - \",learning_type, dataset_type, \"+ PSEUDO TAGGING + SUPERVISED CLASSIFICATION\")\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    # -------------------------------\n",
                "    #  ARIMA MODEL WITH SUGGESTED VALUES\n",
                "    # -------------------------------\n",
                "    # Copy previous step data\n",
                "    df_timeseries_S16 = df_timeseries_S6.copy()\n",
                "    # Instance ARIMA model\n",
                "    arima_model = ARIMA(endog = df_timeseries_S16, order = (suggested_p, suggested_d, suggested_q))\n",
                "    # Train ARIMA model\n",
                "    arima_result = arima_model.fit()\n",
                "    # Predict with trained ARIMA model\n",
                "    prediction_ARIMA = arima_result.predict(start  = len(df_timeseries_S16), end= len(df_timeseries_S16) + prediction_horizon)\n",
                "    # -------------------------------\n",
                "    #  AUTO ARIMA\n",
                "    # -------------------------------\n",
                "    # Decide seasonal behaviour for auto_arima\n",
                "    auto_arima_m = get_auto_arima_m(period=period_S6,seasonal_peaks = seasonal_peaks_S6)\n",
                "    if auto_arima_m > 1:\n",
                "        auto_arima_seasonal = True\n",
                "    else:\n",
                "        auto_arima_seasonal = False\n",
                "    # Instance AUTO ARIMA model\n",
                "    auto_arima_model = auto_arima(y=df_timeseries_S16, seasonal = auto_arima_seasonal, trace = False, m = auto_arima_m)\n",
                "    # Retrieve orders\n",
                "    auto_p, auto_d, auto_q = auto_arima_model.order\n",
                "    # Predict with trained AUTO ARIMA model\n",
                "    prediction_AUTO_ARIMA = auto_arima_model.predict(prediction_horizon)\n",
                "    # -------------------------------\n",
                "    #  COMPARISON\n",
                "    # -------------------------------\n",
                "    log(f\"ARIMA (manual): selected order (p,d,q)=({suggested_p},{suggested_d},{suggested_q})\", level=1, type=\"INFO\")\n",
                "    log(f\"AUTO-ARIMA: selected order (p,d,q)=({auto_p},{auto_d},{auto_q}) with seasonal={auto_arima_seasonal}, m={auto_arima_m}\", level=1, type=\"INFO\")\n",
                "\n",
                "    if (suggested_p == auto_p) and (suggested_d == auto_d) and (suggested_q == auto_q):\n",
                "        log(f\"ARIMA vs AUTO-ARIMA: Orders MATCH\", level=1, type=\"SUCCESS\", bold=True)\n",
                "    else:\n",
                "        log(f\"ARIMA vs AUTO-ARIMA: Orders DO NOT MATCH\", level=1, type=\"WARNING\", bold=True)\n",
                "    # -------------------------------\n",
                "    #  FORECAST PLOT ‚Äî ARIMA\n",
                "    # -------------------------------\n",
                "    fig_arima, ax_arima = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "    # Plot original series\n",
                "    ax_arima.plot(df_timeseries_S16, label=\"Original Time-series\")\n",
                "    # Plot ARIMA forecast\n",
                "    ax_arima.plot(prediction_ARIMA, label=\"Forecast (ARIMA)\", color=\"red\", linewidth=5, linestyle=\"dashed\")\n",
                "    # Title, labels, ticks and legend\n",
                "    ax_arima.set_title(label=f\"Forecast with ARIMA(p={suggested_p}, d={suggested_d}, q={suggested_q})\", fontsize=plot_title_font_size)\n",
                "    ax_arima.set_xlabel(xlabel=df_timeseries_S16.index.name, fontsize=plot_label_font_size)\n",
                "    ax_arima.set_ylabel(ylabel=df_timeseries_S16.name, fontsize=plot_label_font_size)\n",
                "    ax_arima.tick_params(labelsize=plot_tick_font_size)\n",
                "    ax_arima.legend(fontsize=plot_text_font_size)\n",
                "    ax_arima.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "    # Show plot\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    # -------------------------------\n",
                "    #  FORECAST PLOT ‚Äî AUTO ARIMA\n",
                "    # -------------------------------\n",
                "    fig_auto, ax_auto_arima = plt.subplots(nrows=1, ncols=1, figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
                "    # Plot original series\n",
                "    ax_auto_arima.plot(df_timeseries_S16, label=\"Original Time-series\")\n",
                "    # Plot AUTO-ARIMA forecast\n",
                "    ax_auto_arima.plot(prediction_AUTO_ARIMA, label=\"Forecast (AUTO-ARIMA)\", color=\"green\", linewidth=5, linestyle=\"dashed\")\n",
                "    # Title, labels, ticks and legend\n",
                "    ax_auto_arima.set_title(label=f\"Forecast with AUTO-ARIMA(p={auto_p}, d={auto_d}, q={auto_q})\", fontsize=plot_title_font_size)\n",
                "    ax_auto_arima.set_xlabel(xlabel=df_timeseries_S16.index.name, fontsize=plot_label_font_size)\n",
                "    ax_auto_arima.set_ylabel(ylabel=df_timeseries_S16.name, fontsize=plot_label_font_size)\n",
                "    ax_auto_arima.tick_params(labelsize=plot_tick_font_size)\n",
                "    ax_auto_arima.legend(fontsize=plot_text_font_size)\n",
                "    ax_auto_arima.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
                "    # Show plot\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# ===============================\n",
                "# NLP and TABULAR DATASET\n",
                "# ===============================\n",
                "elif dataset_type == \"NLP\" or dataset_type == \"TABULAR\":\n",
                "    # Load processed data according to selected revision\n",
                "    try:\n",
                "        X_train_model = pd.read_csv(processed_data_output_path + \"X_train_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\")\n",
                "        X_test_model  = pd.read_csv(processed_data_output_path + \"X_test_final\"  + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\")\n",
                "        if supervised_learning:\n",
                "            y_train_model = pd.read_csv(processed_data_output_path + \"y_train_final\" + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\").squeeze()\n",
                "            y_test_model  = pd.read_csv(processed_data_output_path + \"y_test_final\"  + \"_\" + learning_type + \"_\" + dataset_type + \"_\" + str(rev_number) + \".csv\").squeeze()\n",
                "        n_samples  = X_train_model.shape[0]\n",
                "        n_features = X_train_model.shape[1]\n",
                "    except FileNotFoundError as e:\n",
                "        raise FileNotFoundError(\n",
                "            f\"‚ùå The selected revision rev={rev_to_use} does NOT exist.\\nMissing file: {e.filename}\\nPlease check available revision numbers in '../data/processed/'.\")\n",
                "    log(f\"Loaded datasets with revision {rev_to_use}\", type=\"INFO\")\n",
                "    if supervised_learning:\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "        #  SUPERVISED PIPELINE\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "\n",
                "        # -------------------------------\n",
                "        #  GRIDS FOR OPIMIZATION\n",
                "        # -------------------------------\n",
                "        # Store number of parameters for the grid\n",
                "        num_grid_param =  n_grid_param(\n",
                "                                    n_samples=X_train_model.shape[0],\n",
                "                                    n_features=X_train_model.shape[1],\n",
                "                                    n_features_BIG_dataset=n_features_BIG_dataset,\n",
                "                                    n_features_MEDIUM_dataset=n_features_MEDIUM_dataset,\n",
                "                                    n_features_SMALL_dataset=n_features_SMALL_dataset,\n",
                "                                    n_samples_BIG_dataset=n_samples_BIG_dataset,\n",
                "                                    n_samples_MEDIUM_dataset=n_samples_MEDIUM_dataset,\n",
                "                                    n_samples_SMALL_dataset=n_samples_SMALL_dataset,\n",
                "                                    n_samples_MICRO_dataset=n_samples_MICRO_dataset)\n",
                "        classification_grids = {\n",
                "            \"LogisticRegression\": {\n",
                "                \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
                "                \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
                "                \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
                "            },\n",
                "            \"DecisionTreeClassifier\": {\n",
                "                \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
                "                \"max_depth\": [None, 5, 10, 20, 30],\n",
                "                \"min_samples_split\": [2, 5, 10, 20, 30]\n",
                "            },\n",
                "            \"RandomForestClassifier\": {\n",
                "                \"n_estimators\": [5, 10, 20, 50, 100, 200],\n",
                "                \"max_depth\": [None, 2, 5, 10, 15, 20]\n",
                "            },\n",
                "            \"AdaBoostClassifier\": {\n",
                "                \"n_estimators\" : smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\" : smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"GradientBoostingClassifier\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\",num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\",num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"XGBClassifier\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"LGBMClassifier\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"LinearSVC\": {\n",
                "                \"C\": smart_logspace_choose(0.1, 10, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"MultinomialNB\": {},\n",
                "            \"GaussianNB\": {},\n",
                "            \"BernoulliNB\": {},\n",
                "            \"KNeighborsClassifier\": {\n",
                "                \"n_neighbors\": smart_logspace_choose(1, max(10, int(min(50, n_samples ** 0.5))), \"integer\", num_grid_param),\n",
                "                \"metric\": [resolve_distance_metric(dataset_type)],\n",
                "                \"weights\": [\"uniform\", \"distance\"]\n",
                "            }\n",
                "        }\n",
                "        regression_grids = {\n",
                "            \"LinearRegression\": {},\n",
                "            \"Lasso\": {\n",
                "                \"alpha\": [0.001, 0.005, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50],\n",
                "                \"max_iter\": [1000, 5000, 10000]\n",
                "            },\n",
                "            \"Ridge\": {\n",
                "                \"alpha\": [0.001, 0.005, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50],\n",
                "                \"max_iter\": [1000, 5000, 10000]\n",
                "            },\n",
                "            \"DecisionTreeRegressor\": {\n",
                "                \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
                "                \"max_depth\": [None, 5, 10, 20, 30],\n",
                "                \"min_samples_split\": [2, 5, 10, 20, 30]\n",
                "            },\n",
                "            \"RandomForestRegressor\": {\n",
                "                \"n_estimators\": [5, 10, 20, 50, 100, 200],\n",
                "                \"max_depth\": [None, 2, 5, 10, 15, 20]\n",
                "            },\n",
                "            \"AdaBoostRegressor\": {\n",
                "                \"n_estimators\" : smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\" : smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"GradientBoostingRegressor\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"XGBRegressor\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"LGBMRegressor\": {\n",
                "                \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "            },\n",
                "            \"KNeighborsRegressor\": {\n",
                "                \"n_neighbors\": smart_logspace_choose(1, max(10, int(min(50, n_samples ** 0.5))), \"integer\", num_grid_param),\n",
                "                \"metric\": [resolve_distance_metric(dataset_type)],\n",
                "                \"weights\": [\"uniform\", \"distance\"]\n",
                "            }\n",
                "        }\n",
                "        # -------------------------------\n",
                "        #  DEFAULT MODELS\n",
                "        # -------------------------------\n",
                "        classification_available = {\n",
                "            \"LogisticRegression\": LogisticRegression(random_state=random_seed),\n",
                "            \"RandomForestClassifier\": RandomForestClassifier(random_state=random_seed),\n",
                "            \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=random_seed),\n",
                "            \"AdaBoostClassifier\": AdaBoostClassifier(random_state=random_seed),\n",
                "            \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=random_seed),\n",
                "            \"XGBClassifier\": XGBClassifier(random_state=random_seed, eval_metric=map_xgb_eval_metric(\"XGBClassifier\", classification_scoring_target), use_label_encoder=False),\n",
                "            \"LGBMClassifier\": LGBMClassifier(random_state=random_seed),\n",
                "            \"LinearSVC\": SVC(random_state=random_seed),\n",
                "            \"MultinomialNB\": MultinomialNB(),\n",
                "            \"GaussianNB\": GaussianNB(),\n",
                "            \"BernoulliNB\": BernoulliNB(),\n",
                "            \"KNeighborsClassifier\": KNeighborsClassifier()\n",
                "        }\n",
                "        regression_available = {\n",
                "            \"LinearRegression\": LinearRegression(),\n",
                "            \"Lasso\": Lasso(random_state=random_seed),\n",
                "            \"Ridge\": Ridge(random_state=random_seed),\n",
                "            \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=random_seed),\n",
                "            \"RandomForestRegressor\": RandomForestRegressor(random_state=random_seed),\n",
                "            \"AdaBoostRegressor\": AdaBoostRegressor(random_state=random_seed),\n",
                "            \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=random_seed),\n",
                "            \"XGBRegressor\": XGBRegressor(random_state=random_seed, eval_metric=map_xgb_eval_metric(\"XGBRegressor\", regression_scoring_target)),\n",
                "            \"LGBMRegressor\": LGBMRegressor(random_state=random_seed),\n",
                "            \"KNeighborsRegressor\": KNeighborsRegressor()\n",
                "        }\n",
                "        # -------------------------------\n",
                "        #  SELECTED MODELS\n",
                "        # -------------------------------\n",
                "        classification_selected_list = []\n",
                "        regression_selected_list = []\n",
                "        for model_name, active in classification_models_selection.items():\n",
                "            if active:\n",
                "                classification_selected_list.append(model_name)\n",
                "        for model_name, active in regression_models_selection.items():\n",
                "            if active:\n",
                "                regression_selected_list.append(model_name)\n",
                "        # Auto-add Lasso/Ridge if LinearRegression is selected\n",
                "        if \"LinearRegression\" in regression_selected_list:\n",
                "            if \"Lasso\" not in regression_selected_list:\n",
                "                regression_selected_list.append(\"Lasso\")\n",
                "            if \"Ridge\" not in regression_selected_list:\n",
                "                regression_selected_list.append(\"Ridge\")\n",
                "        log(f\"Selected CLASSIFICATION models: {classification_selected_list}\", type=\"INFO\")\n",
                "        log(f\"Selected REGRESSION models: {regression_selected_list}\", type=\"INFO\")\n",
                "        # Parameters needed for classification metrics\n",
                "        if len(classification_selected_list) > 0:\n",
                "            proposed_avg, proposed_pos, proposed_score = set_average_proposal(y_train_model, classification_scoring_target)\n",
                "        # -------------------------------\n",
                "        #  ESTIMATED COMPUTING TIME\n",
                "        # -------------------------------\n",
                "        # Estimated computing time per fit in seconds\n",
                "        time_per_fit_factor =  set_time_per_fit_factor(\n",
                "                                n_samples=X_train_model.shape[0],\n",
                "                                n_features=X_train_model.shape[1],\n",
                "                                n_features_BIG_dataset=n_features_BIG_dataset,\n",
                "                                n_features_MEDIUM_dataset=n_features_MEDIUM_dataset,\n",
                "                                n_features_SMALL_dataset=n_features_SMALL_dataset,\n",
                "                                n_samples_BIG_dataset=n_samples_BIG_dataset,\n",
                "                                n_samples_MEDIUM_dataset=n_samples_MEDIUM_dataset,\n",
                "                                n_samples_SMALL_dataset=n_samples_SMALL_dataset)\n",
                "        computing_time_per_fit = {\n",
                "            \"LogisticRegression\": 0.05 * time_per_fit_factor,\n",
                "            \"DecisionTreeClassifier\": 0.05 * time_per_fit_factor,\n",
                "            \"RandomForestClassifier\": 0.12 * time_per_fit_factor,\n",
                "            \"AdaBoostClassifier\": 0.12 * time_per_fit_factor,\n",
                "            \"GradientBoostingClassifier\": 0.35 * time_per_fit_factor,\n",
                "            \"XGBClassifier\": 0.08 * time_per_fit_factor,\n",
                "            \"LGBMClassifier\": 0.05 * time_per_fit_factor,\n",
                "            \"LinearSVC\": 0.08 * time_per_fit_factor,\n",
                "            \"MultinomialNB\": 0.03 * time_per_fit_factor,\n",
                "            \"GaussianNB\": 0.03 * time_per_fit_factor,\n",
                "            \"BernoulliNB\": 0.03 * time_per_fit_factor,\n",
                "            \"KNeighborsClassifier\": 0.05 * time_per_fit_factor,\n",
                "            \"LinearRegression\": 0.05 * time_per_fit_factor,\n",
                "            \"DecisionTreeRegressor\": 0.05 * time_per_fit_factor,\n",
                "            \"RandomForestRegressor\": 0.12 * time_per_fit_factor,\n",
                "            \"Lasso\": 0.07 * time_per_fit_factor,\n",
                "            \"Ridge\": 0.07 * time_per_fit_factor,\n",
                "            \"AdaBoostRegressor\": 0.12 * time_per_fit_factor,\n",
                "            \"GradientBoostingRegressor\": 0.35 * time_per_fit_factor,\n",
                "            \"XGBRegressor\": 0.08 * time_per_fit_factor,\n",
                "            \"LGBMRegressor\": 0.5 * time_per_fit_factor,\n",
                "            \"KNeighborsRegressor\": 0.05 * time_per_fit_factor\n",
                "        }\n",
                "        # Prints total number of model fits\n",
                "        expected_time = 0\n",
                "        log(\"Number of fits required per model:\", type=\"INFO\")\n",
                "        for model_name in classification_selected_list:\n",
                "            grid_params = classification_grids[model_name]\n",
                "            total_fits = count_grid_combinations(grid_params, grid_cross_validation)\n",
                "            time_per_fit = computing_time_per_fit.get(model_name, 0.10)\n",
                "            expected_time = expected_time + total_fits * time_per_fit\n",
                "            log(f\"{model_name} ‚Äî total model fits required: {total_fits}\", level=2, type=\"FOUND\")\n",
                "        for model_name in regression_selected_list:\n",
                "            grid_params = regression_grids[model_name]\n",
                "            total_fits = count_grid_combinations(grid_params, grid_cross_validation)\n",
                "            time_per_fit = computing_time_per_fit.get(model_name, 0.10)\n",
                "            expected_time = expected_time + total_fits * time_per_fit\n",
                "            log(f\"{model_name} ‚Äî total model fits required: {total_fits}\", level=2, type=\"FOUND\")\n",
                "        log(f\"Expected Time to Complete (ETC): {math.floor(0.9*expected_time/60)}m {round(0.9*expected_time % 60)}s - {math.floor(1.1 * expected_time/60)}m {round(1.1 * expected_time % 60)}s\", type=\"INFO\", bold=True)\n",
                "        print(\"============================================================\\n\")\n",
                "        # ===============================\n",
                "        #  CLASSIFICATION MODELS\n",
                "        # ===============================\n",
                "        trained_models = {} \n",
                "        default_results_class = {}\n",
                "        optimized_results_class = {}\n",
                "        for model_name in classification_selected_list:\n",
                "            # Instance DEFAULT model\n",
                "            default_model = classification_available[model_name] \n",
                "            # Train DEFAULT model\n",
                "            default_model.fit(X_train_model, y_train_model)\n",
                "            # Predict with trained DEFAULT model\n",
                "            y_pred_train = default_model.predict(X_train_model)\n",
                "            y_pred_test  = default_model.predict(X_test_model)\n",
                "            # Calculate metricts for DEFAULT model\n",
                "            metrics_train = compute_classification_metrics(y_train_model, y_pred_train, proposed_avg, proposed_pos)\n",
                "            metrics_test  = compute_classification_metrics(y_test_model, y_pred_test, proposed_avg, proposed_pos)\n",
                "            # Build final table with results for DEFAULT model\n",
                "            default_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = metrics_train\n",
                "            default_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = metrics_test\n",
                "            # Store trained model\n",
                "            trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
                "            # Set GRID parameters\n",
                "            grid_params = classification_grids[model_name]\n",
                "            if len(grid_params) > 0:\n",
                "                # Instance GRID\n",
                "                grid = GridSearchCV(\n",
                "                    estimator = classification_available[model_name],\n",
                "                    param_grid = grid_params,\n",
                "                    scoring = proposed_score,\n",
                "                    cv = grid_cross_validation)\n",
                "                # Train GRID\n",
                "                grid.fit(X_train_model, y_train_model)\n",
                "                # Get best estimator configuration (OPTIMIZED model)\n",
                "                best_model = grid.best_estimator_\n",
                "                # Predict with trained OPTIMIZED model\n",
                "                y_train_opt = best_model.predict(X_train_model)\n",
                "                y_test_opt  = best_model.predict(X_test_model)\n",
                "                # Calculate metricts for OPTIMIZED model\n",
                "                metrics_train_opt = compute_classification_metrics(y_train_model, y_train_opt, proposed_avg, proposed_pos)\n",
                "                metrics_test_opt  = compute_classification_metrics(y_test_model,  y_test_opt,  proposed_avg, proposed_pos)\n",
                "                # Build final table with results for OPTIMIZED model\n",
                "                optimized_results_class[\"OPTIMIZED \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_, \"Scoring\": classification_scoring_target}\n",
                "                optimized_results_class[\"OPTIMIZED \" + model_name + \" - üß™ TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_, \"Scoring\": classification_scoring_target}\n",
                "                # Store trained model\n",
                "                trained_models[model_name][\"optimized\"] = best_model\n",
                "                # Store hyperparameters for RandomForestClassifier\n",
                "                if model_name == \"RandomForestClassifier\":\n",
                "                    # Extract grid search raw values\n",
                "                    est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                    depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                    scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                    # Unique n_estimators\n",
                "                    est_unique = np.sort(np.unique(est_values))\n",
                "                    # Clean depths (convert to Python None or int)\n",
                "                    cleaned_depths = []\n",
                "                    for d in depth_values:\n",
                "                        if d is None:\n",
                "                            cleaned_depths.append(None)\n",
                "                        else:\n",
                "                            cleaned_depths.append(int(d))\n",
                "                    # Unique depths excluding None\n",
                "                    depth_unique = []\n",
                "                    for d in cleaned_depths:\n",
                "                        if d is not None and d not in depth_unique:\n",
                "                            depth_unique.append(d)\n",
                "                    depth_unique = sorted(depth_unique)\n",
                "                    # Dictionary: key = max_depth ‚Üí list of mean scores over n_estimators\n",
                "                    depth_score_dict = {}\n",
                "                    for d in depth_unique:\n",
                "                        mean_scores = []\n",
                "                        for e in est_unique:\n",
                "                            mask = (est_values == e) & (np.array(cleaned_depths, dtype=object) == d)\n",
                "                            mean_scores.append(scores[mask].mean())\n",
                "                        depth_score_dict[d] = mean_scores\n",
                "                    # Store internally\n",
                "                    trained_models[model_name][\"est_list\"]   = est_unique\n",
                "                    trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                    trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                # Store hyperparameters for Boosting models\n",
                "                if model_name in [\"GradientBoostingClassifier\", \"XGBClassifier\", \"LGBMClassifier\"]:\n",
                "                    # Retrieve raw values\n",
                "                    est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                    depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                    scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                    # Unique n_estimators\n",
                "                    est_unique = np.sort(np.unique(est_values))\n",
                "                    # Clean depths (convert to int)\n",
                "                    cleaned_depths = []\n",
                "                    for d in depth_values:\n",
                "                        if d is None:\n",
                "                            cleaned_depths.append(None)\n",
                "                        else:\n",
                "                            cleaned_depths.append(int(d))\n",
                "                    # Depth unique (exclude None)\n",
                "                    depth_unique = []\n",
                "                    for d in cleaned_depths:\n",
                "                        if d is not None and d not in depth_unique:\n",
                "                            depth_unique.append(d)\n",
                "                    depth_unique = sorted(depth_unique)\n",
                "                    # Dictionary: depth ‚Üí scores list\n",
                "                    depth_score_dict = {}\n",
                "                    for depth in depth_unique:\n",
                "                        mean_scores = []\n",
                "                        for est in est_unique:\n",
                "                            mask = (est_values == est) & (np.array(cleaned_depths, dtype=object) == depth)\n",
                "                            mean_scores.append(scores[mask].mean())\n",
                "                        depth_score_dict[depth] = mean_scores\n",
                "                    # Store internally\n",
                "                    trained_models[model_name][\"est_list\"] = est_unique\n",
                "                    trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                    trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                # Store hyperparameter K for KNN classifier\n",
                "                if model_name == \"KNeighborsClassifier\":\n",
                "                    # Retrieve k_list from grid OR build manually\n",
                "                    if \"param_n_neighbors\" in grid.cv_results_:\n",
                "                        k_list = np.sort(np.unique(grid.cv_results_[\"param_n_neighbors\"].data.astype(int)))\n",
                "                    else:\n",
                "                        # Fallback ‚Üí use same values as grid input\n",
                "                        k_list = classification_grids[\"KNeighborsClassifier\"][\"n_neighbors\"]\n",
                "                    train_scores = []\n",
                "                    test_scores  = []\n",
                "                    for k in k_list:\n",
                "                        tmp_model = KNeighborsClassifier(n_neighbors=int(k), metric=grid.best_params_.get(\"metric\", \"euclidean\"), weights=grid.best_params_.get(\"weights\", \"uniform\"))\n",
                "                        tmp_model.fit(X_train_model, y_train_model)\n",
                "                        # Compute train & test scores\n",
                "                        train_scores.append(tmp_model.score(X_train_model, y_train_model))\n",
                "                        test_scores.append(tmp_model.score(X_test_model, y_test_model))\n",
                "                    # Store internally for plotting phase\n",
                "                    trained_models[model_name][\"k_list_train_test\"] = (k_list, train_scores, test_scores)\n",
                "            else:\n",
                "                optimized_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "                optimized_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "        # ===============================\n",
                "        #  REGRESSION MODELS\n",
                "        # ===============================\n",
                "        default_results_reg = {}\n",
                "        optimized_results_reg = {}\n",
                "        for model_name in regression_selected_list:\n",
                "            # Instance DEFAULT model\n",
                "            default_model = regression_available[model_name]\n",
                "            # Train DEFAULT model\n",
                "            default_model.fit(X_train_model, y_train_model)\n",
                "            # Predict with trained DEFAULT model\n",
                "            y_pred_train = default_model.predict(X_train_model)\n",
                "            y_pred_test  = default_model.predict(X_test_model)\n",
                "            # Calculate metricts for DEFAULT model\n",
                "            metrics_train = compute_regression_metrics(y_train_model, y_pred_train)\n",
                "            metrics_test  = compute_regression_metrics(y_test_model, y_pred_test)\n",
                "            # Build final table with results for DEFAULT model\n",
                "            default_results_reg[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = metrics_train\n",
                "            default_results_reg[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = metrics_test\n",
                "            # Store trained model\n",
                "            trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
                "            # Set GRID parameters\n",
                "            grid_params = regression_grids[model_name]\n",
                "            if len(grid_params) > 0:\n",
                "                # Instance GRID\n",
                "                grid = GridSearchCV(\n",
                "                    estimator = regression_available[model_name],\n",
                "                    param_grid = grid_params,\n",
                "                    scoring = regression_scoring_target,\n",
                "                    cv = grid_cross_validation)\n",
                "                # Train GRID\n",
                "                grid.fit(X_train_model, y_train_model)\n",
                "                # Get best estimator configuration (OPTIMIZED model)\n",
                "                best_model = grid.best_estimator_\n",
                "                # Predict with trained OPTIMIZED model\n",
                "                y_train_opt = best_model.predict(X_train_model)\n",
                "                y_test_opt  = best_model.predict(X_test_model)\n",
                "                # Calculate metricts for OPTIMIZED model\n",
                "                metrics_train_opt = compute_regression_metrics(y_train_model, y_train_opt)\n",
                "                metrics_test_opt  = compute_regression_metrics(y_test_model,  y_test_opt)\n",
                "                # Build final table with results for OPTIMIZED model\n",
                "                optimized_results_reg[\"OPTIMIZED \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_, \"Scoring\": regression_scoring_target}\n",
                "                optimized_results_reg[\"OPTIMIZED \" + model_name + \" - üß™ TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_, \"Scoring\": regression_scoring_target}\n",
                "                # Store trained model\n",
                "                trained_models[model_name][\"optimized\"] = best_model\n",
                "                # Store hyperparameters for Lasso/Ridge\n",
                "                if model_name == \"Lasso\" or model_name == \"Ridge\":\n",
                "                    # Extract grid search raw values\n",
                "                    alpha_values = grid.cv_results_[\"param_alpha\"].data.astype(float)\n",
                "                    scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                    # Compute the unique alpha values used in the grid (removing repetitions)\n",
                "                    alpha_unique = np.sort(np.unique(alpha_values))\n",
                "                    # Loop through each unique alpha value\n",
                "                    mean_scores = []\n",
                "                    for a in alpha_unique:\n",
                "                        # Create a boolean mask\n",
                "                        mask = (alpha_values == a)\n",
                "                        # Compute the mean score for this alpha\n",
                "                        mean_score = scores[mask].mean()\n",
                "                        # Store the resulting mean score\n",
                "                        mean_scores.append(mean_score)\n",
                "                    # Store internally\n",
                "                    trained_models[model_name][\"alpha_list\"] = alpha_unique\n",
                "                    trained_models[model_name][\"alpha_scores\"] = mean_scores\n",
                "                # Store hyperparameters for RandomForest\n",
                "                if model_name == \"RandomForestRegressor\":\n",
                "                    # Extract grid search raw values\n",
                "                    est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                    depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                    scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                    # Unique values of n_estimators\n",
                "                    est_unique = np.sort(np.unique(est_values))\n",
                "                    # Clean depths (convert to Python None or int)\n",
                "                    cleaned_depths = []\n",
                "                    for d in depth_values:\n",
                "                        if d is None:\n",
                "                            cleaned_depths.append(None)\n",
                "                        else:\n",
                "                            cleaned_depths.append(int(d))\n",
                "                    # Unique valid depths (excluding None)\n",
                "                    depth_unique = []\n",
                "                    for d in cleaned_depths:\n",
                "                        if d is not None and d not in depth_unique:\n",
                "                            depth_unique.append(d)\n",
                "                    depth_unique = sorted(depth_unique)\n",
                "                    # Dictionary: key=max_depth ‚Üí list of mean scores over n_estimators\n",
                "                    depth_score_dict = {}\n",
                "                    for d in depth_unique:\n",
                "                        mean_scores = []\n",
                "                        for e in est_unique:\n",
                "                            # Mask selecting rows matching both parameters\n",
                "                            mask = (est_values == e) & (np.array(cleaned_depths, dtype=object) == d)\n",
                "                            # Mean score for this (n_estimators, max_depth)\n",
                "                            mean_scores.append(scores[mask].mean())\n",
                "                        depth_score_dict[d] = mean_scores\n",
                "                    # Store internally\n",
                "                    trained_models[model_name][\"est_list\"]   = est_unique\n",
                "                    trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                    trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                # Store hyperparameters for Boosting models\n",
                "                if model_name in [\"GradientBoostingRegressor\", \"XGBRegressor\", \"LGBMRegressor\"]:\n",
                "                    # Retrieve raw values\n",
                "                    est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                    depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                    scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                    # Unique n_estimators\n",
                "                    est_unique = np.sort(np.unique(est_values))\n",
                "                    # Clean depths (convert to int)\n",
                "                    cleaned_depths = []\n",
                "                    for d in depth_values:\n",
                "                        if d is None:\n",
                "                            cleaned_depths.append(None)\n",
                "                        else:\n",
                "                            cleaned_depths.append(int(d))\n",
                "                    # Depth unique (exclude None)\n",
                "                    depth_unique = []\n",
                "                    for d in cleaned_depths:\n",
                "                        if d is not None and d not in depth_unique:\n",
                "                            depth_unique.append(d)\n",
                "                    depth_unique = sorted(depth_unique)\n",
                "                    # Dictionary: depth ‚Üí scores list\n",
                "                    depth_score_dict = {}\n",
                "                    for depth in depth_unique:\n",
                "                        mean_scores = []\n",
                "                        for est in est_unique:\n",
                "                            mask = (est_values == est) & (np.array(cleaned_depths, dtype=object) == depth)\n",
                "                            mean_scores.append(scores[mask].mean())\n",
                "                        depth_score_dict[depth] = mean_scores\n",
                "                    # Store internally\n",
                "                    trained_models[model_name][\"est_list\"] = est_unique\n",
                "                    trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                    trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                # Store hyperparameter K for KNN regressor\n",
                "                if model_name == \"KNeighborsRegressor\":\n",
                "                    # Retrieve k_list from grid OR fallback\n",
                "                    if \"param_n_neighbors\" in grid.cv_results_:\n",
                "                        k_list = np.sort(np.unique(grid.cv_results_[\"param_n_neighbors\"].data.astype(int)))\n",
                "                    else:\n",
                "                        k_list = regression_grids[\"KNeighborsRegressor\"][\"n_neighbors\"]\n",
                "                    train_scores = []\n",
                "                    test_scores  = []\n",
                "                    for k in k_list:\n",
                "                        tmp_model = KNeighborsRegressor(n_neighbors=int(k), metric=grid.best_params_.get(\"metric\", \"euclidean\"), weights=grid.best_params_.get(\"weights\", \"uniform\"))\n",
                "                        tmp_model.fit(X_train_model, y_train_model)\n",
                "                        # Compute train & test scores\n",
                "                        train_scores.append(tmp_model.score(X_train_model, y_train_model))\n",
                "                        test_scores.append(tmp_model.score(X_test_model, y_test_model))\n",
                "                    # Store internally for plotting phase\n",
                "                    trained_models[model_name][\"k_list_train_test\"] = (k_list, train_scores, test_scores)\n",
                "            else:\n",
                "                optimized_results_reg[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "                optimized_results_reg[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "        # -------------------------------\n",
                "        #  FINAL TABLES\n",
                "        # -------------------------------\n",
                "        if len(classification_selected_list)>0:\n",
                "            print(\"\\n==================== ‚öñÔ∏è CLASSIFICATION MODELS COMPARISON ====================\")\n",
                "            display(pd.DataFrame(default_results_class).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "            display(pd.DataFrame(optimized_results_class).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "        if len(regression_selected_list)>0:\n",
                "            print(\"\\n==================== üìà REGRESSION MODELS COMPARISON ========================\")\n",
                "            display(pd.DataFrame(default_results_reg).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "            display(pd.DataFrame(optimized_results_reg).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "        # -------------------------------\n",
                "        # PLOTTING\n",
                "        # -------------------------------\n",
                "        for model_name, model_dict in trained_models.items():\n",
                "            # LINEAR MODELS ‚Üí Coefficient Plotter - Only plot for TABULAR datasets\n",
                "            if model_name in [\"LogisticRegression\", \"LinearRegression\", \"LinearSVC\"] and dataset_type == \"TABULAR\":\n",
                "                # Some linear models (like LinearSVC) may not have coef_ if kernel != linear\n",
                "                default_model = model_dict[\"default\"]\n",
                "                if not hasattr(default_model, \"coef_\"):\n",
                "                    continue\n",
                "                print(f\"\\n==================== üìä MODEL PLOT: {model_name} (Coefficients) ====================\")\n",
                "                # Set subplot\n",
                "                num_rows = 1\n",
                "                num_cols = 1\n",
                "                fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                # Plot coefficients\n",
                "                plotter_linear_coefficients(model=default_model, X=X_train_model,ax=ax, model_name=model_name, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "                continue\n",
                "            print(f\"\\n==================== üìä MODEL PLOT: {model_name} ====================\")\n",
                "            # If Lasso or Ridge ‚Üí use Lasso / Ridge hyperparameter plotter\n",
                "            if model_name in [\"Lasso\", \"Ridge\"]:\n",
                "                # Set values \n",
                "                alphas  = model_dict.get(\"alpha_list\", None)\n",
                "                scores  = model_dict.get(\"alpha_scores\", None)\n",
                "                if alphas is not None and scores is not None:\n",
                "                    # Set subplot\n",
                "                    num_rows= 1\n",
                "                    num_cols= 1\n",
                "                    fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                    # Plot\n",
                "                    plotter_Lasso_Ridge_hparame(model_name=model_name, grid_params=alphas, grid_scores=scores, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target)\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "                continue\n",
                "            # If DecisionTree ‚Üí use Decision Tree plotter\n",
                "            if model_name in [\"DecisionTreeClassifier\", \"DecisionTreeRegressor\"]:\n",
                "                # Set subplot\n",
                "                num_rows= 1\n",
                "                num_cols= 2\n",
                "                fig, axes = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                # Plot DEFAULT\n",
                "                plotter_DecisionTree_model(model=model_dict[\"default\"], X=X_train_model, ax=axes[0])\n",
                "                axes[0].set_title(f\"{model_name} - DEFAULT\", fontsize=plot_title_font_size)\n",
                "                # Plot OPTIMIZED\n",
                "                plotter_DecisionTree_model(model=model_dict[\"optimized\"], X=X_train_model, ax=axes[1])\n",
                "                axes[1].set_title(f\"{model_name} - OPTIMIZED\", fontsize=plot_title_font_size)\n",
                "                plt.tight_layout()\n",
                "                plt.show()\n",
                "                continue\n",
                "            if model_name in [\"KNeighborsClassifier\", \"KNeighborsRegressor\"]:\n",
                "                k_tuple = model_dict.get(\"k_list_train_test\", None)\n",
                "                if k_tuple is not None:\n",
                "                    k_list, train_scores, test_scores = k_tuple\n",
                "                    # Set Subplot\n",
                "                    num_rows = 1\n",
                "                    num_cols = 1\n",
                "                    fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                    # Plot\n",
                "                    plotter_KNN_hparam_k(model_name=model_name, k_list=k_list, train_scores=train_scores, test_scores=test_scores, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "                continue\n",
                "            # If RandomForest ‚Üí use unified RandomForest hyperparameter plotter\n",
                "            if model_name in [\"RandomForestClassifier\", \"RandomForestRegressor\"]:\n",
                "                # Set values \n",
                "                est_list   = model_dict.get(\"est_list\", None)\n",
                "                depth_list = model_dict.get(\"depth_list\", None)\n",
                "                depth_score_dict = model_dict.get(\"depth_score_dict\", None)\n",
                "                if est_list is not None and depth_list is not None and depth_score_dict is not None:\n",
                "                    # Set subplot\n",
                "                    num_rows= 1\n",
                "                    num_cols= 1\n",
                "                    fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                    # Plot\n",
                "                    plotter_RandomForest_hparam(model_name=model_name, est_list=est_list, depth_list=depth_list, depth_score_dict=depth_score_dict, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "                continue\n",
                "            # If Boosting models ‚Üí use unified Boosting hyperparameter plotter\n",
                "            if model_name in [\"GradientBoostingClassifier\", \"GradientBoostingRegressor\", \"XGBClassifier\", \"XGBRegressor\", \"LGBMClassifier\", \"LGBMRegressor\"]:\n",
                "                # Set values\n",
                "                est_list   = model_dict.get(\"est_list\", None)\n",
                "                depth_list = model_dict.get(\"depth_list\", None)\n",
                "                depth_score_dict = model_dict.get(\"depth_score_dict\", None)\n",
                "                if est_list is not None and depth_list is not None and depth_score_dict is not None:\n",
                "                    # Set subplot\n",
                "                    num_rows= 1\n",
                "                    num_cols= 1\n",
                "                    fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                    # Plot\n",
                "                    plotter_Boosting_hparam(model_name=model_name, est_list=est_list, depth_list=depth_list, depth_score_dict=depth_score_dict, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                    plt.tight_layout()\n",
                "                    plt.show()\n",
                "                continue\n",
                "    else:\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "        #  UNSUPERVISED PIPELINE\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "\n",
                "        # -------------------------------\n",
                "        # INITIALIZE UNSUPERVISED WORKING DATA\n",
                "        # -------------------------------\n",
                "        X_train_unsup = X_train_model.copy()\n",
                "        X_test_unsup  = X_test_model.copy()\n",
                "        # -------------------------------\n",
                "        # DIMENSIONALITY REDUCTION\n",
                "        # -------------------------------\n",
                "        if reduce_dimensionality:\n",
                "            if (not use_PCA) and (not use_TruncatedSVD):\n",
                "                log(\"Dimensionality reduction enabled, but no method selected (PCA / TruncatedSVD).\", type=\"WARNING\", bold=True)\n",
                "                log(\"Proceeding WITHOUT dimensionality reduction.\", level=2, type=\"INFO\")\n",
                "                reduce_dimensionality = False\n",
                "            if dataset_type == \"TABULAR\" and not data_has_been_scaled:\n",
                "                scaler = StandardScaler(with_mean=True, with_std=True)\n",
                "                X_train_unsup = scaler.fit_transform(X_train_unsup)\n",
                "                X_test_unsup  = scaler.transform(X_test_unsup)\n",
                "                log(\"Data has been scaled using StandardScaler()\", type=\"SUCCESS\", bold=True)\n",
                "            if reduce_dimensionality:\n",
                "                n_components = min(100, max(2, int(0.25 * n_features)))\n",
                "                if use_TruncatedSVD:\n",
                "                    reducer = TruncatedSVD(n_components=n_components, random_state=random_seed)\n",
                "                    reducer_name = \"TruncatedSVD\"\n",
                "                else:\n",
                "                    reducer = PCA(n_components=n_components, random_state=random_seed)\n",
                "                    reducer_name = \"PCA\"\n",
                "                X_train_unsup = reducer.fit_transform(X_train_unsup)\n",
                "                X_test_unsup  = reducer.transform(X_test_unsup)\n",
                "                log(f\"Dimensionality reduction applied: {reducer_name} ‚Üí from {n_features} features to {X_train_unsup.shape[1]} components\", type=\"SUCCESS\", bold=True)\n",
                "        else:\n",
                "            if dataset_type == \"TABULAR\" and not data_has_been_scaled:\n",
                "                scaler = StandardScaler(with_mean=True, with_std=True)\n",
                "                X_train_unsup = scaler.fit_transform(X_train_unsup)\n",
                "                X_test_unsup  = scaler.transform(X_test_unsup)\n",
                "                log(\"Data has been scaled using StandardScaler()\", type=\"SUCCESS\", bold=True)\n",
                "        # -------------------------------\n",
                "        #  GRIDS FOR OPIMIZATION\n",
                "        # -------------------------------\n",
                "        # Store number of parameters for the grid\n",
                "        num_grid_param =  n_grid_param(\n",
                "                                    n_samples=X_train_unsup.shape[0],\n",
                "                                    n_features=X_train_unsup.shape[1],\n",
                "                                    n_features_BIG_dataset=n_features_BIG_dataset,\n",
                "                                    n_features_MEDIUM_dataset=n_features_MEDIUM_dataset,\n",
                "                                    n_features_SMALL_dataset=n_features_SMALL_dataset,\n",
                "                                    n_samples_BIG_dataset=n_samples_BIG_dataset,\n",
                "                                    n_samples_MEDIUM_dataset=n_samples_MEDIUM_dataset,\n",
                "                                    n_samples_SMALL_dataset=n_samples_SMALL_dataset,\n",
                "                                    n_samples_MICRO_dataset=n_samples_MICRO_dataset)\n",
                "        clustering_grids = {\n",
                "            \"KMeans\": {\n",
                "                \"n_clusters\": smart_logspace_choose(2,  min(15, max(3, int(np.sqrt(n_samples)))), \"integer\", num_grid_param),\n",
                "                \"n_init\": smart_logspace_choose(5, 20, \"integer\", num_grid_param)\n",
                "            },\n",
                "            \"AgglomerativeClustering\": {\n",
                "                \"n_clusters\": smart_logspace_choose(2,  min(15, max(3, int(np.sqrt(n_samples)))), \"integer\", num_grid_param),\n",
                "                \"linkage\": [\"ward\", \"complete\", \"average\"]\n",
                "            },\n",
                "            \"DBSCAN\": {\n",
                "                \"eps\": smart_logspace_choose(0.1, 1.0, \"float\", num_grid_param),\n",
                "                \"min_samples\": smart_logspace_choose(5, 15, \"integer\", num_grid_param)\n",
                "            }            \n",
                "        }\n",
                "        # -------------------------------\n",
                "        #  SELECTED  MODELS\n",
                "        # -------------------------------\n",
                "        clustering_selected_list = []\n",
                "        for model_name, active in cluster_models_selection.items():\n",
                "            if active:\n",
                "                clustering_selected_list.append(model_name)\n",
                "        log(f\"Selected CLUSTERING models: {clustering_selected_list}\", type=\"INFO\")\n",
                "        # -------------------------------\n",
                "        #  TRAIN UNSUPERVISED MODELS (DEFAULT + OPTIMIZED)\n",
                "        # -------------------------------\n",
                "        trained_unsup_models = {}\n",
                "        default_results_unsup = {}\n",
                "        optimized_results_unsup = {}\n",
                "        # -------------------------------\n",
                "        #  ELBOW ANALYSIS (multi-curve + silhouette)\n",
                "        # -------------------------------\n",
                "        for model_name in clustering_selected_list:\n",
                "            elbow_results = compute_elbow_diagnostics_curves(model_name=model_name, X_train_unsup=X_train_unsup, clustering_grids=clustering_grids, dataset_type=dataset_type, random_seed=random_seed, n_points_elbow=10)\n",
                "            # Dual subplot: inertia + silhouette\n",
                "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(figWidth_unit * 2, figHeight_unit))\n",
                "            plotter_elbow_diagnostics_dual(elbow_results=elbow_results, axes=axes, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, text_fontsize=plot_text_font_size, show_elbows=True)\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "            # Select DEFAULT elbow (best silhouette)\n",
                "            elbow_x = select_best_elbow_by_silhouette(elbow_results)\n",
                "            # -------------------------------\n",
                "            # GRID REFINEMENT\n",
                "            # -------------------------------\n",
                "            clustering_grids, refined = refine_clustering_grid_around_elbow(model_name=model_name, clustering_grids=clustering_grids, elbow_x=elbow_x, dataset_type=dataset_type, num_grid_param=num_grid_param)\n",
                "            # -------------------------------\n",
                "            # DEFAULT MODEL SELECTION\n",
                "            # -------------------------------\n",
                "            if model_name == \"KMeans\":\n",
                "                k_default = int(elbow_x) if elbow_x is not None else 3\n",
                "                default_model = KMeans(n_clusters=k_default, n_init=5, random_state=random_seed)\n",
                "                default_model.fit(X_train_unsup)\n",
                "                labels_train = default_model.labels_\n",
                "                labels_test  = default_model.predict(X_test_unsup)\n",
                "            elif model_name == \"AgglomerativeClustering\":\n",
                "                k_default = int(elbow_x) if elbow_x is not None else 3\n",
                "                metric_to_use_agglo = resolve_distance_metric(dataset_type, model_name=\"AgglomerativeClustering\", linkage=\"ward\")\n",
                "                default_model = AgglomerativeClustering(n_clusters=k_default, metric=metric_to_use_agglo, linkage=\"ward\")\n",
                "                labels_train = default_model.fit_predict(X_train_unsup)\n",
                "                labels_test  = clustering_predict(model=default_model, X_train=X_train_unsup, X_new=X_test_unsup, dataset_type=dataset_type) # AgglomerativeClustering has no native predict ‚Üí use helper\n",
                "            elif model_name == \"DBSCAN\":\n",
                "                eps_default = float(elbow_x) if elbow_x is not None else 0.1\n",
                "                default_model = DBSCAN(eps=eps_default, min_samples=5, metric=resolve_distance_metric(dataset_type))\n",
                "                default_model.fit(X_train_unsup)\n",
                "                labels_train = default_model.labels_\n",
                "                labels_test  = clustering_predict(model=default_model, X_train=X_train_unsup, X_new=X_test_unsup, dataset_type=dataset_type) # DBSCAN has no native predict ‚Üí use helper\n",
                "            # -------------------------------\n",
                "            # DEFAULT MODEL METRICS\n",
                "            # -------------------------------\n",
                "            default_diag = compute_clustering_diagnostics(X=X_train_unsup, labels=labels_train)\n",
                "            default_results_unsup[\"DEFAULT \" + model_name] = {\n",
                "                \"n_clusters_found\": default_diag[\"n_clusters_found\"],\n",
                "                \"inertia_like\": default_diag[\"inertia_like\"],\n",
                "                \"silhouette\": default_diag[\"silhouette\"],\n",
                "                \"noise_ratio\": default_diag[\"noise_ratio\"]\n",
                "            }\n",
                "            trained_unsup_models[model_name] = {\n",
                "                \"default\": default_model,\n",
                "                \"optimized\": None,\n",
                "                \"labels_train_default\": labels_train,\n",
                "                \"labels_test_default\": labels_test\n",
                "            }\n",
                "            # -------------------------------\n",
                "            # OPTIMIZATION (grid search style)\n",
                "            # -------------------------------\n",
                "            grid_params = clustering_grids[model_name]\n",
                "            best_params = None\n",
                "            best_score  = None\n",
                "            best_model  = None\n",
                "            best_labels_train = None\n",
                "            best_labels_test  = None\n",
                "            best_inertia = None\n",
                "            best_n_clusters_found = None            \n",
                "            if len(grid_params) > 0: # If empty grid, skip\n",
                "                # Build explicit combinations\n",
                "                param_combinations = []\n",
                "                if model_name == \"KMeans\":\n",
                "                    for k in grid_params[\"n_clusters\"]:\n",
                "                        for n_init in grid_params[\"n_init\"]:\n",
                "                            param_combinations.append({\"n_clusters\": k, \"n_init\": n_init})\n",
                "                if model_name == \"DBSCAN\":\n",
                "                    for eps in grid_params[\"eps\"]:\n",
                "                        for ms in grid_params[\"min_samples\"]:\n",
                "                            param_combinations.append({\"eps\": eps, \"min_samples\": ms, \"metric\": resolve_distance_metric(dataset_type=dataset_type)})\n",
                "                if model_name == \"AgglomerativeClustering\":\n",
                "                    for k in grid_params[\"n_clusters\"]:\n",
                "                        for link in grid_params[\"linkage\"]:\n",
                "                            param_combinations.append({\"n_clusters\": k, \"metric\": resolve_distance_metric(dataset_type=dataset_type, model_name=\"AgglomerativeClustering\", linkage=link),\"linkage\": link})\n",
                "                for params in param_combinations:\n",
                "                    if model_name == \"KMeans\":\n",
                "                        tmp_model = KMeans(n_clusters=int(params[\"n_clusters\"]), n_init=int(params[\"n_init\"]), random_state=random_seed)\n",
                "                        tmp_model.fit(X_train_unsup)\n",
                "                        tmp_labels_train = tmp_model.labels_\n",
                "                        tmp_labels_test  = tmp_model.predict(X_test_unsup)\n",
                "                    elif model_name == \"AgglomerativeClustering\":\n",
                "                        tmp_model = AgglomerativeClustering(n_clusters=int(params[\"n_clusters\"]), metric=params[\"metric\"], linkage=params[\"linkage\"])\n",
                "                        tmp_labels_train = tmp_model.fit_predict(X_train_unsup)\n",
                "                        tmp_labels_test = clustering_predict(model=tmp_model, X_train=X_train_unsup, X_new=X_test_unsup, dataset_type=dataset_type)\n",
                "                    elif model_name == \"DBSCAN\":\n",
                "                        tmp_model = DBSCAN(eps=float(params[\"eps\"]), min_samples=int(params[\"min_samples\"]), metric=params[\"metric\"])\n",
                "                        tmp_model.fit(X_train_unsup)\n",
                "                        tmp_labels_train = tmp_model.labels_\n",
                "                        tmp_labels_test = clustering_predict(model=tmp_model, X_train=X_train_unsup, X_new=X_test_unsup, dataset_type=dataset_type)\n",
                "                    # -------------------------------\n",
                "                    # OPTIMIZED MODEL METRICS\n",
                "                    # -------------------------------\n",
                "                    # Get metrics\n",
                "                    tmp_diag = compute_clustering_diagnostics(X=X_train_unsup, labels=tmp_labels_train)\n",
                "                    # Inertia\n",
                "                    tmp_inertia = tmp_diag[\"inertia_like\"]\n",
                "                    # Number of clusters\n",
                "                    tmp_clusters_found = tmp_diag[\"n_clusters_found\"]\n",
                "                    # Silhouette scoring\n",
                "                    if tmp_diag[\"silhouette\"] is None:\n",
                "                        tmp_score = -999999.0\n",
                "                    else:\n",
                "                        tmp_score = float(tmp_diag[\"silhouette\"])\n",
                "                    # Best model selection\n",
                "                    if (best_score is None) or (tmp_score > best_score):\n",
                "                        best_score = tmp_score\n",
                "                        best_params = params\n",
                "                        best_model = tmp_model\n",
                "                        best_labels_train = tmp_labels_train\n",
                "                        best_labels_test = tmp_labels_test\n",
                "                        best_inertia = tmp_inertia\n",
                "                        best_n_clusters_found = tmp_clusters_found\n",
                "                # Stored trained optimized models\n",
                "                trained_unsup_models[model_name][\"optimized\"] = best_model\n",
                "                trained_unsup_models[model_name][\"labels_train_optimized\"] = best_labels_train\n",
                "                trained_unsup_models[model_name][\"labels_test_optimized\"]  = best_labels_test\n",
                "                # Build final tables\n",
                "                optimized_results_unsup[\"OPTIMIZED \" + model_name] = {\n",
                "                    \"n_clusters_found\": best_n_clusters_found,\n",
                "                    \"inertia_like\": best_inertia,\n",
                "                    \"silhouette\": best_score,\n",
                "                    \"noise_ratio\": compute_clustering_diagnostics(X=X_train_unsup, labels=best_labels_train)[\"noise_ratio\"],\n",
                "                    \"Best Parameters\": best_params,\n",
                "                    \"Scoring\": \"silhouette\"}\n",
                "        # -------------------------------\n",
                "        #  FINAL TABLES (UNSUPERVISED)\n",
                "        # -------------------------------\n",
                "        print(\"\\n==================== üß≠ UNSUPERVISED MODELS COMPARISON ======================\")\n",
                "        display(pd.DataFrame(default_results_unsup).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "        display(pd.DataFrame(optimized_results_unsup).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "        # -------------------------------\n",
                "        #  PLOT CLUSTERS (scatter colored by optimized clusters)\n",
                "        # -------------------------------\n",
                "        print(\"\\n==================== CLUSTER GEOMETRICAL VISUALIZATION ====================\")\n",
                "        if X_train_unsup.shape[1] > 2: # Ensure I have a 2D embedding\n",
                "            # If data was reduced before (PCA / SVD), first components are meaningful\n",
                "            if reduce_dimensionality:\n",
                "                X_train_2d = X_train_unsup[:, 0:2]\n",
                "                log(\"Using first 2 components from reduced feature space for visualization\", type=\"INFO\")\n",
                "            # Otherwise, build PCA(2) only for plotting\n",
                "            else:\n",
                "                pca_plot = PCA(n_components=2, random_state=random_seed)\n",
                "                X_train_2d = pca_plot.fit_transform(X_train_unsup)\n",
                "                log(\"PCA(2) applied only for cluster visualization\", type=\"INFO\")\n",
                "        else:\n",
                "            # Else: build a PCA(2) embedding ONLY for visualization purposes\n",
                "            X_train_2d = X_train_unsup\n",
                "        # Plot one subplot per model\n",
                "        num_rows = 1\n",
                "        num_cols = max(1, len(clustering_selected_list))\n",
                "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "        if len(clustering_selected_list) == 1:\n",
                "            axes = [axes]\n",
                "        for i, model_name in enumerate(clustering_selected_list):\n",
                "            model_dict = trained_unsup_models[model_name]\n",
                "            if \"labels_train_optimized\" in model_dict:\n",
                "                labels_to_plot = model_dict[\"labels_train_optimized\"]\n",
                "                title = model_name + \" - OPTIMIZED\"\n",
                "            else:\n",
                "                labels_to_plot = model_dict[\"labels_train_default\"]\n",
                "                title = model_name + \" - DEFAULT\"\n",
                "            plotter_clusters_scatter(X_2d=X_train_2d, labels=labels_to_plot, ax=axes[i], title=title,label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        # -------------------------------\n",
                "        #  CLUSTER INTERPRETATION (original feature space)\n",
                "        # -------------------------------\n",
                "        for model_name in clustering_selected_list:\n",
                "            model_dict = trained_unsup_models[model_name]\n",
                "            if \"labels_train_optimized\" in model_dict:\n",
                "                labels_to_plot = model_dict[\"labels_train_optimized\"]\n",
                "                title_suffix = \"OPTIMIZED\"\n",
                "            else:\n",
                "                labels_to_plot = model_dict[\"labels_train_default\"]\n",
                "                title_suffix = \"DEFAULT\"\n",
                "            print(f\"\\n==================== üìä CLUSTER INTERPRETATION: {model_name} ({title_suffix} ====================\")\n",
                "            plotter_clusters_pairwise_triangular(X_original=X_train_model, labels=labels_to_plot, figWidth_unit=figWidth_unit, figHeight_unit=figWidth_unit, label_fontsize=plot_label_font_size)\n",
                "\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "        #  PSEUDO-TAGGING + SUPERVISED CLASSIFIER PIPELINE\n",
                "        # ===============================\n",
                "        # ===============================\n",
                "        if pseudo_tagging:\n",
                "            print(\"-------------------------------\")\n",
                "            print(\"STEP 17 - PART 2) SUPERVISED CLASSIFICATION\")\n",
                "            print(\"-------------------------------\\n\")\n",
                "            # Choose which clustering labels to use as pseudo-target\n",
                "            # Here: prefer optimized KMeans if available; else first available optimized; else first default\n",
                "            chosen_model_name, chosen_variant = select_best_pseudo_clustering(optimized_results=optimized_results_unsup, default_results=default_results_unsup, alpha_noise=0.5)\n",
                "            if chosen_model_name is None:\n",
                "                log(\"No valid clustering found for pseudo-labeling (silhouette invalid or degenerate clusters) -> Skipping supervised stage\", type=\"WARNING\", bold=True)\n",
                "            else:\n",
                "                log(f\"Pseudo-labeling with: {chosen_model_name} ({chosen_variant})\", type=\"INFO\", bold=True)\n",
                "                base_name = chosen_model_name.replace(\"OPTIMIZED \", \"\").replace(\"DEFAULT \", \"\")\n",
                "                chosen_dict = trained_unsup_models[base_name]\n",
                "                if chosen_variant == \"optimized\" and \"labels_train_optimized\" in chosen_dict:\n",
                "                    y_train_model = pd.Series(chosen_dict[\"labels_train_optimized\"])\n",
                "                    y_test_model  = pd.Series(chosen_dict[\"labels_test_optimized\"])\n",
                "                else:\n",
                "                    y_train_model = pd.Series(chosen_dict[\"labels_train_default\"])\n",
                "                    y_test_model  = pd.Series(chosen_dict[\"labels_test_default\"])\n",
                "                # Clean noise labels if DBSCAN used: remove -1 rows for supervised training\n",
                "                if (y_train_model == -1).any():\n",
                "                    mask_keep = (y_train_model != -1)\n",
                "                    X_train_model = pd.DataFrame(X_train_model).loc[mask_keep].reset_index(drop=True)\n",
                "                    y_train_model = y_train_model.loc[mask_keep].reset_index(drop=True)\n",
                "                    log(\"DBSCAN noise (-1) removed from TRAIN for supervised stage\", type=\"WARNING\")\n",
                "                \n",
                "                # -------------------------------\n",
                "                #  GRIDS FOR OPIMIZATION\n",
                "                # -------------------------------\n",
                "                # Store number of parameters for the grid\n",
                "                num_grid_param =  n_grid_param(\n",
                "                                            n_samples=X_train_model.shape[0],\n",
                "                                            n_features=X_train_model.shape[1],\n",
                "                                            n_features_BIG_dataset=n_features_BIG_dataset,\n",
                "                                            n_features_MEDIUM_dataset=n_features_MEDIUM_dataset,\n",
                "                                            n_features_SMALL_dataset=n_features_SMALL_dataset,\n",
                "                                            n_samples_BIG_dataset=n_samples_BIG_dataset,\n",
                "                                            n_samples_MEDIUM_dataset=n_samples_MEDIUM_dataset,\n",
                "                                            n_samples_SMALL_dataset=n_samples_SMALL_dataset,\n",
                "                                            n_samples_MICRO_dataset=n_samples_MICRO_dataset)\n",
                "                classification_grids = {\n",
                "                    \"LogisticRegression\": {\n",
                "                        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
                "                        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
                "                        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
                "                    },\n",
                "                    \"DecisionTreeClassifier\": {\n",
                "                        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
                "                        \"max_depth\": [None, 5, 10, 20, 30],\n",
                "                        \"min_samples_split\": [2, 5, 10, 20, 30]\n",
                "                    },\n",
                "                    \"RandomForestClassifier\": {\n",
                "                        \"n_estimators\": [5, 10, 20, 50, 100, 200],\n",
                "                        \"max_depth\": [None, 2, 5, 10, 15, 20]\n",
                "                    },\n",
                "                    \"AdaBoostClassifier\": {\n",
                "                        \"n_estimators\" : smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                        \"learning_rate\" : smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param)\n",
                "                    },\n",
                "                    \"GradientBoostingClassifier\": {\n",
                "                        \"n_estimators\": smart_logspace_choose(2, 200, \"integer\",num_grid_param),\n",
                "                        \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\",num_grid_param),\n",
                "                        \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                        \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "                    },\n",
                "                    \"XGBClassifier\": {\n",
                "                        \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                        \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                        \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                        \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "                    },\n",
                "                    \"LGBMClassifier\": {\n",
                "                        \"n_estimators\": smart_logspace_choose(2, 200, \"integer\", num_grid_param),\n",
                "                        \"learning_rate\": smart_logspace_choose(0.01, 1.0, \"float\", num_grid_param),\n",
                "                        \"max_depth\": smart_logspace_choose(5, 30, \"integer\", num_grid_param),\n",
                "                        \"subsample\": smart_logspace_choose(0.5, 1.0, \"float\", num_grid_param)\n",
                "                    },\n",
                "                    \"LinearSVC\": {\n",
                "                        \"C\": smart_logspace_choose(0.1, 10, \"float\", num_grid_param)\n",
                "                    },\n",
                "                    \"MultinomialNB\": {},\n",
                "                    \"GaussianNB\": {},\n",
                "                    \"BernoulliNB\": {},\n",
                "                    \"KNeighborsClassifier\": {\n",
                "                        \"n_neighbors\": smart_logspace_choose(1, max(10, int(min(50, n_samples ** 0.5))), \"integer\", num_grid_param),\n",
                "                        \"metric\": [resolve_distance_metric(dataset_type)],\n",
                "                        \"weights\": [\"uniform\", \"distance\"]\n",
                "                    }\n",
                "                }\n",
                "                # -------------------------------\n",
                "                #  DEFAULT MODELS\n",
                "                # -------------------------------\n",
                "                classification_available = {\n",
                "                    \"LogisticRegression\": LogisticRegression(random_state=random_seed),\n",
                "                    \"RandomForestClassifier\": RandomForestClassifier(random_state=random_seed),\n",
                "                    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=random_seed),\n",
                "                    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=random_seed),\n",
                "                    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=random_seed),\n",
                "                    \"XGBClassifier\": XGBClassifier(random_state=random_seed, eval_metric=map_xgb_eval_metric(\"XGBClassifier\", classification_scoring_target), use_label_encoder=False),\n",
                "                    \"LGBMClassifier\": LGBMClassifier(random_state=random_seed),\n",
                "                    \"LinearSVC\": SVC(random_state=random_seed),\n",
                "                    \"MultinomialNB\": MultinomialNB(),\n",
                "                    \"GaussianNB\": GaussianNB(),\n",
                "                    \"BernoulliNB\": BernoulliNB(),\n",
                "                    \"KNeighborsClassifier\": KNeighborsClassifier()\n",
                "                }\n",
                "                # -------------------------------\n",
                "                #  SELECTED MODELS\n",
                "                # -------------------------------\n",
                "                classification_selected_list = []\n",
                "                for model_name, active in classification_models_selection.items():\n",
                "                    if active:\n",
                "                        classification_selected_list.append(model_name)\n",
                "                log(f\"Selected CLASSIFICATION models: {classification_selected_list}\", type=\"INFO\")\n",
                "                # Parameters needed for classification metrics\n",
                "                if len(classification_selected_list) > 0:\n",
                "                    proposed_avg, proposed_pos, proposed_score = set_average_proposal(y_train_model, classification_scoring_target)\n",
                "                # -------------------------------\n",
                "                #  ESTIMATED COMPUTING TIME\n",
                "                # -------------------------------\n",
                "                # Estimated computing time per fit in seconds\n",
                "                time_per_fit_factor =  set_time_per_fit_factor(\n",
                "                                        n_samples=X_train_model.shape[0],\n",
                "                                        n_features=X_train_model.shape[1],\n",
                "                                        n_features_BIG_dataset=n_features_BIG_dataset,\n",
                "                                        n_features_MEDIUM_dataset=n_features_MEDIUM_dataset,\n",
                "                                        n_features_SMALL_dataset=n_features_SMALL_dataset,\n",
                "                                        n_samples_BIG_dataset=n_samples_BIG_dataset,\n",
                "                                        n_samples_MEDIUM_dataset=n_samples_MEDIUM_dataset,\n",
                "                                        n_samples_SMALL_dataset=n_samples_SMALL_dataset)\n",
                "                computing_time_per_fit = {\n",
                "                    \"LogisticRegression\": 0.05 * time_per_fit_factor,\n",
                "                    \"DecisionTreeClassifier\": 0.05 * time_per_fit_factor,\n",
                "                    \"RandomForestClassifier\": 0.12 * time_per_fit_factor,\n",
                "                    \"AdaBoostClassifier\": 0.12 * time_per_fit_factor,\n",
                "                    \"GradientBoostingClassifier\": 0.35 * time_per_fit_factor,\n",
                "                    \"XGBClassifier\": 0.08 * time_per_fit_factor,\n",
                "                    \"LGBMClassifier\": 0.05 * time_per_fit_factor,\n",
                "                    \"LinearSVC\": 0.08 * time_per_fit_factor,\n",
                "                    \"MultinomialNB\": 0.03 * time_per_fit_factor,\n",
                "                    \"GaussianNB\": 0.03 * time_per_fit_factor,\n",
                "                    \"BernoulliNB\": 0.03 * time_per_fit_factor,\n",
                "                    \"KNeighborsClassifier\": 0.05 * time_per_fit_factor\n",
                "                }\n",
                "                # Prints total number of model fits\n",
                "                expected_time = 0\n",
                "                log(\"Number of fits required per model:\", type=\"INFO\")\n",
                "                for model_name in classification_selected_list:\n",
                "                    grid_params = classification_grids[model_name]\n",
                "                    total_fits = count_grid_combinations(grid_params, grid_cross_validation)\n",
                "                    time_per_fit = computing_time_per_fit.get(model_name, 0.10)\n",
                "                    expected_time = expected_time + total_fits * time_per_fit\n",
                "                    log(f\"{model_name} ‚Äî total model fits required: {total_fits}\", level=2, type=\"FOUND\")\n",
                "                log(f\"Expected Time to Complete (ETC): {math.floor(0.9*expected_time/60)}m {round(0.9*expected_time % 60)}s - {math.floor(1.1 * expected_time/60)}m {round(1.1 * expected_time % 60)}s\", type=\"INFO\", bold=True)\n",
                "                print(\"============================================================\\n\")\n",
                "                # ===============================\n",
                "                #  CLASSIFICATION MODELS\n",
                "                # ===============================\n",
                "                trained_models = {} \n",
                "                default_results_class = {}\n",
                "                optimized_results_class = {}\n",
                "                for model_name in classification_selected_list:\n",
                "                    # Instance DEFAULT model\n",
                "                    default_model = classification_available[model_name] \n",
                "                    # Train DEFAULT model\n",
                "                    default_model.fit(X_train_model, y_train_model)\n",
                "                    # Predict with trained DEFAULT model\n",
                "                    y_pred_train = default_model.predict(X_train_model)\n",
                "                    y_pred_test  = default_model.predict(X_test_model)\n",
                "                    # Calculate metricts for DEFAULT model\n",
                "                    metrics_train = compute_classification_metrics(y_train_model, y_pred_train, proposed_avg, proposed_pos)\n",
                "                    metrics_test  = compute_classification_metrics(y_test_model, y_pred_test, proposed_avg, proposed_pos)\n",
                "                    # Build final table with results for DEFAULT model\n",
                "                    default_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = metrics_train\n",
                "                    default_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = metrics_test\n",
                "                    # Store trained model\n",
                "                    trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
                "                    # Set GRID parameters\n",
                "                    grid_params = classification_grids[model_name]\n",
                "                    if len(grid_params) > 0:\n",
                "                        # Instance GRID\n",
                "                        grid = GridSearchCV(\n",
                "                            estimator = classification_available[model_name],\n",
                "                            param_grid = grid_params,\n",
                "                            scoring = proposed_score,\n",
                "                            cv = grid_cross_validation)\n",
                "                        # Train GRID\n",
                "                        grid.fit(X_train_model, y_train_model)\n",
                "                        # Get best estimator configuration (OPTIMIZED model)\n",
                "                        best_model = grid.best_estimator_\n",
                "                        # Predict with trained OPTIMIZED model\n",
                "                        y_train_opt = best_model.predict(X_train_model)\n",
                "                        y_test_opt  = best_model.predict(X_test_model)\n",
                "                        # Calculate metricts for OPTIMIZED model\n",
                "                        metrics_train_opt = compute_classification_metrics(y_train_model, y_train_opt, proposed_avg, proposed_pos)\n",
                "                        metrics_test_opt  = compute_classification_metrics(y_test_model,  y_test_opt,  proposed_avg, proposed_pos)\n",
                "                        # Build final table with results for OPTIMIZED model\n",
                "                        optimized_results_class[\"OPTIMIZED \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_, \"Scoring\": classification_scoring_target}\n",
                "                        optimized_results_class[\"OPTIMIZED \" + model_name + \" - üß™ TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_, \"Scoring\": classification_scoring_target}\n",
                "                        # Store trained model\n",
                "                        trained_models[model_name][\"optimized\"] = best_model\n",
                "                        # Store hyperparameters for RandomForestClassifier\n",
                "                        if model_name == \"RandomForestClassifier\":\n",
                "                            # Extract grid search raw values\n",
                "                            est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                            depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                            scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                            # Unique n_estimators\n",
                "                            est_unique = np.sort(np.unique(est_values))\n",
                "                            # Clean depths (convert to Python None or int)\n",
                "                            cleaned_depths = []\n",
                "                            for d in depth_values:\n",
                "                                if d is None:\n",
                "                                    cleaned_depths.append(None)\n",
                "                                else:\n",
                "                                    cleaned_depths.append(int(d))\n",
                "                            # Unique depths excluding None\n",
                "                            depth_unique = []\n",
                "                            for d in cleaned_depths:\n",
                "                                if d is not None and d not in depth_unique:\n",
                "                                    depth_unique.append(d)\n",
                "                            depth_unique = sorted(depth_unique)\n",
                "                            # Dictionary: key = max_depth ‚Üí list of mean scores over n_estimators\n",
                "                            depth_score_dict = {}\n",
                "                            for d in depth_unique:\n",
                "                                mean_scores = []\n",
                "                                for e in est_unique:\n",
                "                                    mask = (est_values == e) & (np.array(cleaned_depths, dtype=object) == d)\n",
                "                                    mean_scores.append(scores[mask].mean())\n",
                "                                depth_score_dict[d] = mean_scores\n",
                "                            # Store internally\n",
                "                            trained_models[model_name][\"est_list\"]   = est_unique\n",
                "                            trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                            trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                        # Store hyperparameters for Boosting models\n",
                "                        if model_name in [\"GradientBoostingClassifier\", \"XGBClassifier\", \"LGBMClassifier\"]:\n",
                "                            # Retrieve raw values\n",
                "                            est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
                "                            depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
                "                            scores = grid.cv_results_[\"mean_test_score\"]\n",
                "                            # Unique n_estimators\n",
                "                            est_unique = np.sort(np.unique(est_values))\n",
                "                            # Clean depths (convert to int)\n",
                "                            cleaned_depths = []\n",
                "                            for d in depth_values:\n",
                "                                if d is None:\n",
                "                                    cleaned_depths.append(None)\n",
                "                                else:\n",
                "                                    cleaned_depths.append(int(d))\n",
                "                            # Depth unique (exclude None)\n",
                "                            depth_unique = []\n",
                "                            for d in cleaned_depths:\n",
                "                                if d is not None and d not in depth_unique:\n",
                "                                    depth_unique.append(d)\n",
                "                            depth_unique = sorted(depth_unique)\n",
                "                            # Dictionary: depth ‚Üí scores list\n",
                "                            depth_score_dict = {}\n",
                "                            for depth in depth_unique:\n",
                "                                mean_scores = []\n",
                "                                for est in est_unique:\n",
                "                                    mask = (est_values == est) & (np.array(cleaned_depths, dtype=object) == depth)\n",
                "                                    mean_scores.append(scores[mask].mean())\n",
                "                                depth_score_dict[depth] = mean_scores\n",
                "                            # Store internally\n",
                "                            trained_models[model_name][\"est_list\"] = est_unique\n",
                "                            trained_models[model_name][\"depth_list\"] = depth_unique\n",
                "                            trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
                "                        # Store hyperparameter K for KNN classifier\n",
                "                        if model_name == \"KNeighborsClassifier\":\n",
                "                            # Retrieve k_list from grid OR build manually\n",
                "                            if \"param_n_neighbors\" in grid.cv_results_:\n",
                "                                k_list = np.sort(np.unique(grid.cv_results_[\"param_n_neighbors\"].data.astype(int)))\n",
                "                            else:\n",
                "                                # Fallback ‚Üí use same values as grid input\n",
                "                                k_list = classification_grids[\"KNeighborsClassifier\"][\"n_neighbors\"]\n",
                "                            train_scores = []\n",
                "                            test_scores  = []\n",
                "                            for k in k_list:\n",
                "                                tmp_model = KNeighborsClassifier(n_neighbors=int(k), metric=grid.best_params_.get(\"metric\", \"euclidean\"), weights=grid.best_params_.get(\"weights\", \"uniform\"))\n",
                "                                tmp_model.fit(X_train_model, y_train_model)\n",
                "                                # Compute train & test scores\n",
                "                                train_scores.append(tmp_model.score(X_train_model, y_train_model))\n",
                "                                test_scores.append(tmp_model.score(X_test_model, y_test_model))\n",
                "                            # Store internally for plotting phase\n",
                "                            trained_models[model_name][\"k_list_train_test\"] = (k_list, train_scores, test_scores)\n",
                "                    else:\n",
                "                        optimized_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "                        optimized_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
                "                # -------------------------------\n",
                "                #  FINAL TABLES\n",
                "                # -------------------------------\n",
                "                if len(classification_selected_list)>0:\n",
                "                    print(\"\\n==================== ‚öñÔ∏è CLASSIFICATION MODELS COMPARISON ====================\")\n",
                "                    display(pd.DataFrame(default_results_class).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "                    display(pd.DataFrame(optimized_results_class).T.style.set_table_styles([{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}]))\n",
                "                # -------------------------------\n",
                "                # PLOTTING\n",
                "                # -------------------------------\n",
                "                for model_name, model_dict in trained_models.items():\n",
                "                    # LINEAR MODELS ‚Üí Coefficient Plotter - Only plot for TABULAR datasets\n",
                "                    if model_name in [\"LogisticRegression\", \"LinearSVC\"] and dataset_type == \"TABULAR\":\n",
                "                        # Some linear models (like LinearSVC) may not have coef_ if kernel != linear\n",
                "                        default_model = model_dict[\"default\"]\n",
                "                        if not hasattr(default_model, \"coef_\"):\n",
                "                            continue\n",
                "                        print(f\"\\n==================== üìä MODEL PLOT: {model_name} (Coefficients) ====================\")\n",
                "                        # Set subplot\n",
                "                        num_rows = 1\n",
                "                        num_cols = 1\n",
                "                        fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                        # Plot coefficients\n",
                "                        plotter_linear_coefficients(model=default_model, X=X_train_model,ax=ax, model_name=model_name, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size)\n",
                "                        plt.tight_layout()\n",
                "                        plt.show()\n",
                "                        continue\n",
                "                    print(f\"\\n==================== üìä MODEL PLOT: {model_name} ====================\")\n",
                "                    # If DecisionTree ‚Üí use Decision Tree plotter\n",
                "                    if model_name in [\"DecisionTreeClassifier\"]:\n",
                "                        # Set subplot\n",
                "                        num_rows= 1\n",
                "                        num_cols= 2\n",
                "                        fig, axes = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                        # Plot DEFAULT\n",
                "                        plotter_DecisionTree_model(model=model_dict[\"default\"], X=X_train_model, ax=axes[0])\n",
                "                        axes[0].set_title(f\"{model_name} - DEFAULT\", fontsize=plot_title_font_size)\n",
                "                        # Plot OPTIMIZED\n",
                "                        plotter_DecisionTree_model(model=model_dict[\"optimized\"], X=X_train_model, ax=axes[1])\n",
                "                        axes[1].set_title(f\"{model_name} - OPTIMIZED\", fontsize=plot_title_font_size)\n",
                "                        plt.tight_layout()\n",
                "                        plt.show()\n",
                "                        continue\n",
                "                    if model_name in [\"KNeighborsClassifier\"]:\n",
                "                        k_tuple = model_dict.get(\"k_list_train_test\", None)\n",
                "                        if k_tuple is not None:\n",
                "                            k_list, train_scores, test_scores = k_tuple\n",
                "                            # Set Subplot\n",
                "                            num_rows = 1\n",
                "                            num_cols = 1\n",
                "                            fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                            # Plot\n",
                "                            plotter_KNN_hparam_k(model_name=model_name, k_list=k_list, train_scores=train_scores, test_scores=test_scores, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                            plt.tight_layout()\n",
                "                            plt.show()\n",
                "                        continue\n",
                "                    # If RandomForest ‚Üí use unified RandomForest hyperparameter plotter\n",
                "                    if model_name in [\"RandomForestClassifier\"]:\n",
                "                        # Set values \n",
                "                        est_list   = model_dict.get(\"est_list\", None)\n",
                "                        depth_list = model_dict.get(\"depth_list\", None)\n",
                "                        depth_score_dict = model_dict.get(\"depth_score_dict\", None)\n",
                "                        if est_list is not None and depth_list is not None and depth_score_dict is not None:\n",
                "                            # Set subplot\n",
                "                            num_rows= 1\n",
                "                            num_cols= 1\n",
                "                            fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                            # Plot\n",
                "                            plotter_RandomForest_hparam(model_name=model_name, est_list=est_list, depth_list=depth_list, depth_score_dict=depth_score_dict, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                            plt.tight_layout()\n",
                "                            plt.show()\n",
                "                        continue\n",
                "                    # If Boosting models ‚Üí use unified Boosting hyperparameter plotter\n",
                "                    if model_name in [\"GradientBoostingClassifier\", \"XGBClassifier\", \"LGBMClassifier\"]:\n",
                "                        # Set values\n",
                "                        est_list   = model_dict.get(\"est_list\", None)\n",
                "                        depth_list = model_dict.get(\"depth_list\", None)\n",
                "                        depth_score_dict = model_dict.get(\"depth_score_dict\", None)\n",
                "                        if est_list is not None and depth_list is not None and depth_score_dict is not None:\n",
                "                            # Set subplot\n",
                "                            num_rows= 1\n",
                "                            num_cols= 1\n",
                "                            fig, ax = plt.subplots(num_rows, num_cols, figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
                "                            # Plot\n",
                "                            plotter_Boosting_hparam(model_name=model_name, est_list=est_list, depth_list=depth_list, depth_score_dict=depth_score_dict, ax=ax, label_fontsize=plot_label_font_size, title_fontsize=plot_title_font_size, reg_scoring_target=regression_scoring_target, cla_scoring_target=classification_scoring_target)\n",
                "                            plt.tight_layout()\n",
                "                            plt.show()\n",
                "                        continue"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 18 - SAVE MODELS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===============================\n",
                "# STEP TITLE\n",
                "# ===============================\n",
                "print(\"-------------------------------\")\n",
                "print(\"STEP 18) SAVE MODELS - \",learning_type, dataset_type)\n",
                "print(\"-------------------------------\\n\")\n",
                "\n",
                "# ===============================\n",
                "# TIME-SERIES DATASET\n",
                "# ===============================\n",
                "if dataset_type == \"TIME-SERIES\":\n",
                "    arima_filename = models_output_path + f\"ARIMA_rev{rev_to_use}.sav\"\n",
                "    dump(arima_result, open(arima_filename, \"wb\"))\n",
                "    log(f\"Model saved: {arima_filename}\", type=\"SUCCESS\")\n",
                "    auto_arima_filename = models_output_path + f\"AUTO_MANUAL_rev{rev_to_use}.sav\"\n",
                "    dump(auto_arima_model, open(auto_arima_filename, \"wb\"))\n",
                "    log(f\"Model saved: {auto_arima_filename}\", type=\"SUCCESS\")\n",
                "\n",
                "# ===============================\n",
                "# NLP & TABULAR DATASET\n",
                "# ===============================\n",
                "elif supervised_learning:\n",
                "    for model_name, model_dict in trained_models.items():\n",
                "        # DEFAULT\n",
                "        if model_dict.get(\"default\") is not None:\n",
                "            fname = models_output_path + f\"SUPERVISED_{model_name}_DEFAULT_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"default\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")\n",
                "        # OPTIMIZED\n",
                "        if model_dict.get(\"optimized\") is not None:\n",
                "            fname = models_output_path + f\"SUPERVISED_{model_name}_OPTIMIZED_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"optimized\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")\n",
                "\n",
                "elif not supervised_learning:\n",
                "    for model_name, model_dict in trained_unsup_models.items():\n",
                "        # DEFAULT clustering model\n",
                "        if model_dict.get(\"default\") is not None:\n",
                "            fname = models_output_path + f\"UNSUPERVISED_{model_name}_DEFAULT_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"default\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")\n",
                "        # OPTIMIZED clustering model\n",
                "        if model_dict.get(\"optimized\") is not None:\n",
                "            fname = models_output_path + f\"UNSUPERVISED_{model_name}_OPTIMIZED_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"optimized\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")\n",
                "\n",
                "if pseudo_tagging:\n",
                "    for model_name, model_dict in trained_models.items():\n",
                "        # DEFAULT\n",
                "        if model_dict.get(\"default\") is not None:\n",
                "            fname = models_output_path + f\"SUPERVISED_PSEUDO_TAGGED_{model_name}_DEFAULT_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"default\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")\n",
                "\n",
                "        # OPTIMIZED\n",
                "        if model_dict.get(\"optimized\") is not None:\n",
                "            fname = models_output_path + f\"SUPERVISED_PSEUDO_TAGGED_{model_name}_OPTIMIZED_rev{rev_to_use}.sav\"\n",
                "            dump(model_dict[\"optimized\"], open(fname, \"wb\"))\n",
                "            log(f\"Model saved: {fname}\", type=\"SUCCESS\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
