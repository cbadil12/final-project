{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eef1f9c",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE PYTHON & DATA MANIPULATION LIBRARIES\n",
    "import numpy as np                # Numerical computations, arrays, math operations\n",
    "import pandas as pd               # Data handling, DataFrames, time-series structures\n",
    "import os                         # OS-level utilities (path handling, directory checks, file management)\n",
    "# VISUALIZATION LIBRARIES\n",
    "import matplotlib.pyplot as plt   # Main plotting library\n",
    "import seaborn as sns             # Statistical and enhanced visualization tools\n",
    "# STEP 3) DECOMPOSING\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose         # Decompose time-series into trend, seasonal, and residual components\n",
    "from statsmodels.tsa.stattools import acf                       # Compute autocorrelation values\n",
    "# STEP 4) STATIONARITY ANALYSIS\n",
    "from statsmodels.tsa.stattools import adfuller                  # Dickey-Fuller test for stationarity evaluation\n",
    "# STEP 5) RESIDUALS DIAGNOSTICS\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox         # Ljung-Box test for checking autocorrelation in residuals\n",
    "# STEP 6) ACF/PACF DIAGNOSTICS + ORDER GRIDS\n",
    "from statsmodels.tsa.stattools import pacf                      # Compute partial autocorrelation values\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf   # Plot ACF & PACF with confidence bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da3f91",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Log printer\n",
    "# ===============================\n",
    "def log(\n",
    "    message: str,\n",
    "    icon: str = \"INFO\",\n",
    "    level: int = 1,\n",
    "    bold: bool = False\n",
    "):\n",
    "    # Icon dictionary (semantic â†’ emoji)\n",
    "    icons = {\n",
    "        \"INFO\": \"â„¹ï¸\",\n",
    "        \"FOUND\": \"ðŸ”\",\n",
    "        \"SUCCESS\": \"âœ…\",\n",
    "        \"ERROR\": \"âŒ\",\n",
    "        \"WARNING\": \"âš ï¸\",\n",
    "        \"SUGGESTION\": \"ðŸ’¡\"\n",
    "    }\n",
    "\n",
    "    # If icon matches a key â†’ use mapped emoji\n",
    "    # Else â†’ assume user passed a custom emoji/string\n",
    "    final_icon = icons.get(icon.upper(), icon)\n",
    "    # Bold wrapper (ANSI)\n",
    "    if bold:\n",
    "        message = f\"\\033[1m{message}\\033[0m\"\n",
    "    # Prefix by level\n",
    "    if level == 1:\n",
    "        prefix = \"â€¢\"\n",
    "    elif level == 2:\n",
    "        prefix = \"   -\"\n",
    "    elif level == 3:\n",
    "        prefix = \"      Â·\"\n",
    "    else:\n",
    "        prefix = \"-\"\n",
    "    # Print\n",
    "    print(f\"{prefix} {final_icon} {message}\")\n",
    "\n",
    "# ===============================\n",
    "# Get revision number - Returns the next free integer revision based on existing files\n",
    "# ===============================\n",
    "def get_revision_number(base_path, base_name):\n",
    "    rev = 0\n",
    "    while True:\n",
    "        full_path = os.path.join(base_path, base_name + \"_\" + str(rev) + \".csv\")\n",
    "        if not os.path.exists(full_path):\n",
    "            return rev\n",
    "        rev += 1\n",
    "\n",
    "# ===============================\n",
    "# Determines granularity given seconds\n",
    "# ===============================\n",
    "def determine_granularity(seconds: float) -> str:\n",
    "    # Use tolerance windows (robust)\n",
    "    if 3500 <= seconds <= 3700:\n",
    "        return \"1-hour\"\n",
    "    if 3.5*3600 <= seconds <= 4.5*3600:\n",
    "        return \"4-hour\"\n",
    "    if 23*3600 <= seconds <= 25*3600:\n",
    "        return \"1-day\"\n",
    "    return \"other\"\n",
    "\n",
    "# ===============================\n",
    "# Map granularity to pandas frequency alias\n",
    "# ===============================\n",
    "def get_pandas_freq_from_granularity(granularity: str) -> str | None:\n",
    "    if granularity==\"1-hour\":\n",
    "        return \"h\"\n",
    "    if granularity==\"4-hour\":\n",
    "        return \"4h\"\n",
    "    if granularity==\"1-day\":\n",
    "        return \"D\"\n",
    "    return None\n",
    "\n",
    "# ===============================\n",
    "# Pick a safe max lag for ACF/PACF based on granularity + seasonal period\n",
    "# ===============================\n",
    "def get_safe_lag_for_acf_pacf(granularity: str, period_m: int, n: int, max_cap: int = 300) -> int:\n",
    "    base_candidates=lag_candidates_from_granularity(granularity)  # base lags from granularity\n",
    "    nice=[l for l in base_candidates if l>=2]  # keep only meaningful lags\n",
    "    if period_m is not None and period_m>1:  # add seasonal period multiples if valid\n",
    "        nice=nice+[period_m,2*period_m,3*period_m]  # enrich candidates with seasonality\n",
    "    nice=sorted(list(set(nice)))  # deduplicate and sort candidates\n",
    "    target=max(nice) if nice else 50  # choose highest candidate or default\n",
    "    safe=min(target,max_cap,n-2)  # cap by user max and sample size\n",
    "    return max(10,safe)  # enforce minimum lag floor\n",
    "\n",
    "# ===============================\n",
    "# Candidate seasonal periods by granularity (time steps)\n",
    "# ===============================\n",
    "def period_candidates_from_granularity(granularity: str) -> list[int]:\n",
    "    if granularity == \"1-hour\":\n",
    "        return [24, 168]\n",
    "    if granularity == \"4-hour\":\n",
    "        return [6, 42]\n",
    "    if granularity == \"1-day\":\n",
    "        return [7, 14]\n",
    "    return []\n",
    "\n",
    "# ===============================\n",
    "# Returns an integer period (m) to use in decomposition / SARIMA.\n",
    "#- If manual_period is provided -> use it.\n",
    "# - Else -> try ACF among plausible candidates.\n",
    "# - Else -> fallback to the first plausible candidate.\n",
    "# If there are no candidates, raises ValueError.\n",
    "# ===============================\n",
    "def decide_seasonal_period(\n",
    "    timeseries: pd.Series,\n",
    "    granularity: str,\n",
    "    manual_period: int | None = None,\n",
    "    acf_threshold: float = 0.20\n",
    "):\n",
    "    candidates = period_candidates_from_granularity(granularity)\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No valid period candidates for granularity='{granularity}'\")\n",
    "    # 1) Manual override\n",
    "    log(f\"1st) Trying to infer period from manual input:\", level=2)\n",
    "    if manual_period is not None:\n",
    "        return manual_period, \"MANUAL PERIOD\"\n",
    "    else:\n",
    "        log(f\"No manual input was indicated for seasonal period\", icon=\"WARNING\", level=3)\n",
    "    # 2) ACF among candidates\n",
    "    #m_acf = infer_period_from_acf_candidates(timeseries, candidates, threshold=acf_threshold)\n",
    "    m_acf=infer_period_from_acf_candidates(timeseries,candidates,threshold=acf_threshold,debug=True)\n",
    "    if m_acf is not None:\n",
    "        return m_acf, \"ACF PERIOD\"\n",
    "    # 3) Fallback: first candidate (daily if available, else weekly)\n",
    "    log(f\"3rd) Fallback to infer period as first candidate based on granularity\", level=2)\n",
    "    return candidates[0], \"FIRST CANDIDATE PERIOD\"\n",
    "\n",
    "# ===============================\n",
    "# Choose the candidate m with the highest ACF value at lag m using a more stationary version of the series (diff)\n",
    "# Returns:\n",
    "#- best m if ACF(m) >= threshold\n",
    "#- None otherwise\n",
    "# ===============================\n",
    "def infer_period_from_acf_candidates_old(\n",
    "    series: pd.Series,\n",
    "    candidates: list[int],\n",
    "    threshold: float = 0.20,\n",
    "    max_lag_cap: int = 400\n",
    ") -> int | None:\n",
    "    x=pd.Series(series).dropna().astype(float)          # ensure numeric series and drop NaNs\n",
    "    x=x.diff().dropna()                                 # difference to make series more stationary\n",
    "    if x.empty or not candidates:                       # no data or no candidates to evaluate\n",
    "        return None\n",
    "    max_lag=min(max(max(candidates)*2,50),max_lag_cap)  # set lag window with caps\n",
    "    if len(x)<max_lag+5:                                # not enough samples for reliable ACF\n",
    "        return None\n",
    "    ac=acf(x,nlags=max_lag,fft=True)                    # compute ACF values up to max_lag\n",
    "    best_m=None                                         # track best candidate lag\n",
    "    best_val=-np.inf                                    # track best ACF value\n",
    "    for m in candidates:                                # evaluate each candidate lag\n",
    "        if m<=max_lag and ac[m]>best_val:               # keep max ACF among valid lags\n",
    "            best_val=ac[m]                              # update best ACF value\n",
    "            best_m=m                                    # update best candidate lag\n",
    "    if best_m is not None and best_val>=threshold:      # accept only if above threshold\n",
    "        return best_m\n",
    "    return None                                         # otherwise reject and return None\n",
    "\n",
    "# ===============================\n",
    "# Infer seasonal period from ACF values among candidate lags\n",
    "# ===============================\n",
    "def infer_period_from_acf_candidates(series: pd.Series,candidates: list[int],threshold: float = 0.20,max_lag_cap: int = 400,debug: bool = True) -> int | None:\n",
    "    x=pd.Series(series).dropna().astype(float)\n",
    "    if debug:\n",
    "        log(f\"2nd) Trying to infer period from acf (n={len(x)} | candidates={candidates} | threshold={threshold}):\", level=2)\n",
    "    x=x.diff().dropna()\n",
    "    if debug:\n",
    "        log(f\"After differentiation: n={len(x)}\", level=3)\n",
    "    if x.empty or not candidates:\n",
    "        if debug:\n",
    "            log(\"After differentiation: empty series or no candidates -> return None\", icon=\"WARNING\", level=3)\n",
    "        return None\n",
    "    max_lag=min(max(max(candidates)*2,50),max_lag_cap)\n",
    "    if debug:\n",
    "        log(f\"Lags: max_lag={max_lag} (cap={max_lag_cap})\", level=3)\n",
    "    if len(x)<max_lag+5:\n",
    "        if debug:\n",
    "            log(f\"Not enough samples for nlags={max_lag} -> need >= {max_lag+5}, have {len(x)} -> return None\", icon=\"WARNING\", level=3)\n",
    "        return None\n",
    "    ac=acf(x,nlags=max_lag,fft=True,missing=\"drop\")\n",
    "    best_m=None\n",
    "    best_val=-np.inf\n",
    "    for m in candidates:\n",
    "        if m<=max_lag:\n",
    "            val=float(ac[m])\n",
    "            if debug:\n",
    "                log(f\"ACF at lag m={m}: {val:.4f}\", icon=\"INFO\", level=3)\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_m=m\n",
    "    if debug:\n",
    "        log(f\"Best candidate: m={best_m} -> acf={best_val:.4f}\", level=3)\n",
    "    if best_m is not None and best_val>=threshold:\n",
    "        if debug:\n",
    "            log(f\"ACCEPT period inference from acf m={best_m} (acf={best_val:.4f} >= {threshold})\", icon=\"SUCCESS\", level=3, bold=True)\n",
    "        return best_m\n",
    "    if debug:\n",
    "        log(f\"REJECT period inference from acf (best acf={best_val:.4f} < {threshold}) -> return None\", icon=\"WARNING\", level=3)\n",
    "    return None\n",
    "\n",
    "# ===============================\n",
    "# Generate returns for crypto time-series\n",
    "# ===============================\n",
    "def generate_returns(\n",
    "    price_series: pd.Series, # Time-indexed price series (e.g. close).\n",
    "    method: str = \"log\",      # \"log\" or \"diff\" -> log returns (recommended for crypto)\n",
    "    clip_outliers: bool = True, #Whether to clip extreme returns (robustness)\n",
    "    clip_quantile: float = 0.01 # Quantile for clipping (0.01 -> clip 1% tails)\n",
    ") -> pd.Series:\n",
    "    s = price_series.astype(float).copy()\n",
    "    if method == \"log\":\n",
    "        # Avoid log(0)\n",
    "        s = s.replace(0, np.nan)\n",
    "        returns = np.log(s).diff()\n",
    "    elif method == \"diff\":\n",
    "        returns = s.diff()\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'log' or 'diff'\")\n",
    "    returns = returns.dropna()\n",
    "    # Optional: clip extreme tails (important for crypto spikes)\n",
    "    if clip_outliers and len(returns) > 50:\n",
    "        lower = returns.quantile(clip_quantile)\n",
    "        upper = returns.quantile(1 - clip_quantile)\n",
    "        returns = returns.clip(lower, upper)\n",
    "    return returns\n",
    "    \n",
    "# ===============================\n",
    "# 6) Evaluates how strong the seasonality is using:\n",
    "# 1) Variance ratio: Var(seasonal) / Var(original)\n",
    "# 2) ACF at the seasonal period\n",
    "# ===============================\n",
    "def assess_seasonality_strength(\n",
    "    original: pd.Series,\n",
    "    seasonal: pd.Series,\n",
    "    period: int,\n",
    "    acf_threshold: float,\n",
    "    var_ratio: float):\n",
    "    # Align indices and remove NaN values from the seasonal component\n",
    "    valid_mask = seasonal.notna()\n",
    "    original_valid = original[valid_mask]\n",
    "    seasonal_valid = seasonal[valid_mask]\n",
    "    # If there are not enough valid points â†’ cannot assess\n",
    "    if len(original_valid) < max(10, period * 2):\n",
    "        metrics = {\n",
    "            \"seasonal_var_ratio\": np.nan,\n",
    "            \"acf_at_period\": np.nan\n",
    "        }\n",
    "        return False, metrics\n",
    "    # 1) Variance ratio\n",
    "    total_var = np.var(original_valid)\n",
    "    seasonal_var = np.var(seasonal_valid)\n",
    "    if total_var == 0:\n",
    "        seasonal_var_ratio = 0.0\n",
    "    else:\n",
    "        seasonal_var_ratio = seasonal_var / total_var\n",
    "    # 2) ACF at seasonal period\n",
    "    acf_values = acf(\n",
    "        original_valid,\n",
    "        nlags = period,\n",
    "        fft = True,\n",
    "        missing = \"drop\"\n",
    "    )\n",
    "    acf_at_period = acf_values[period]\n",
    "    # 3) Decision rule\n",
    "    strong_seasonality = ((seasonal_var_ratio >= var_ratio) and (acf_at_period >= acf_threshold))\n",
    "    # 4) Metrics\n",
    "    metrics = {\n",
    "        \"seasonal_var_ratio\": seasonal_var_ratio,\n",
    "        \"acf_at_period\": acf_at_period\n",
    "    }\n",
    "    return strong_seasonality, metrics\n",
    "\n",
    "# ===============================\n",
    "# Performs Dickey-Fuller test to determine if a series is stacionary or not\n",
    "# ===============================\n",
    "def test_stationarity(series):\n",
    "    dftest = adfuller(series, autolag = \"AIC\")\n",
    "    dfoutput = pd.Series(dftest[0:4], index = [\"Test Statistic\", \"p-value\", \"#Lags Used\", \"Number of Observations Used\"])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput[\"Critical Value (%s)\"%key] = value\n",
    "    return dfoutput\n",
    "\n",
    "# ===============================\n",
    "# Recursively differences the time-series until Dickey-Fuller test accepts stationarity (p < alpha)\n",
    "# ===============================\n",
    "def make_stationary_recursive(series, alpha: float = 0.05, max_diff: int = 5):\n",
    "    current_series = series.copy()\n",
    "    diff_count = 0\n",
    "    while diff_count <= max_diff:\n",
    "        test_results = test_stationarity(current_series)\n",
    "        if test_results[\"p-value\"] < alpha:\n",
    "            return current_series, diff_count, test_results\n",
    "        current_series = current_series.diff().dropna()\n",
    "        diff_count += 1\n",
    "    # If exceeded max_diff â†’ return last attempt\n",
    "    return current_series, diff_count, test_results\n",
    "\n",
    "# ===============================\n",
    "# Returns meaningful Ljungâ€“Box / ACF lags (expressed in number of time steps) based on time granularity\n",
    "# ===============================\n",
    "def lag_candidates_from_granularity(granularity: str) -> list[int]:\n",
    "    if granularity == \"1-hour\":\n",
    "        return [\n",
    "            2,      # 2h (short-term micro autocorrelation)\n",
    "            6,      # 6h\n",
    "            24,     # 1 day\n",
    "            48,     # 2 days\n",
    "            168     # 1 week\n",
    "        ]\n",
    "    if granularity == \"4-hour\":\n",
    "        return [\n",
    "            2,      # 8h (very short-term)\n",
    "            3,      # 12h\n",
    "            6,      # 1 day\n",
    "            12,     # 2 days\n",
    "            42      # 1 week\n",
    "        ]\n",
    "    if granularity == \"1-day\":\n",
    "        return [\n",
    "            2,      # 2 days\n",
    "            7,      # 1 week\n",
    "            14,     # 2 weeks\n",
    "            30      # ~1 month\n",
    "        ]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2cd75",
   "metadata": {},
   "source": [
    "INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f519ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# INPUTS: HALVING REGIMES\n",
    "# ===============================\n",
    "# Exact Bitcoin halving dates (YYYY-MM-DD) used for regime windows\n",
    "halving_dates = [\"2012-11-28\", \"2016-07-09\", \"2020-05-11\", \"2024-04-20\"]\n",
    "\n",
    "# ===============================\n",
    "# STEP 1) EXPLORE DATAFRAME AND BUILD TIMESERIES\n",
    "# ===============================\n",
    "raw_data_separator = \";\"    # CSV separator for the raw file\n",
    "raw_data_input_path = \"../data/raw/raw_price_dowloaded_4h_START_31-12-2011_END_20-12-2025.csv\"  # Path to raw input CSV\n",
    "time_column = \"time\"        # Datetime column name used as primary time axis\n",
    "target_column = \"close\"     # Target variable to model as time-series\n",
    "day_comes_first = True      # Date parsing order: day/month/year\n",
    "\n",
    "# ===============================\n",
    "# STEP 2) SEASONALITY / DECOMPOSITION\n",
    "# ===============================\n",
    "seasonal_period = None                  # Manual seasonal period m; None -> auto inference\n",
    "\n",
    "# ===============================\n",
    "# STEP 3) STATIONARITY (ADF) -> INFER d (FOR ARIMA & SARIMA) AND D (FOR SARIMA)\n",
    "# ===============================\n",
    "accepted_alpha_dickey_fuller = 0.05     # ADF alpha threshold to accept stationarity\n",
    "\n",
    "# ===============================\n",
    "# STEP 5) TRAIN/TEST SPLIT + NAIVE BASELINE + METRICS\n",
    "# ===============================\n",
    "test_size = 0.2                         # Fraction of data reserved for test split\n",
    "\n",
    "# ===============================\n",
    "# STEPS 5-6) OUTPUT FILES\n",
    "# ===============================\n",
    "processed_data_output_path = \"../data/processed/\"  # Output folder for saved datasets/metrics\n",
    "\n",
    "# ===============================\n",
    "# STEPS 1, 5, 6) PLOTTING\n",
    "# ===============================\n",
    "figHeight_unit = 8          # Base figure height\n",
    "figWidth_unit = 12          # Base figure width\n",
    "plot_tick_font_size = 15    # Tick label font size\n",
    "plot_label_font_size = 15   # Axis label font size\n",
    "plot_title_font_size = 30   # Plot title font size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd386c",
   "metadata": {},
   "source": [
    "STEP 1) EXPLORE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================\")\n",
    "print(\"STEP 1) EXPLORE DATAFRAME AND BUILD TIMESERIES\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# LOAD RAW DATAFRAME\n",
    "log(f\"LOAD RAW DATAFRAME\", icon=\"1.\", bold=True)\n",
    "# Load dataframe\n",
    "df_raw = pd.read_csv(raw_data_input_path, sep=raw_data_separator)\n",
    "# Print raw df info\n",
    "log(f\"Shape of the raw DataFrame: {df_raw.shape}\", level=2)\n",
    "log(\"Preview of the raw DataFrame:\", level=2)\n",
    "display(df_raw.head(5))\n",
    "print(\"\\n\")\n",
    "\n",
    "# EXPLORE DATAFRAME\n",
    "log(f\"EXPLORE DATAFRAME\", icon=\"2.\", bold=True)\n",
    "# Previous data\n",
    "df = df_raw.copy()\n",
    "# Get time series\n",
    "serie_time = pd.to_datetime(df[time_column], errors=\"coerce\", dayfirst=day_comes_first) #'coerce': then invalid parsing will be set as NaT\n",
    "# Keep only rows with valid timestamps (avoid NaT contaminating checks)\n",
    "valid_mask=serie_time.notna()\n",
    "df=df.loc[valid_mask].copy()\n",
    "serie_time=serie_time.loc[valid_mask].copy()\n",
    "# Sort by time before diff/frequency checks\n",
    "order_idx=serie_time.sort_values().index\n",
    "df=df.loc[order_idx].copy()\n",
    "serie_time=serie_time.loc[order_idx].copy()\n",
    "# Find if any time goes backwards\n",
    "serie_time_diff = serie_time.diff().dropna()\n",
    "bad_idx = serie_time_diff[serie_time_diff < pd.Timedelta(0)].index\n",
    "# Find if any duplicated timestamps\n",
    "dup_count = serie_time.duplicated().sum()\n",
    "dup_times=serie_time[serie_time.duplicated(keep=False)]\n",
    "# Numeric metrics\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "log(f\"Numeric metrics detected: {numeric_cols}\", icon=\"ðŸ“ˆ\", level=2)\n",
    "# Statistics for each metric\n",
    "log(\"Basic statistics per numeric variable:\", level=2)\n",
    "display(df[numeric_cols].describe().T)\n",
    "print(\"\\n\")\n",
    "\n",
    "# TIME INDEX INFORMATION\n",
    "log(\"TIME INDEX INFORMATION\", icon=\"3.\", bold=True)\n",
    "log(f\"Detected time column: '{time_column}'\", icon=\"ðŸ“…\", level=2)\n",
    "log(f\"Rows removed due to invalid timestamps (NaT): {len(df_raw)-len(df)}\", icon=\"ðŸ“…\", level=2)\n",
    "log(f\"Start date: {serie_time.min()}\", icon=\"ðŸ“…\", level=2)\n",
    "log(f\"End date: {serie_time.max()}\", icon=\"ðŸ“…\", level=2)\n",
    "log(f\"Total duration: {serie_time.max() - serie_time.min()}\", icon=\"ðŸ“…\", level=2)\n",
    "if len(bad_idx) >0:\n",
    "    log(f\"Number of negative diffs: {len(bad_idx)}\", icon=\"ERROR\", level=2)\n",
    "    log(f\"Example indices: {bad_idx[:10].tolist()}\", icon=\"ERROR\", level=2)\n",
    "    raise ValueError(f\"Negative time differences found\")\n",
    "else:\n",
    "    log(\"No negative diffs detected\", icon=\"SUCCESS\", level=2)\n",
    "if dup_count>0:\n",
    "    log(f\"Number of duplicated timestamps: {dup_count}\", icon=\"ERROR\", level=2)\n",
    "    log(f\"Duplicated timestamps sample: {dup_times.head(10).tolist()}\", icon=\"ERROR\", level=2)\n",
    "    raise ValueError(f\"Duplicated timestamps found\")\n",
    "else:\n",
    "    log(\"No duplicated timestamps detected\", icon=\"SUCCESS\", level=2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ESTIMATE FREQUENCY AND GRANULARITY\n",
    "log(f\"ESTIMATE FREQUENCY AND GRANULARITY\", icon=\"4.\", bold=True)\n",
    "if len(serie_time_diff) > 0:\n",
    "    df_most_common_delta = serie_time_diff.mode()[0] # Most common interval\n",
    "    df_smallest_delta = serie_time_diff.min() # Minimal interval\n",
    "    df_freq_ratio = (serie_time_diff == df_most_common_delta).mean()\n",
    "    if df_freq_ratio < 0.7:\n",
    "        raise ValueError(f\"Decomposition skipped due to Low frequency regularity (freq_ratio={df_freq_ratio:.3f})\")\n",
    "    # Determine granularity\n",
    "    granularity = determine_granularity(df_most_common_delta.total_seconds())\n",
    "    if granularity==\"other\":\n",
    "        log(f\"Granularity not recognized from seconds={df_most_common_delta.total_seconds()} -> period inference may fail\",level=2,type=\"WARNING\")\n",
    "    log(f\"Most common interval: {df_most_common_delta} ({df_most_common_delta.total_seconds()} secs) -> granularity: {granularity}\", icon=\"ðŸ“…\", level=2, bold=True)\n",
    "    log(f\"Smallest interval: {df_smallest_delta} ({df_smallest_delta.total_seconds()} secs)\", icon=\"ðŸ“…\", level=2, bold=True)\n",
    "    log(f\"Frequency consistency ratio: {df_freq_ratio:.8f}\", icon=\"ðŸ“…\", level=2, bold=True)\n",
    "else:\n",
    "    log(\"Not enough data points to estimate frequency nor granurality\", icon=\"ERROR\", level=2)\n",
    "    raise ValueError(f\"Not enough data points\")\n",
    "# Missing or irregular timestamps\n",
    "if len(serie_time_diff) > 0:\n",
    "    missing_ratio = 1 - df_freq_ratio\n",
    "else:\n",
    "    missing_ratio = None\n",
    "if missing_ratio is not None and missing_ratio > 0.10:\n",
    "    log(\"Irregular timestamps detected (missing or uneven intervals)\", level=2, type=\"WARNING\")\n",
    "    log(f\"Irregularity ratio: {missing_ratio:.2f}\", level=2, custom_icon=\"âš ï¸\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# BUILD TIMESERIES\n",
    "log(f\"BUILD TIMESERIES\", icon=\"5.\", bold=True)\n",
    "# Assign the datetime index\n",
    "df.index=pd.DatetimeIndex(serie_time)\n",
    "# Make sure the index has a name\n",
    "df.index.name=time_column\n",
    "if not df.index.is_monotonic_increasing:\n",
    "    raise ValueError(\"Datetime index is not sorted\")\n",
    "if df.index.duplicated().any():\n",
    "    raise ValueError(\"Datetime index has duplicates\")\n",
    "log(f\"Indexed DataFrame by '{time_column}'\", level=2)\n",
    "# Show preview\n",
    "log(\"Preview of time-indexed DataFrame:\", level=2)\n",
    "display(df.head(5))\n",
    "log(f\"Target column detected: '{target_column}'\", level=2)\n",
    "nan_target=df[target_column].isna().sum()\n",
    "if nan_target>0:\n",
    "    log(f\"NaN values found in target '{target_column}': {nan_target} -> rows will be dropped for modeling\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    log(f\"None NaN values found in target '{target_column}'\", icon=\"SUCCESS\", level=2)\n",
    "# Extract target timeseries\n",
    "timeseries=df[target_column].dropna().astype(float)\n",
    "log(f\"Target column '{target_column}' ({len(timeseries)} rows) has been successfully extracted as time-series:\", icon=\"SUCCESS\", level=2)\n",
    "print(timeseries[:5].to_string(dtype=False))\n",
    "# Visualization plot\n",
    "fig,axis=plt.subplots(figsize=(figWidth_unit,figHeight_unit))\n",
    "sns.lineplot(data=timeseries)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc756d4",
   "metadata": {},
   "source": [
    "STEP 2) DECOMPOSE TIME-SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================\")\n",
    "print(\"STEP 2) DECOMPOSE TIME-SERIES\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# AUTOMATIC DETERMINE PERIOD\n",
    "log(f\"AUTOMATIC DETERMINATION OF SEASONAL PERIOD (m):\", icon=\"1.\", bold=True)\n",
    "period , period_origin =decide_seasonal_period(timeseries=timeseries,granularity=granularity,manual_period=seasonal_period) # manual_period can be set as e.g. 6/24/7/14\n",
    "log(f\"   -> Selected seasonal period (m)={period} for '{granularity}' granularity\",icon=\"SUCCESS\", level=2, bold=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "# SEASONALITY STRENGTH METRICS BASED ON RETURNS\n",
    "log(f\"SEASONALITY STRENGTH METRICS BASED ON RETURNS (LOG-DIFF)\", icon=\"2.\", bold=True)\n",
    "if len(timeseries)<period*3:\n",
    "    raise ValueError(f\"Not enough samples for decomposition: n={len(timeseries)} < 3*period={period*3}\")\n",
    "returns=generate_returns(timeseries,method=\"log\")\n",
    "if len(returns)<period*3:\n",
    "    raise ValueError(f\"Not enough samples for returns decomposition: n={len(returns)} < 3*period={period*3}\")\n",
    "log(f\"Returns generated: n={len(returns)} (dropped {len(timeseries)-len(returns)} rows due to diff/log)\", icon=\"SUCCESS\", level=2)\n",
    "# Decompose RETURNS series for consistent seasonality strength metrics (all in returns units)\n",
    "try:\n",
    "    decomposition_ret=seasonal_decompose(x=returns,model=\"additive\",period=period)\n",
    "    trend_ret=decomposition_ret.trend\n",
    "    seasonal_ret=decomposition_ret.seasonal\n",
    "    residual_ret=decomposition_ret.resid\n",
    "    log(\"Returns decomposition completed successfully\", icon=\"SUCCESS\", level=2)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Returns decomposition failed: {e}\")\n",
    "#Compute seasonality strength metrics on RETURNS decomposition (in returns units)\n",
    "strong_seasonality,seasonality_metrics_S3=assess_seasonality_strength(original=returns,seasonal=seasonal_ret,period=period,acf_threshold=0.2,var_ratio=0.02)\n",
    "if strong_seasonality:\n",
    "    log(f\"Strong seasonality detected (var_ratio={seasonality_metrics_S3['seasonal_var_ratio']:.3f},acf={seasonality_metrics_S3['acf_at_period']:.3f})\", icon=\"SUCCESS\", level=2)\n",
    "    log(f\"Recommendation: try SARIMA with seasonal period m={period}\", icon=\"SUGGESTION\", level=2)\n",
    "else:\n",
    "    log(f\"Weak or no seasonality (var_ratio={seasonality_metrics_S3['seasonal_var_ratio']:.3f},acf={seasonality_metrics_S3['acf_at_period']:.3f})\", icon=\"WARNING\", level=2)\n",
    "    log(\"Recommendation: ARIMA baseline (no clear seasonality detected)\", icon=\"SUGGESTION\", level=2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# RETURN RESIDUAL DIAGNOSTICS\n",
    "log(f\"RETURN RESIDUAL DIAGNOSTICS (n={len(residual_ret)})\", icon=\"3.\", bold=True)\n",
    "# Use RETURNS residuals as primary diagnostics (consistent with seasonality strength logic)\n",
    "residual_ret=residual_ret.dropna().astype(float)\n",
    "# Safety: need enough points for diagnostics\n",
    "if len(residual_ret)<50:\n",
    "    log(\"Not enough residual points for robust diagnostics (n<50). Skipping residual checks.\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    # -------------------------------------------\n",
    "    #RULE A: Check for visible trend in residuals (slope ~ 0)\n",
    "    # -------------------------------------------\n",
    "    x_index=np.arange(len(residual_ret))\n",
    "    slope,intercept=np.polyfit(x_index,residual_ret.values,1)\n",
    "    residual_std=np.std(residual_ret.values)\n",
    "    if residual_std<1e-8:\n",
    "        log(\"Residual slope analysis: residuals are almost constant -> no visible trend (good)\", icon=\"SUCCESS\", level=2)\n",
    "    else:\n",
    "        if abs(slope)<max(residual_std*0.01,1e-6):\n",
    "            log(f\"Residual slope analysis: no visible trend (slope={slope:.3e}) (good)\", icon=\"SUCCESS\", level=2)\n",
    "        else:\n",
    "            log(f\"Residual slope analysis: trend detected (slope={slope:.3e}) (bad)\", icon=\"WARNING\", level=2)\n",
    "    # -------------------------------------------\n",
    "    #RULE B: Check periodicity using ACF (should NOT show strong peaks)\n",
    "    # -------------------------------------------\n",
    "    if residual_std<1e-8:\n",
    "        log(\"Residual ACF analysis: residuals almost constant -> no periodicity possible (good)\", icon=\"SUCCESS\", level=2)\n",
    "    else:\n",
    "        nlags=min(max(10,2*period),min(80,len(residual_ret)-2))\n",
    "        if nlags<5:\n",
    "            log(\"Residual ACF analysis: not enough points to compute ACF reliably\", icon=\"WARNING\", level=2)\n",
    "        else:\n",
    "            acf_res=acf(residual_ret,nlags=nlags,fft=True,missing=\"drop\")\n",
    "            acf_res_no0=acf_res[1:]\n",
    "            max_acf_lag=np.argmax(np.abs(acf_res_no0))+1\n",
    "            max_acf_value=acf_res[max_acf_lag]\n",
    "            log(f\"Residual ACF strongest lag={max_acf_lag},value={max_acf_value:.3f}\", level=2)\n",
    "            if np.isnan(max_acf_value):\n",
    "                log(\"Residual ACF analysis: ACF could not be computed reliably -> assume no periodicity (good)\", icon=\"SUCCESS\", level=2)\n",
    "            elif abs(max_acf_value)<0.3:\n",
    "                log(\"Residual ACF analysis: no meaningful periodic patterns detected (good)\", icon=\"SUCCESS\", level=2)\n",
    "            else:\n",
    "                log(\"Residual ACF analysis: periodic pattern detected in residuals (bad)\", icon=\"WARNING\", level=2)\n",
    "    # -------------------------------------------\n",
    "    # RULE C: Check residuals are centered (mean ~ 0 for additive)\n",
    "    # -------------------------------------------\n",
    "    residual_center=0.0\n",
    "    mean_res=residual_ret.mean()\n",
    "    sd_res=residual_ret.std()\n",
    "    threshold_center=max(sd_res*0.05,1e-6)\n",
    "    if abs(mean_res-residual_center)<threshold_center:\n",
    "        log(f\"Residual center analysis: centered around {residual_center} (mean={mean_res:.3e}) (good)\", icon=\"SUCCESS\", level=2)\n",
    "    else:\n",
    "        log(f\"Residual center analysis: not well centered (mean={mean_res:.3e}) (bad)\", icon=\"WARNING\", level=2)\n",
    "    # -------------------------------------------\n",
    "    # RULE D: Randomness / autocorrelation using Ljungâ€“Box (want p > 0.05)\n",
    "    # -------------------------------------------\n",
    "    # Center residuals (0 for additive, 1 for multiplicative already handled before)\n",
    "    residual_centered=residual_ret-residual_center\n",
    "    sd_centered=residual_centered.std()\n",
    "\n",
    "    # If residuals are almost constant â†’ cannot test randomness, assume GOOD\n",
    "    if sd_centered<1e-8:\n",
    "        log(\"Residual randomness analysis: residuals almost constant -> assume random (good)\", icon=\"SUCCESS\", level=2)\n",
    "    else:\n",
    "        # Get interpretable lags based on granularity\n",
    "        test_lags=lag_candidates_from_granularity(granularity)\n",
    "        # Keep only safe lags\n",
    "        test_lags=[l for l in test_lags if 2<=l<len(residual_centered)-2]\n",
    "\n",
    "        if not test_lags:\n",
    "            log(\"Residual randomness analysis: no valid lags available for Ljung-Box\", icon=\"WARNING\", level=2)\n",
    "        else:\n",
    "            # Ljungâ€“Box on residuals\n",
    "            lb_df=acorr_ljungbox(residual_centered,lags=test_lags,return_df=True)\n",
    "            for L in test_lags:\n",
    "                pval=float(lb_df.loc[L,\"lb_pvalue\"])\n",
    "                log(f\"Ljung-Box p-value (lag {L})={pval:.4f}\", level=2)\n",
    "\n",
    "            if (lb_df[\"lb_pvalue\"]>0.05).all():\n",
    "                log(\"Residual randomness analysis: residuals behave like white noise (good)\", icon=\"SUCCESS\", level=2)\n",
    "            else:\n",
    "                log(\"Residual randomness analysis: residuals show autocorrelation (bad)\", icon=\"WARNING\", level=2)\n",
    "            # Optional: volatility clustering check (ARCH/GARCH hint)\n",
    "            lb_abs=acorr_ljungbox(np.abs(residual_centered),lags=test_lags,return_df=True)\n",
    "            if (lb_abs[\"lb_pvalue\"]<=0.05).any():\n",
    "                log(\"Volatility effect detected: |residuals| are autocorrelated (ARCH/GARCH-like behavior)\", icon=\"WARNING\",bold=True, level=2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# DECOMPOSITION OF PRICE FOR VISUALIZATION (trend/seasonal/residual in price units)\n",
    "log(f\"DECOMPOSITION OF PRICE FOR VISUALIZATION:\", icon=\"4.\", bold=True)\n",
    "try:\n",
    "    decomposition_price=seasonal_decompose(x=timeseries,model=\"additive\",period=period)\n",
    "    trend_price=decomposition_price.trend\n",
    "    seasonal_price=decomposition_price.seasonal\n",
    "    residual_price=decomposition_price.resid\n",
    "    log(\"Price decomposition completed successfully\", icon=\"SUCCESS\", level=2)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Price decomposition failed: {e}\")\n",
    "#Plot PRICE decomposition (visual understanding in price units)\n",
    "fig,axis=plt.subplots(figsize=(figWidth_unit,figHeight_unit))\n",
    "sns.lineplot(data=timeseries,color=\"blue\",label=\"Price (close)\")\n",
    "sns.lineplot(data=trend_price,color=\"orange\",label=\"Trend\",linestyle=\"--\")\n",
    "sns.lineplot(data=residual_price,color=\"red\",label=\"Residual\")\n",
    "sns.lineplot(data=seasonal_price,color=\"green\",label=\"Seasonal\",linestyle=\"--\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3b621",
   "metadata": {},
   "source": [
    "STEP 3) STATIONARITY (ADF) -> INFER d AND D (FOR SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f47c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================\")\n",
    "print(\"STEP 3) STATIONARITY (ADF) -> INFER d (FOR ARIMA & SARIMA) AND D (FOR SARIMA)\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# INFER NON-SEASONAL DIFFERENCING ORDER - d (FOR ARIMA & SARIMA)\n",
    "log(\"INFER NON-SEASONAL DIFFERENCING ORDER - d (FOR ARIMA & SARIMA)\", icon=\"1.\", bold=True)\n",
    "series_adf_base=timeseries.dropna().astype(float)\n",
    "adf_base=test_stationarity(series=series_adf_base)\n",
    "log(\"DICKEY-FULLER TEST TO DETERMINE IF BASE SERIES IS STATIONARY\", icon=\"FOUND\", level=2)\n",
    "print(adf_base.to_string(dtype=False))\n",
    "if adf_base[\"p-value\"]<accepted_alpha_dickey_fuller:\n",
    "    d=0\n",
    "    series_stationary_nonseasonal=series_adf_base\n",
    "    log(f\"Reject H0 (unit root) at alpha={accepted_alpha_dickey_fuller}: series IS stationary -> d={d}\", icon=\"SUCCESS\", level=3)\n",
    "else:\n",
    "    log(f\"Fail to reject H0 (unit root) at alpha={accepted_alpha_dickey_fuller}: series is NOT stationary -> Applying recursive differencing to infer d\", icon=\"WARNING\", level=3)\n",
    "    series_stationary_nonseasonal,d,adf_after_diff=make_stationary_recursive(series=series_adf_base,alpha=accepted_alpha_dickey_fuller)\n",
    "    log(f\"DICKEY-FULLER TEST TO DETERMINE IF SERIES IS STATIONARY AFTER {d} DIFFERENTIATION\", icon=\"FOUND\", level=2)\n",
    "    display(adf_after_diff)\n",
    "    if d>2:\n",
    "        log(f\"High differencing order detected (d={d}). This is unusual; consider log transform or returns-based modeling\", icon=\"WARNING\", level=3)\n",
    "    if adf_after_diff[\"p-value\"]<accepted_alpha_dickey_fuller:\n",
    "        log(f\"Reject H0 after differencing: stationary achieved with d={d}\", icon=\"SUCCESS\", level=3)\n",
    "    else:\n",
    "        log(f\"Fail to reject H0 even after differencing up to d={d}: series may still be non-stationary or needs alternative transforms\", icon=\"WARNING\", level=3)\n",
    "print(\"\\n\")\n",
    "\n",
    "# INFER SEASONAL DIFFERENCING ORDER - D (ONLY MEANINGFUL FOR SARIMA)\n",
    "log(\"INFER SEASONAL DIFFERENCING ORDER - D (ONLY MEANINGFUL FOR SARIMA)\", icon=\"2.\", bold=True)\n",
    "D=0\n",
    "if period is None or period<=1:\n",
    "    log(\"No valid seasonal period m provided -> skipping seasonal differencing inference (D=0)\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    if not strong_seasonality:\n",
    "        log(\"Seasonality strength is weak (from STEP 2) -> keep D=0 for baseline comparison\", icon=\"WARNING\", level=2, bold=True)\n",
    "        D=0\n",
    "    else:\n",
    "        log(\"Seasonality strength is strong (from STEP 2) -> evaluating seasonal differencing D using ADF on d-differenced series\", level=2, bold=True)\n",
    "        series_for_D=series_adf_base.copy()\n",
    "        if d>0:\n",
    "            series_for_D=series_for_D.diff(d).dropna()\n",
    "        if len(series_for_D)<max(60,period*5):\n",
    "            log(\"Not enough data after non-seasonal differencing to evaluate D reliably -> keep D=0\", icon=\"WARNING\", level=2)\n",
    "            D=0\n",
    "        else:\n",
    "            log(\"ADF on series after non-seasonal differencing (reference check before adding seasonal differencing)\", icon=\"INFO\", level=2)\n",
    "            adf_ref=test_stationarity(series=series_for_D)\n",
    "            log(f\"ADF results (after d={d}, no seasonal diff): {adf_ref}\", level=2)\n",
    "            log(\"ADF on series after non-seasonal + seasonal differencing (candidate D=1)\", level=2)\n",
    "            series_for_D_seasonal=series_for_D.diff(period).dropna()\n",
    "            if len(series_for_D_seasonal)<max(60,period*5):\n",
    "                log(\"Not enough data after adding seasonal differencing -> keep D=0\", icon=\"WARNING\", level=2)\n",
    "                D=0\n",
    "            else:\n",
    "                adf_seasonal=test_stationarity(series=series_for_D_seasonal)\n",
    "                log(f\"ADF results (after d={d} and seasonal diff lag m): {adf_seasonal}\")\n",
    "                if adf_seasonal[\"p-value\"]<accepted_alpha_dickey_fuller:\n",
    "                    D=1\n",
    "                    log(f\"Reject H0 after adding seasonal differencing: seasonal stationarity improved -> suggest D={D}\", icon=\"SUCCESS\", level=2, bold=True)\n",
    "                else:\n",
    "                    D=0\n",
    "                    log(f\"Fail to reject H0 after adding seasonal differencing: no clear benefit -> suggest D={D}\", icon=\"WARNING\", level=2, bold=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "#C) MODELS PARAMETERS RECOMMENDATION\n",
    "log(\"MODELS PARAMETERS RECOMMENDATION\", icon=\"3.\", bold=True)\n",
    "log(f\"ARIMA baseline structure: (p,d,q) with d={d}\", icon=\"SUGGESTION\", level=2, bold=True)\n",
    "log(f\"SARIMA candidate structure: (p,d,q)(P,D,Q,m) with d={d},D={D},m={period}\", icon=\"SUGGESTION\", level=2, bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f27604",
   "metadata": {},
   "source": [
    "STEP 4) ACF/PACF DIAGNOSTICS + ORDER SUGGESTIONS (ARIMA vs SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57640d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# STEP 4) ACF/PACF DIAGNOSTICS + ORDER SUGGESTIONS\n",
    "# (adds small AIC/BIC grid check + regime window plots)\n",
    "# ===============================\n",
    "print(\"===============================\")\n",
    "print(\"STEP 4) ACF/PACF DIAGNOSTICS + ORDER SUGGESTIONS\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1) PREPARE DIAGNOSTICS SERIES\n",
    "# -------------------------------\n",
    "log(\"PREPARE DIAGNOSTICS SERIES\", icon=\"1.\", bold=True)\n",
    "# Prepare diagnostics series in PRICE units, but differenced to be closer to stationary\n",
    "series_for_diag = timeseries.dropna().astype(float).copy()\n",
    "freq = get_pandas_freq_from_granularity(granularity)\n",
    "if freq is not None:\n",
    "    try:\n",
    "        series_for_diag = series_for_diag.asfreq(freq)\n",
    "        log(f\"Applied pandas frequency '{freq}' for diagnostics.\", icon=\"SUCCESS\", level=2)\n",
    "    except Exception as e:\n",
    "        log(f\"Could not apply pandas frequency '{freq}': {e}\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    log(f\"No pandas freq mapping for granularity='{granularity}' -> skipping asfreq.\", icon=\"WARNING\", level=2)\n",
    "\n",
    "# Non-seasonal differencing for ARIMA diagnostics (use d from STEP 3)\n",
    "series_diag_ns = series_for_diag.copy()\n",
    "if d > 0:\n",
    "    series_diag_ns = series_diag_ns.diff(d).dropna()\n",
    "log(f\"Diagnostics series prepared with non-seasonal differencing d={d} (n={len(series_diag_ns)})\", icon=\"INFO\", level=2)\n",
    "\n",
    "# Default suggestions\n",
    "suggested_p = 0\n",
    "suggested_q = 0\n",
    "suggested_P = 0\n",
    "suggested_Q = 0\n",
    "\n",
    "# -------------------------------\n",
    "# 2) NON-SEASONAL ACF/PACF -> suggest (p, q)\n",
    "# -------------------------------\n",
    "log(\"NON-SEASONAL ACF/PACF -> suggest p/q\", icon=\"2.\", bold=True)\n",
    "if len(series_diag_ns) < 30 or series_diag_ns.std() < 1e-8:\n",
    "    log(\"ACF/PACF analysis: differenced series too short or almost constant -> default p=q=0.\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    n = len(series_diag_ns)\n",
    "    safe_lag = get_safe_lag_for_acf_pacf(granularity, period, n, max_cap=300)\n",
    "    conf_limit = 1.96 / np.sqrt(n)\n",
    "    log(f\"Using safe_lag={safe_lag} (n={n}, conf~=+/-{conf_limit:.3f})\", icon=\"INFO\", level=2)\n",
    "\n",
    "    # ACF/PACF on non-seasonal differenced series -> suggest (p, q)\n",
    "    ac_vals = acf(series_diag_ns, nlags=safe_lag, fft=True, missing=\"drop\")\n",
    "    pc_vals = pacf(series_diag_ns, nlags=safe_lag, method=\"ywm\")\n",
    "    sig_acf = [lag for lag in range(1, len(ac_vals)) if abs(ac_vals[lag]) > conf_limit]\n",
    "    sig_pacf = [lag for lag in range(1, len(pc_vals)) if abs(pc_vals[lag]) > conf_limit]\n",
    "\n",
    "    if len(sig_acf) == 0:\n",
    "        suggested_q = 0\n",
    "        log(\"ACF: no significant lags -> q~0 candidate\", icon=\"INFO\", level=2)\n",
    "    else:\n",
    "        suggested_q = int(sig_acf[0])\n",
    "        log(f\"ACF: first significant lag={suggested_q} -> q~{suggested_q} candidate\", icon=\"INFO\", level=2)\n",
    "\n",
    "    if len(sig_pacf) == 0:\n",
    "        suggested_p = 0\n",
    "        log(\"PACF: no significant lags -> p~0 candidate\", icon=\"INFO\", level=2)\n",
    "    else:\n",
    "        suggested_p = int(sig_pacf[0])\n",
    "        log(f\"PACF: first significant lag={suggested_p} -> p~{suggested_p} candidate\", icon=\"INFO\", level=2)\n",
    "\n",
    "    # Plot ACF/PACF (non-seasonal diagnostics)\n",
    "    fig_acf, ax_acf = plt.subplots(figsize=(figWidth_unit, 0.5 * figHeight_unit))\n",
    "    plot_acf(series_diag_ns, lags=safe_lag, ax=ax_acf)\n",
    "    ax_acf.set_title(\"ACF on differenced PRICE series (suggest q)\", fontsize=plot_title_font_size)\n",
    "    ax_acf.set_xlabel(\"Lag\", fontsize=plot_label_font_size)\n",
    "    ax_acf.set_ylabel(\"Autocorrelation\", fontsize=plot_label_font_size)\n",
    "    ax_acf.tick_params(labelsize=plot_tick_font_size)\n",
    "    ax_acf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
    "    if period is not None and period > 1 and period <= safe_lag:\n",
    "        ax_acf.axvline(x=period, linewidth=3, alpha=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_pacf, ax_pacf = plt.subplots(figsize=(figWidth_unit, 0.5 * figHeight_unit))\n",
    "    plot_pacf(series_diag_ns, lags=safe_lag, ax=ax_pacf, method=\"ywm\")\n",
    "    ax_pacf.set_title(\"PACF on differenced PRICE series (suggest p)\", fontsize=plot_title_font_size)\n",
    "    ax_pacf.set_xlabel(\"Lag\", fontsize=plot_label_font_size)\n",
    "    ax_pacf.set_ylabel(\"Partial autocorrelation\", fontsize=plot_label_font_size)\n",
    "    ax_pacf.tick_params(labelsize=plot_tick_font_size)\n",
    "    ax_pacf.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3) SEASONAL ACF/PACF -> suggest (P, Q)\n",
    "# -------------------------------\n",
    "log(\"SEASONAL ACF/PACF -> suggest P/Q\", icon=\"3.\", bold=True)\n",
    "if period is None or period <= 1:\n",
    "    log(\"Seasonal order suggestion skipped: invalid seasonal period m.\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    if not strong_seasonality:\n",
    "        log(\"Seasonality is weak -> keep P=0,Q=0 (still can test SARIMA later).\", icon=\"WARNING\", level=2)\n",
    "    else:\n",
    "        series_diag_s = series_diag_ns.copy()\n",
    "        if D > 0:\n",
    "            series_diag_s = series_diag_s.diff(period).dropna()\n",
    "        log(f\"Seasonal diagnostics series prepared with D={D} and m={period} (n={len(series_diag_s)})\", icon=\"INFO\", level=2)\n",
    "\n",
    "        if len(series_diag_s) < max(60, period * 5):\n",
    "            log(\"Not enough points for seasonal ACF/PACF -> keep P=0,Q=0.\", icon=\"WARNING\", level=2)\n",
    "        else:\n",
    "            safe_lag_s = int(min(max(2 * period, 60), len(series_diag_s) - 2, 400))\n",
    "            conf_limit_s = 1.96 / np.sqrt(len(series_diag_s))\n",
    "            ac_s = acf(series_diag_s, nlags=safe_lag_s, fft=True, missing=\"drop\")\n",
    "            pc_s = pacf(series_diag_s, nlags=safe_lag_s, method=\"ywm\")\n",
    "\n",
    "            seasonal_lags = [k * period for k in range(1, (safe_lag_s // period) + 1)]\n",
    "            sig_seasonal_acf = [L for L in seasonal_lags if L < len(ac_s) and abs(ac_s[L]) > conf_limit_s]\n",
    "            sig_seasonal_pacf = [L for L in seasonal_lags if L < len(pc_s) and abs(pc_s[L]) > conf_limit_s]\n",
    "\n",
    "            if len(sig_seasonal_acf) > 0:\n",
    "                suggested_Q = 1\n",
    "                log(f\"Seasonal ACF: significant at lag {int(sig_seasonal_acf[0])} -> Q~1 candidate\", icon=\"INFO\", level=2)\n",
    "            else:\n",
    "                suggested_Q = 0\n",
    "                log(\"Seasonal ACF: no significant seasonal multiples -> Q~0 candidate\", icon=\"INFO\", level=2)\n",
    "\n",
    "            if len(sig_seasonal_pacf) > 0:\n",
    "                suggested_P = 1\n",
    "                log(f\"Seasonal PACF: significant at lag {int(sig_seasonal_pacf[0])} -> P~1 candidate\", icon=\"INFO\", level=2)\n",
    "            else:\n",
    "                suggested_P = 0\n",
    "                log(\"Seasonal PACF: no significant seasonal multiples -> P~0 candidate\", icon=\"INFO\", level=2)\n",
    "\n",
    "            fig_acf_s, ax_acf_s = plt.subplots(figsize=(figWidth_unit, 0.5 * figHeight_unit))\n",
    "            plot_acf(series_diag_s, lags=safe_lag_s, ax=ax_acf_s)\n",
    "            ax_acf_s.set_title(\"ACF on (d + seasonal) differenced PRICE series (suggest Q)\", fontsize=plot_title_font_size)\n",
    "            ax_acf_s.set_xlabel(\"Lag\", fontsize=plot_label_font_size)\n",
    "            ax_acf_s.set_ylabel(\"Autocorrelation\", fontsize=plot_label_font_size)\n",
    "            ax_acf_s.tick_params(labelsize=plot_tick_font_size)\n",
    "            ax_acf_s.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
    "            ax_acf_s.axvline(x=period, linewidth=3, alpha=0.8)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            fig_pacf_s, ax_pacf_s = plt.subplots(figsize=(figWidth_unit, 0 * figHeight_unit))\n",
    "            plot_pacf(series_diag_s, lags=safe_lag_s, ax=ax_pacf_s, method=\"ywm\")\n",
    "            ax_pacf_s.set_title(\"PACF on (d + seasonal) differenced PRICE series (suggest P)\", fontsize=plot_title_font_size)\n",
    "            ax_pacf_s.set_xlabel(\"Lag\", fontsize=plot_label_font_size)\n",
    "            ax_pacf_s.set_ylabel(\"Partial autocorrelation\", fontsize=plot_label_font_size)\n",
    "            ax_pacf_s.tick_params(labelsize=plot_tick_font_size)\n",
    "            ax_pacf_s.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
    "            ax_pacf_s.axvline(x=period, linewidth=3, alpha=0.8)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 4) SMALL AIC/BIC GRID CHECK (sanity only)\n",
    "# -------------------------------\n",
    "log(\"SMALL AIC/BIC GRID CHECK (sanity)\", icon=\"4.\", bold=True)\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    _SARIMAX_OK = True\n",
    "except Exception as e:\n",
    "    _SARIMAX_OK = False\n",
    "    log(f\"SARIMAX import failed -> skip AIC/BIC grid ({e})\", icon=\"WARNING\", level=2)\n",
    "\n",
    "if _SARIMAX_OK:\n",
    "    p_cands = sorted(list(set([0, 1, suggested_p])))\n",
    "    q_cands = sorted(list(set([0, 1, suggested_q])))\n",
    "    P_cands = sorted(list(set([0, 1, suggested_P])))\n",
    "    Q_cands = sorted(list(set([0, 1, suggested_Q])))\n",
    "\n",
    "    arima_scores = []\n",
    "    for p_try in p_cands:\n",
    "        for q_try in q_cands:\n",
    "            try:\n",
    "                res = SARIMAX(endog=series_for_diag, order=(p_try, d, q_try), seasonal_order=(0, 0, 0, 0),\n",
    "                              enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                arima_scores.append((p_try, d, q_try, res.aic, res.bic))\n",
    "            except Exception as e:\n",
    "                log(f\"ARIMA({p_try},{d},{q_try}) failed: {e}\", icon=\"WARNING\", level=2)\n",
    "\n",
    "    if arima_scores:\n",
    "        arima_by_aic = sorted(arima_scores, key=lambda x: x[3])[:5]\n",
    "        arima_by_bic = sorted(arima_scores, key=lambda x: x[4])[:5]\n",
    "\n",
    "        def fmt_arima(rows):\n",
    "            return \"; \".join([f\"(p,d,q)=({p},{d},{q}) | AIC={aic:.2f} BIC={bic:.2f}\" for p,d,q,aic,bic in rows])\n",
    "\n",
    "        log(f\"Top ARIMA by AIC: {fmt_arima(arima_by_aic)}\", icon=\"INFO\", level=2)\n",
    "        log(f\"Top ARIMA by BIC: {fmt_arima(arima_by_bic)}\", icon=\"INFO\", level=2)\n",
    "\n",
    "    if period is None or period <= 1:\n",
    "        log(\"SARIMA grid skipped: invalid seasonal period m\", icon=\"WARNING\", level=2)\n",
    "    else:\n",
    "        sarima_scores = []\n",
    "        for p_try in p_cands:\n",
    "            for q_try in q_cands:\n",
    "                for P_try in P_cands:\n",
    "                    for Q_try in Q_cands:\n",
    "                        try:\n",
    "                            res = SARIMAX(endog=series_for_diag, order=(p_try, d, q_try),\n",
    "                                          seasonal_order=(P_try, D, Q_try, period),\n",
    "                                          enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                            sarima_scores.append((p_try, d, q_try, P_try, D, Q_try, period, res.aic, res.bic))\n",
    "                        except Exception as e:\n",
    "                            log(f\"SARIMA({p_try},{d},{q_try})({P_try},{D},{Q_try},{period}) failed: {e}\", icon=\"WARNING\", level=2)\n",
    "\n",
    "        if sarima_scores:\n",
    "            sarima_by_aic = sorted(sarima_scores, key=lambda x: x[7])[:5]\n",
    "            sarima_by_bic = sorted(sarima_scores, key=lambda x: x[8])[:5]\n",
    "\n",
    "            def fmt_sarima(rows):\n",
    "                return \"; \".join([f\"(p,d,q)({p},{d},{q}) (P,D,Q,m)=({P},{D},{Q},{m}) | AIC={aic:.2f} BIC={bic:.2f}\"\n",
    "                                for p,d,q,P,D,Q,m,aic,bic in rows])\n",
    "\n",
    "            log(f\"Top SARIMA by AIC: {fmt_sarima(sarima_by_aic)}\", icon=\"INFO\", level=2)\n",
    "            log(f\"Top SARIMA by BIC: {fmt_sarima(sarima_by_bic)}\", icon=\"INFO\", level=2)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) REGIME WINDOW PLOTS (HALVING-BASED)\n",
    "# -------------------------------\n",
    "log(\"REGIME WINDOW PLOTS (HALVING-BASED)\", icon=\"5.\", bold=True)\n",
    "series_plot = series_for_diag.copy().dropna()\n",
    "if len(series_plot) < 120:\n",
    "    log(\"Not enough points for regime window plots (need >=120).\", icon=\"WARNING\", level=2)\n",
    "else:\n",
    "    try:\n",
    "        hv_dates = pd.to_datetime(halving_dates, errors=\"coerce\").dropna().sort_values()\n",
    "    except Exception as e:\n",
    "        hv_dates = pd.to_datetime([], errors=\"coerce\")\n",
    "        log(f\"Could not parse halving_dates -> {e}\", icon=\"WARNING\", level=2)\n",
    "\n",
    "    # Build regime edges from data range + halving dates inside range\n",
    "    start = series_plot.index.min()\n",
    "    end = series_plot.index.max()\n",
    "    hv_in_range = [d for d in hv_dates if start < d < end]\n",
    "    edges = [start] + hv_in_range + [end]\n",
    "\n",
    "    if len(edges) < 2:\n",
    "        log(\"No valid halving boundaries inside data range -> skip regime plot.\", icon=\"WARNING\", level=2)\n",
    "    else:\n",
    "        fig_reg, ax_reg = plt.subplots(figsize=(2 * figWidth_unit, 1 * figHeight_unit))\n",
    "        for i in range(len(edges) - 1):\n",
    "            seg = series_plot.loc[edges[i]:edges[i + 1]]\n",
    "            label = f\"Regime {i+1}: {edges[i].date()} -> {edges[i+1].date()}\"\n",
    "            if len(seg) > 0:\n",
    "                ax_reg.plot(seg.index, seg.values, label=label, alpha=0.8)\n",
    "\n",
    "        # Mark halving dates\n",
    "        for hv_date in hv_in_range:\n",
    "            ax_reg.axvline(x=hv_date, color=\"black\", linewidth=1.5, alpha=0.6)\n",
    "\n",
    "        ax_reg.set_title(\"Regime windows by halving dates\", fontsize=plot_title_font_size)\n",
    "        ax_reg.set_xlabel(\"Time\", fontsize=plot_label_font_size)\n",
    "        ax_reg.set_ylabel(\"Price\", fontsize=plot_label_font_size)\n",
    "        ax_reg.tick_params(labelsize=plot_tick_font_size)\n",
    "        ax_reg.legend()\n",
    "        ax_reg.grid(True, linestyle=\"dotted\", linewidth=0.5, color=\"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Final suggestions logging\n",
    "# -------------------------------\n",
    "print(\"\\n\")\n",
    "log(\"ORDER SUGGESTIONS SUMMARY:\", bold=True)\n",
    "log(f\"ARIMA suggestion: (p,d,q)=({suggested_p},{d},{suggested_q})\", icon=\"INFO\", level=2)\n",
    "log(f\"SARIMA suggestion: (p,d,q)(P,D,Q,m)=({suggested_p},{d},{suggested_q})({suggested_P},{D},{suggested_Q},{period})\", icon=\"INFO\", level=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Build small, coherent grids for the next STEP (model search/backtesting)\n",
    "# -------------------------------\n",
    "arima_grid = []\n",
    "for p_try in sorted(list(set([0, 1, suggested_p]))):\n",
    "    for q_try in sorted(list(set([0, 1, suggested_q]))):\n",
    "        arima_grid.append((p_try, d, q_try))\n",
    "\n",
    "sarima_grid = []\n",
    "for p_try in sorted(list(set([0, 1, suggested_p]))):\n",
    "    for q_try in sorted(list(set([0, 1, suggested_q]))):\n",
    "        for P_try in sorted(list(set([0, 1, suggested_P]))):\n",
    "            for Q_try in sorted(list(set([0, 1, suggested_Q]))):\n",
    "                sarima_grid.append((p_try, d, q_try, P_try, D, Q_try, period))\n",
    "\n",
    "log(f\"ARIMA candidate grid size={len(arima_grid)} -> examples: {arima_grid[:8]}\", icon=\"INFO\", level=2)\n",
    "log(f\"SARIMA candidate grid size={len(sarima_grid)} -> examples: {sarima_grid[:8]}\", icon=\"INFO\", level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240c49f",
   "metadata": {},
   "source": [
    "STEP 5) TIME SERIES TRAIN/TEST SPLIT + NAIVE BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# STEP 5) TRAIN/TEST SPLIT + NAIVE BASELINE + METRICS\n",
    "# ===============================\n",
    "# Split configuration (time-ordered split: last chunk is test)\n",
    "split_idx=int(len(timeseries)*(1.0-test_size))\n",
    "timeseries_train=timeseries.iloc[:split_idx].copy()\n",
    "timeseries_test=timeseries.iloc[split_idx:].copy()\n",
    "log(f\"Split done: train={len(timeseries_train)} rows, test={len(timeseries_test)} rows (test_ratioâ‰ˆ{len(timeseries_test)/len(timeseries):.2f})\",icon=\"SUCCESS\",bold=True)\n",
    "log(f\"Train range: {timeseries_train.index.min()} -> {timeseries_train.index.max()}\",level=2,icon=\"INFO\")\n",
    "log(f\"Test range:  {timeseries_test.index.min()} -> {timeseries_test.index.max()}\",level=2,icon=\"INFO\")\n",
    "# Naive baseline (persistence): predicts next value as the previous observed value\n",
    "# y_hat[t] = y[t-1]\n",
    "log(\"Building naive baseline (persistence): y_hat[t]=y[t-1]\",icon=\"INFO\",bold=True)\n",
    "naive_pred=timeseries_test.shift(1)\n",
    "# First test prediction uses last train value (bridge the train->test boundary)\n",
    "if len(timeseries_train)>0 and len(naive_pred)>0:\n",
    "    naive_pred.iloc[0]=timeseries_train.iloc[-1]\n",
    "# Align and drop NaNs (defensive alignment)\n",
    "y_true=timeseries_test.copy()\n",
    "y_pred_naive=naive_pred.copy()\n",
    "mask_valid_y=y_true.notna() & y_pred_naive.notna()\n",
    "y_true=y_true.loc[mask_valid_y]\n",
    "y_pred_naive=y_pred_naive.loc[mask_valid_y]\n",
    "if len(y_true)<10:\n",
    "    raise ValueError(\"Not enough points to evaluate naive baseline after alignment\")\n",
    "# Metrics helpers (local, minimal)\n",
    "# MAE: Mean Absolute Error -> average absolute deviation |y - y_hat| (same units as the target; easy to interpret)\n",
    "def mae(y_true: pd.Series,y_pred: pd.Series)->float:\n",
    "    return float(np.mean(np.abs(y_true.values-y_pred.values)))\n",
    "# RMSE: Root Mean Squared Error -> penalizes large errors more than MAE (same units as the target; sensitive to spikes)\n",
    "def rmse(y_true: pd.Series,y_pred: pd.Series)->float:\n",
    "    return float(np.sqrt(np.mean((y_true.values-y_pred.values)**2)))\n",
    "# MASE: Mean Absolute Scaled Error -> MAE scaled by in-sample naive MAE (scale-free; <1 means better than naive on average)\n",
    "def mase(y_true: pd.Series,y_pred: pd.Series,y_train: pd.Series)->float:\n",
    "    y_train=y_train.dropna().astype(float)\n",
    "    if len(y_train)<2:\n",
    "        return np.nan\n",
    "    naive_in_sample=np.abs(y_train.diff().dropna()).mean()\n",
    "    if naive_in_sample==0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs(y_true.values-y_pred.values))/naive_in_sample)\n",
    "# SMAPE: Symmetric Mean Absolute Percentage Error -> percentage error with symmetric denominator (more stable than MAPE; bounded-ish)\n",
    "def smape(y_true: pd.Series,y_pred: pd.Series)->float:\n",
    "    y_t=y_true.values.astype(float)\n",
    "    y_p=y_pred.values.astype(float)\n",
    "    num=np.abs(y_p-y_t)\n",
    "    den=(np.abs(y_t)+np.abs(y_p))\n",
    "    mask=den!=0\n",
    "    if mask.sum()==0:\n",
    "        return np.nan\n",
    "    return float(100*np.mean(2*num[mask]/den[mask]))\n",
    "# Compute baseline metrics\n",
    "mae_naive=mae(y_true,y_pred_naive)\n",
    "rmse_naive=rmse(y_true,y_pred_naive)\n",
    "mase_naive=mase(y_true,y_pred_naive,timeseries_train)\n",
    "smape_naive=smape(y_true,y_pred_naive)\n",
    "log(\"Naive baseline metrics (to beat)\",icon=\"INFO\",bold=True)\n",
    "log(f\"MAE   (naive) = {mae_naive:.6f} -> average absolute error in target units\",level=2,icon=\"INFO\")\n",
    "log(f\"RMSE  (naive) = {rmse_naive:.6f} -> penalizes large errors (spikes) in target units\",level=2,icon=\"INFO\")\n",
    "log(f\"MASE  (naive) = {mase_naive:.6f} -> scaled vs in-sample naive; <1 means better than naive\",level=2,icon=\"INFO\")\n",
    "log(f\"SMAPE (naive) = {smape_naive:.2f}% -> symmetric percentage error (robust alternative to MAPE)\",level=2,icon=\"INFO\")\n",
    "# Quick plot: train/test + naive predictions\n",
    "log(\"Plotting train/test split + naive baseline predictions\",icon=\"INFO\")\n",
    "fig,axis=plt.subplots(figsize=(2*figWidth_unit,figHeight_unit))\n",
    "sns.lineplot(x=timeseries_train.index,y=timeseries_train.values,label=\"Train\")\n",
    "sns.lineplot(x=timeseries_test.index,y=timeseries_test.values,label=\"Test\",color=\"green\")\n",
    "sns.lineplot(x=y_pred_naive.index,y=y_pred_naive.values,label=\"Naive (t-1)\",color=\"red\",alpha=0.4,linewidth=3)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ===============================\n",
    "# BUILD FILENAMES WITH REVISION NUMBER\n",
    "# ===============================\n",
    "rev_number=get_revision_number(processed_data_output_path,\"timeseries_train\")\n",
    "output_path_train=processed_data_output_path+\"timeseries_train\"+\"_\"+str(rev_number)+\".csv\"\n",
    "output_path_test=processed_data_output_path+\"timeseries_test\"+\"_\"+str(rev_number)+\".csv\"\n",
    "# ===============================\n",
    "# SAVE ALL DATASETS (keep datetime index)\n",
    "# ===============================\n",
    "timeseries_train.to_csv(output_path_train,index=True)\n",
    "log(f\"timeseries_train saved in {processed_data_output_path} with revision number: {rev_number}\",icon=\"SUCCESS\")\n",
    "timeseries_test.to_csv(output_path_test,index=True)\n",
    "log(f\"timeseries_test saved in {processed_data_output_path} with revision number: {rev_number}\",icon=\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159da587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# STEP 6) TRAIN + EVALUATE ARIMA vs SARIMA (MANUAL + AUTO_ARIMA) (vs Naive Baseline)\n",
    "# - Fit on TRAIN\n",
    "# - Forecast exactly the TEST horizon\n",
    "# - Compare metrics vs Naive baseline (STEP 7)\n",
    "# ===============================\n",
    "print(\"===============================\")\n",
    "print(\"STEP 6) ARIMA vs SARIMA (manual + auto_arima) | train -> forecast test | compare vs Naive\")\n",
    "print(\"===============================\")\n",
    "# ===============================\n",
    "# Imports\n",
    "# ===============================\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    _PMDARIMA_OK=True\n",
    "except Exception as e:\n",
    "    _PMDARIMA_OK=False\n",
    "    log(f\"pmdarima not available -> auto_arima will be skipped ({e})\",icon=\"WARNING\",bold=True)\n",
    "# ===============================\n",
    "# Defensive checks\n",
    "# ===============================\n",
    "if not isinstance(timeseries_train.index,pd.DatetimeIndex) or not isinstance(timeseries_test.index,pd.DatetimeIndex):\n",
    "    raise ValueError(\"Train/Test series must have a DatetimeIndex (check STEP 2 and STEP 7 CSV saving/loading).\")\n",
    "h=len(timeseries_test)\n",
    "if h<10:\n",
    "    raise ValueError(\"Test horizon too small to evaluate models reliably (need >=10).\")\n",
    "if len(timeseries_train)<max(50,period*5):\n",
    "    log(\"Train set is relatively small for seasonal modeling; SARIMA may be unstable.\",icon=\"WARNING\",bold=True)\n",
    "# ===============================\n",
    "# Helpers: alignment + metrics\n",
    "# ===============================\n",
    "def align_true_pred(y_true:pd.Series,y_pred:pd.Series)->tuple[pd.Series,pd.Series]:\n",
    "    mask=y_true.notna() & y_pred.notna()\n",
    "    return y_true.loc[mask].copy(),y_pred.loc[mask].copy()\n",
    "def mae(y_true:pd.Series,y_pred:pd.Series)->float:\n",
    "    return float(np.mean(np.abs(y_true.values-y_pred.values)))\n",
    "def rmse(y_true:pd.Series,y_pred:pd.Series)->float:\n",
    "    return float(np.sqrt(np.mean((y_true.values-y_pred.values)**2)))\n",
    "def mase(y_true:pd.Series,y_pred:pd.Series,y_train:pd.Series)->float:\n",
    "    y_train=y_train.dropna().astype(float)\n",
    "    if len(y_train)<2:\n",
    "        return np.nan\n",
    "    naive_in_sample=np.abs(y_train.diff().dropna()).mean()\n",
    "    if naive_in_sample==0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs(y_true.values-y_pred.values))/naive_in_sample)\n",
    "def smape(y_true:pd.Series,y_pred:pd.Series)->float:\n",
    "    y_t=y_true.values.astype(float)\n",
    "    y_p=y_pred.values.astype(float)\n",
    "    num=np.abs(y_p-y_t)\n",
    "    den=(np.abs(y_t)+np.abs(y_p))\n",
    "    mask=den!=0\n",
    "    if mask.sum()==0:\n",
    "        return np.nan\n",
    "    return float(100*np.mean(2*num[mask]/den[mask]))\n",
    "# ===============================\n",
    "# Helper: fit + forecast using SARIMAX (covers ARIMA and SARIMA)\n",
    "# ===============================\n",
    "def fit_forecast_sarimax(y_train:pd.Series,order:tuple[int,int,int],seasonal_order:tuple[int,int,int,int],forecast_steps:int,forecast_index:pd.Index)->tuple[object,pd.Series]:\n",
    "    model=SARIMAX(endog=y_train,order=order,seasonal_order=seasonal_order,enforce_stationarity=False,enforce_invertibility=False)\n",
    "    res=model.fit(disp=False)\n",
    "    fc=res.get_forecast(steps=forecast_steps).predicted_mean\n",
    "    fc=pd.Series(fc.values,index=forecast_index,name=\"forecast\")\n",
    "    return res,fc\n",
    "# ===============================\n",
    "# MANUAL (AIC) GRID SEARCH ON TRAIN (keeps d/D/m fixed from STEP 4 & STEP 3)\n",
    "# - Manual ARIMA: seasonal_order=(0,0,0,0)\n",
    "# - Manual SARIMA: seasonal_order=(P,D,Q,m)\n",
    "# ===============================\n",
    "p_grid=[0,1,2]\n",
    "q_grid=[0,1,2]\n",
    "P_grid=[0,1,2]\n",
    "Q_grid=[0,1,2]\n",
    "log(\"MANUAL ORDER SELECTION (small AIC grid-search on TRAIN; keeps d/D/m fixed)\",icon=\"INFO\",bold=True)\n",
    "best_arima_aic=np.inf\n",
    "best_arima_order=None\n",
    "best_arima_res=None\n",
    "for p_try in p_grid:\n",
    "    for q_try in q_grid:\n",
    "        try:\n",
    "            res_try=SARIMAX(endog=timeseries_train,order=(p_try,d,q_try),seasonal_order=(0,0,0,0),enforce_stationarity=False,enforce_invertibility=False).fit(disp=False)\n",
    "            if res_try.aic<best_arima_aic:\n",
    "                best_arima_aic=res_try.aic\n",
    "                best_arima_order=(p_try,d,q_try)\n",
    "                best_arima_res=res_try\n",
    "        except Exception as e:\n",
    "            log(f\"ARIMA({p_try},{d},{q_try}) failed: {e}\",level=2,icon=\"WARNING\")\n",
    "if best_arima_order is None:\n",
    "    raise ValueError(\"Manual ARIMA grid-search failed for all candidates.\")\n",
    "log(f\"Manual ARIMA best by AIC: order={best_arima_order} | AIC={best_arima_aic:.2f}\",icon=\"SUCCESS\",level=2,bold=True)\n",
    "best_sarima_aic=np.inf\n",
    "best_sarima_order=None\n",
    "best_sarima_seasonal=None\n",
    "best_sarima_res=None\n",
    "if period is None or period<=1:\n",
    "    log(\"No valid seasonal period m -> manual SARIMA skipped\",icon=\"WARNING\",bold=True)\n",
    "else:\n",
    "    if not strong_seasonality:\n",
    "        log(\"Seasonality flagged as weak (STEP 3) -> SARIMA is still tested for comparison\",icon=\"WARNING\",bold=True)\n",
    "    for p_try in p_grid:\n",
    "        for q_try in q_grid:\n",
    "            for P_try in P_grid:\n",
    "                for Q_try in Q_grid:\n",
    "                    try:\n",
    "                        res_try=SARIMAX(endog=timeseries_train,order=(p_try,d,q_try),seasonal_order=(P_try,D,Q_try,period),enforce_stationarity=False,enforce_invertibility=False).fit(disp=False)\n",
    "                        if res_try.aic<best_sarima_aic:\n",
    "                            best_sarima_aic=res_try.aic\n",
    "                            best_sarima_order=(p_try,d,q_try)\n",
    "                            best_sarima_seasonal=(P_try,D,Q_try,period)\n",
    "                            best_sarima_res=res_try\n",
    "                    except Exception as e:\n",
    "                        log(f\"SARIMA({p_try},{d},{q_try})({P_try},{D},{Q_try},{period}) failed: {e}\",level=3,icon=\"WARNING\")\n",
    "    if best_sarima_order is None:\n",
    "        log(\"Manual SARIMA grid-search failed for all candidates -> SARIMA(manual) skipped\",icon=\"WARNING\",bold=True)\n",
    "    else:\n",
    "        log(f\"Manual SARIMA best by AIC: order={best_sarima_order} seasonal={best_sarima_seasonal} | AIC={best_sarima_aic:.2f}\",icon=\"SUCCESS\",level=2,bold=True)\n",
    "# ===============================\n",
    "# AUTO_ARIMA ORDER SELECTION ON TRAIN (optional)\n",
    "# - Auto ARIMA: seasonal=False\n",
    "# - Auto SARIMA: seasonal=True and m=period\n",
    "# ===============================\n",
    "log(\"AUTO_ARIMA ORDER SELECTION (on TRAIN)\",icon=\"INFO\",bold=True)\n",
    "auto_arima_order=None\n",
    "auto_sarima_order=None\n",
    "auto_sarima_seasonal=None\n",
    "auto_arima_model=None\n",
    "auto_sarima_model=None\n",
    "if _PMDARIMA_OK:\n",
    "    try:\n",
    "        auto_arima_model=auto_arima(y=timeseries_train,seasonal=False,trace=False,suppress_warnings=True,error_action=\"ignore\",stepwise=True)\n",
    "        auto_arima_order=auto_arima_model.order\n",
    "        log(f\"auto_arima (non-seasonal) selected order={auto_arima_order}\",icon=\"SUCCESS\",level=2)\n",
    "    except Exception as e:\n",
    "        log(f\"auto_arima (non-seasonal) failed: {e}\",icon=\"WARNING\",level=2,bold=True)\n",
    "    if period is None or period<=1:\n",
    "        log(\"No valid seasonal period m -> auto_sarima skipped\",icon=\"WARNING\",level=2,bold=True)\n",
    "    else:\n",
    "        try:\n",
    "            auto_sarima_model=auto_arima(y=timeseries_train,seasonal=True,m=period,trace=False,suppress_warnings=True,error_action=\"ignore\",stepwise=True)\n",
    "            auto_sarima_order=auto_sarima_model.order\n",
    "            auto_sarima_seasonal=auto_sarima_model.seasonal_order\n",
    "            log(f\"auto_arima (seasonal) selected order={auto_sarima_order} seasonal={auto_sarima_seasonal}\",icon=\"SUCCESS\",level=2)\n",
    "        except Exception as e:\n",
    "            log(f\"auto_arima (seasonal) failed: {e}\",icon=\"WARNING\",level=2,bold=True)\n",
    "else:\n",
    "    log(\"auto_arima not available -> skipping auto ARIMA/SARIMA\",icon=\"WARNING\",bold=True)\n",
    "# ===============================\n",
    "# FORECAST TEST HORIZON (exactly h steps)\n",
    "# ===============================\n",
    "log(f\"FORECASTING TEST HORIZON: h={h} steps\",icon=\"INFO\",bold=True)\n",
    "# Manual ARIMA forecast\n",
    "arima_manual_pred=None\n",
    "try:\n",
    "    if best_arima_res is not None:\n",
    "        fc=best_arima_res.get_forecast(steps=h).predicted_mean\n",
    "        arima_manual_pred=pd.Series(fc.values,index=timeseries_test.index,name=\"arima_manual_pred\")\n",
    "    else:\n",
    "        _,arima_manual_pred=fit_forecast_sarimax(timeseries_train,order=best_arima_order,seasonal_order=(0,0,0,0),forecast_steps=h,forecast_index=timeseries_test.index)\n",
    "        arima_manual_pred.name=\"arima_manual_pred\"\n",
    "    log(\"Manual ARIMA forecast generated successfully\",icon=\"SUCCESS\",level=2)\n",
    "except Exception as e:\n",
    "    log(f\"Manual ARIMA forecast failed: {e}\",icon=\"ERROR\",bold=True)\n",
    "# Manual SARIMA forecast\n",
    "sarima_manual_pred=None\n",
    "if best_sarima_order is not None and best_sarima_seasonal is not None:\n",
    "    try:\n",
    "        if best_sarima_res is not None:\n",
    "            fc=best_sarima_res.get_forecast(steps=h).predicted_mean\n",
    "            sarima_manual_pred=pd.Series(fc.values,index=timeseries_test.index,name=\"sarima_manual_pred\")\n",
    "        else:\n",
    "            _,sarima_manual_pred=fit_forecast_sarimax(timeseries_train,order=best_sarima_order,seasonal_order=best_sarima_seasonal,forecast_steps=h,forecast_index=timeseries_test.index)\n",
    "            sarima_manual_pred.name=\"sarima_manual_pred\"\n",
    "        log(\"Manual SARIMA forecast generated successfully\",icon=\"SUCCESS\",level=2)\n",
    "    except Exception as e:\n",
    "        log(f\"Manual SARIMA forecast failed: {e}\",icon=\"WARNING\",level=2,bold=True)\n",
    "        sarima_manual_pred=None\n",
    "# Auto ARIMA forecast\n",
    "arima_auto_pred=None\n",
    "if auto_arima_model is not None:\n",
    "    try:\n",
    "        fc=auto_arima_model.predict(n_periods=h)\n",
    "        arima_auto_pred=pd.Series(fc,index=timeseries_test.index,name=\"arima_auto_pred\")\n",
    "        log(\"Auto ARIMA forecast generated successfully\",icon=\"SUCCESS\",level=2)\n",
    "    except Exception as e:\n",
    "        log(f\"Auto ARIMA forecast failed: {e}\",icon=\"WARNING\",level=2,bold=True)\n",
    "        arima_auto_pred=None\n",
    "# Auto SARIMA forecast\n",
    "sarima_auto_pred=None\n",
    "if auto_sarima_model is not None:\n",
    "    try:\n",
    "        fc=auto_sarima_model.predict(n_periods=h)\n",
    "        sarima_auto_pred=pd.Series(fc,index=timeseries_test.index,name=\"sarima_auto_pred\")\n",
    "        log(\"Auto SARIMA forecast generated successfully\",icon=\"SUCCESS\",level=2)\n",
    "    except Exception as e:\n",
    "        log(f\"Auto SARIMA forecast failed: {e}\",icon=\"WARNING\",level=2,bold=True)\n",
    "        sarima_auto_pred=None\n",
    "# ===============================\n",
    "# EVALUATION (compare vs Naive using MAE/RMSE/MASE/SMAPE)\n",
    "# ===============================\n",
    "log(\"EVALUATION ON TEST (lower is better)\",icon=\"INFO\",bold=True)\n",
    "# Recompute naive metrics defensively (ensures alignment)\n",
    "y_true_naive,y_pred_naive=align_true_pred(timeseries_test,y_pred_naive)\n",
    "mae_naive=mae(y_true_naive,y_pred_naive)\n",
    "rmse_naive=rmse(y_true_naive,y_pred_naive)\n",
    "mase_naive=mase(y_true_naive,y_pred_naive,timeseries_train)\n",
    "smape_naive=smape(y_true_naive,y_pred_naive)\n",
    "log(f\"NAIVE  -> MAE={mae_naive:.6f} | RMSE={rmse_naive:.6f} | MASE={mase_naive:.6f} | SMAPE={smape_naive:.2f}%\",level=2,icon=\"INFO\")\n",
    "# Collect all model predictions to score\n",
    "model_preds={}\n",
    "if arima_manual_pred is not None:\n",
    "    model_preds[\"ARIMA_MANUAL\"]=arima_manual_pred\n",
    "if sarima_manual_pred is not None:\n",
    "    model_preds[\"SARIMA_MANUAL\"]=sarima_manual_pred\n",
    "if arima_auto_pred is not None:\n",
    "    model_preds[\"ARIMA_AUTO\"]=arima_auto_pred\n",
    "if sarima_auto_pred is not None:\n",
    "    model_preds[\"SARIMA_AUTO\"]=sarima_auto_pred\n",
    "if not model_preds:\n",
    "    raise ValueError(\"No model forecasts were produced (ARIMA/SARIMA manual/auto all failed).\")\n",
    "# Score each model vs y_true\n",
    "results=[]\n",
    "for name,pred in model_preds.items():\n",
    "    y_t,y_p=align_true_pred(timeseries_test,pred)\n",
    "    if len(y_t)<10:\n",
    "        log(f\"{name}: not enough aligned points for evaluation\",icon=\"WARNING\",level=2,bold=True)\n",
    "        continue\n",
    "    mae_v=mae(y_t,y_p)\n",
    "    rmse_v=rmse(y_t,y_p)\n",
    "    mase_v=mase(y_t,y_p,timeseries_train)\n",
    "    smape_v=smape(y_t,y_p)\n",
    "    results.append((name,mae_v,rmse_v,mase_v,smape_v))\n",
    "    extra=\"\"\n",
    "    if name==\"ARIMA_MANUAL\":\n",
    "        extra=f\" | order={best_arima_order}\"\n",
    "    if name==\"SARIMA_MANUAL\":\n",
    "        extra=f\" | order={best_sarima_order} seasonal={best_sarima_seasonal}\"\n",
    "    if name==\"ARIMA_AUTO\":\n",
    "        extra=f\" | order={auto_arima_order}\"\n",
    "    if name==\"SARIMA_AUTO\":\n",
    "        extra=f\" | order={auto_sarima_order} seasonal={auto_sarima_seasonal}\"\n",
    "    log(f\"{name} -> MAE={mae_v:.6f} | RMSE={rmse_v:.6f} | MASE={mase_v:.6f} | SMAPE={smape_v:.2f}%{extra}\",level=2,icon=\"INFO\")\n",
    "# Decide winner (primary criterion: MASE, secondary: RMSE)\n",
    "df_results=pd.DataFrame(results,columns=[\"model\",\"mae\",\"rmse\",\"mase\",\"smape\"])\n",
    "df_results=df_results.sort_values(by=[\"mase\",\"rmse\"],ascending=True).reset_index(drop=True)\n",
    "winner=df_results.iloc[0][\"model\"]\n",
    "log(\"MODEL SELECTION (primary=MASE, secondary=RMSE)\",icon=\"INFO\",bold=True)\n",
    "log(f\"Winner on TEST: {winner}\",icon=\"SUCCESS\",level=2,bold=True)\n",
    "# Beat naive checks (using MASE as main decision)\n",
    "log(\"BEAT-NAIVE CHECK (using MASE)\",icon=\"INFO\",bold=True)\n",
    "for _,row in df_results.iterrows():\n",
    "    name=row[\"model\"]\n",
    "    if np.isfinite(row[\"mase\"]) and np.isfinite(mase_naive) and row[\"mase\"]<mase_naive:\n",
    "        log(f\"{name} beats NAIVE on MASE\",icon=\"SUCCESS\",level=2,bold=True)\n",
    "    else:\n",
    "        log(f\"{name} does NOT beat NAIVE on MASE\",icon=\"WARNING\",level=2,bold=True)\n",
    "# ===============================\n",
    "# PLOT: TRAIN/TEST + forecasts (Naive + models)\n",
    "# ===============================\n",
    "log(\"PLOTTING FORECASTS VS TEST\",icon=\"INFO\",bold=True)\n",
    "fig,axis=plt.subplots(figsize=(2*figWidth_unit,figHeight_unit))\n",
    "sns.lineplot(x=timeseries_train.index,y=timeseries_train.values,label=\"Train\")\n",
    "sns.lineplot(x=timeseries_test.index,y=timeseries_test.values,label=\"Test\",color=\"green\")\n",
    "sns.lineplot(x=y_pred_naive.index,y=y_pred_naive.values,label=\"Naive (t-1)\",color=\"red\",alpha=0.35,linewidth=3)\n",
    "for name,pred in model_preds.items():\n",
    "    sns.lineplot(x=pred.index,y=pred.values,label=name,alpha=0.85,linewidth=3)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ===============================\n",
    "# SAVE PREDICTIONS WITH DATETIME INDEX\n",
    "# ===============================\n",
    "rev_number_pred=get_revision_number(processed_data_output_path,\"timeseries_predictions\")\n",
    "pred_path=processed_data_output_path+\"timeseries_predictions_\"+str(rev_number_pred)+\".csv\"\n",
    "df_pred=pd.DataFrame(index=timeseries_test.index)\n",
    "df_pred.index.name=time_column\n",
    "df_pred[\"y_true\"]=timeseries_test.astype(float).values\n",
    "df_pred[\"y_pred_naive\"]=y_pred_naive.reindex(timeseries_test.index).astype(float).values\n",
    "if arima_manual_pred is not None:\n",
    "    df_pred[\"y_pred_arima_manual\"]=arima_manual_pred.astype(float).values\n",
    "if sarima_manual_pred is not None:\n",
    "    df_pred[\"y_pred_sarima_manual\"]=sarima_manual_pred.astype(float).values\n",
    "if arima_auto_pred is not None:\n",
    "    df_pred[\"y_pred_arima_auto\"]=arima_auto_pred.astype(float).values\n",
    "if sarima_auto_pred is not None:\n",
    "    df_pred[\"y_pred_sarima_auto\"]=sarima_auto_pred.astype(float).values\n",
    "df_pred.to_csv(pred_path,index=True)\n",
    "log(f\"Predictions saved with datetime index: {pred_path}\",icon=\"SUCCESS\",bold=True)\n",
    "# ===============================\n",
    "# SAVE METRICS SUMMARY\n",
    "# ===============================\n",
    "metrics_path=processed_data_output_path+\"timeseries_metrics_\"+str(rev_number_pred)+\".csv\"\n",
    "rows=[{\"model\":\"NAIVE\",\"mae\":mae_naive,\"rmse\":rmse_naive,\"mase\":mase_naive,\"smape\":smape_naive,\"order\":\"\",\"seasonal_order\":\"\"}]\n",
    "if best_arima_order is not None and arima_manual_pred is not None:\n",
    "    r=df_results[df_results[\"model\"]==\"ARIMA_MANUAL\"].iloc[0]\n",
    "    rows.append({\"model\":\"ARIMA_MANUAL\",\"mae\":float(r[\"mae\"]),\"rmse\":float(r[\"rmse\"]),\"mase\":float(r[\"mase\"]),\"smape\":float(r[\"smape\"]),\"order\":str(best_arima_order),\"seasonal_order\":str((0,0,0,0))})\n",
    "if best_sarima_order is not None and best_sarima_seasonal is not None and sarima_manual_pred is not None:\n",
    "    r=df_results[df_results[\"model\"]==\"SARIMA_MANUAL\"].iloc[0]\n",
    "    rows.append({\"model\":\"SARIMA_MANUAL\",\"mae\":float(r[\"mae\"]),\"rmse\":float(r[\"rmse\"]),\"mase\":float(r[\"mase\"]),\"smape\":float(r[\"smape\"]),\"order\":str(best_sarima_order),\"seasonal_order\":str(best_sarima_seasonal)})\n",
    "if auto_arima_order is not None and arima_auto_pred is not None:\n",
    "    r=df_results[df_results[\"model\"]==\"ARIMA_AUTO\"].iloc[0]\n",
    "    rows.append({\"model\":\"ARIMA_AUTO\",\"mae\":float(r[\"mae\"]),\"rmse\":float(r[\"rmse\"]),\"mase\":float(r[\"mase\"]),\"smape\":float(r[\"smape\"]),\"order\":str(auto_arima_order),\"seasonal_order\":str((0,0,0,0))})\n",
    "if auto_sarima_order is not None and auto_sarima_seasonal is not None and sarima_auto_pred is not None:\n",
    "    r=df_results[df_results[\"model\"]==\"SARIMA_AUTO\"].iloc[0]\n",
    "    rows.append({\"model\":\"SARIMA_AUTO\",\"mae\":float(r[\"mae\"]),\"rmse\":float(r[\"rmse\"]),\"mase\":float(r[\"mase\"]),\"smape\":float(r[\"smape\"]),\"order\":str(auto_sarima_order),\"seasonal_order\":str(auto_sarima_seasonal)})\n",
    "pd.DataFrame(rows).to_csv(metrics_path,index=False)\n",
    "log(f\"Metrics summary saved: {metrics_path}\",icon=\"SUCCESS\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# REGIME DETECTION + PER-REGIME MODELS (HALVING-BASED)\n",
    "# ===============================\n",
    "log(\"REGIME DETECTION (HALVING-BASED)\", icon=\"8.\", bold=True)\n",
    "series_reg = timeseries.dropna().astype(float).copy()\n",
    "if len(series_reg) < min_regime_points * 2:\n",
    "    log(\"Not enough points for regime detection (need at least 2 regimes with min points).\", icon=\"WARNING\", level=2)\n",
    "    regime_change_detected = False\n",
    "else:\n",
    "    try:\n",
    "        hv_dates = pd.to_datetime(halving_dates, errors=\"coerce\").dropna().sort_values()\n",
    "    except Exception as e:\n",
    "        hv_dates = pd.to_datetime([], errors=\"coerce\")\n",
    "        log(f\"Could not parse halving_dates -> {e}\", icon=\"WARNING\", level=2)\n",
    "\n",
    "    start = series_reg.index.min()\n",
    "    end = series_reg.index.max()\n",
    "    hv_in_range = [d for d in hv_dates if start < d < end]\n",
    "    edges = [start] + hv_in_range + [end]\n",
    "\n",
    "    regimes = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        seg = series_reg.loc[edges[i]:edges[i + 1]]\n",
    "        if len(seg) < min_regime_points:\n",
    "            log(f\"Regime {i+1} skipped (n={len(seg)} < min_regime_points={min_regime_points})\", icon=\"WARNING\", level=2)\n",
    "            continue\n",
    "        reg_mean = float(seg.mean())\n",
    "        reg_std = float(seg.std())\n",
    "        regimes.append({\n",
    "            \"label\": f\"Regime {i+1}: {edges[i].date()} -> {edges[i+1].date()}\",\n",
    "            \"series\": seg,\n",
    "            \"mean\": reg_mean,\n",
    "            \"std\": reg_std,\n",
    "        })\n",
    "        log(f\"{regimes[-1]['label']} | mean={reg_mean:.3f} std={reg_std:.3f} n={len(seg)}\", icon=\"INFO\", level=2)\n",
    "\n",
    "    # Detect regime changes based on mean/std shifts\n",
    "    regime_change_detected = False\n",
    "    for i in range(1, len(regimes)):\n",
    "        prev = regimes[i - 1]\n",
    "        curr = regimes[i]\n",
    "        mean_change = abs(curr[\"mean\"] - prev[\"mean\"]) / (abs(prev[\"mean\"]) + 1e-9)\n",
    "        std_change = abs(curr[\"std\"] - prev[\"std\"]) / (abs(prev[\"std\"]) + 1e-9)\n",
    "        if mean_change >= regime_change_threshold_mean or std_change >= regime_change_threshold_std:\n",
    "            regime_change_detected = True\n",
    "            log(\n",
    "                f\"Regime change detected between '{prev['label']}' and '{curr['label']}' \"\n",
    "                f\"(mean_change={mean_change:.2f}, std_change={std_change:.2f})\",\n",
    "                icon=\"SUCCESS\",\n",
    "                level=2,\n",
    "                bold=True\n",
    "            )\n",
    "        else:\n",
    "            log(\n",
    "                f\"No strong change between '{prev['label']}' and '{curr['label']}' \"\n",
    "                f\"(mean_change={mean_change:.2f}, std_change={std_change:.2f})\",\n",
    "                icon=\"INFO\",\n",
    "                level=2\n",
    "            )\n",
    "\n",
    "# ===============================\n",
    "# PER-REGIME MODELING (only if change detected)\n",
    "# ===============================\n",
    "if regime_change_detected:\n",
    "    log(\"PER-REGIME MODELING (ARIMA/SARIMA)\", icon=\"9.\", bold=True)\n",
    "\n",
    "    def fit_best_orders_small(y_train, d, D, period, p_grid, q_grid, P_grid, Q_grid):\n",
    "        best_arima = None\n",
    "        best_arima_aic = np.inf\n",
    "        for p_try in p_grid:\n",
    "            for q_try in q_grid:\n",
    "                try:\n",
    "                    res = SARIMAX(endog=y_train, order=(p_try, d, q_try), seasonal_order=(0, 0, 0, 0),\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                    if res.aic < best_arima_aic:\n",
    "                        best_arima_aic = res.aic\n",
    "                        best_arima = (p_try, d, q_try)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        best_sarima = None\n",
    "        best_sarima_aic = np.inf\n",
    "        if period is not None and period > 1:\n",
    "            for p_try in p_grid:\n",
    "                for q_try in q_grid:\n",
    "                    for P_try in P_grid:\n",
    "                        for Q_try in Q_grid:\n",
    "                            try:\n",
    "                                res = SARIMAX(endog=y_train, order=(p_try, d, q_try),\n",
    "                                              seasonal_order=(P_try, D, Q_try, period),\n",
    "                                              enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                                if res.aic < best_sarima_aic:\n",
    "                                    best_sarima_aic = res.aic\n",
    "                                    best_sarima = (p_try, d, q_try, P_try, D, Q_try, period)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "        return best_arima, best_sarima\n",
    "\n",
    "    p_grid_r = [0, 1]\n",
    "    q_grid_r = [0, 1]\n",
    "    P_grid_r = [0, 1]\n",
    "    Q_grid_r = [0, 1]\n",
    "\n",
    "    for reg in regimes:\n",
    "        y_reg = reg[\"series\"].copy()\n",
    "        if len(y_reg) < min_regime_points:\n",
    "            continue\n",
    "\n",
    "        split_idx = int(len(y_reg) * (1.0 - test_size))\n",
    "        y_train = y_reg.iloc[:split_idx]\n",
    "        y_test = y_reg.iloc[split_idx:]\n",
    "        if len(y_test) < 10:\n",
    "            log(f\"{reg['label']}: test horizon too small -> skip\", icon=\"WARNING\", level=2)\n",
    "            continue\n",
    "\n",
    "        log(f\"{reg['label']} | train={len(y_train)} test={len(y_test)}\", icon=\"INFO\", level=2, bold=True)\n",
    "\n",
    "        y_pred_naive_reg = y_test.shift(1)\n",
    "        if len(y_train) > 0 and len(y_pred_naive_reg) > 0:\n",
    "            y_pred_naive_reg.iloc[0] = y_train.iloc[-1]\n",
    "        y_true_reg, y_pred_naive_reg = align_true_pred(y_test, y_pred_naive_reg)\n",
    "        mae_naive_reg = mae(y_true_reg, y_pred_naive_reg)\n",
    "        rmse_naive_reg = rmse(y_true_reg, y_pred_naive_reg)\n",
    "        mase_naive_reg = mase(y_true_reg, y_pred_naive_reg, y_train)\n",
    "        smape_naive_reg = smape(y_true_reg, y_pred_naive_reg)\n",
    "        log(f\"Naive -> MAE={mae_naive_reg:.6f} | RMSE={rmse_naive_reg:.6f} | MASE={mase_naive_reg:.6f} | SMAPE={smape_naive_reg:.2f}%\", icon=\"INFO\", level=3)\n",
    "\n",
    "        best_arima_order, best_sarima_order = fit_best_orders_small(\n",
    "            y_train, d, D, period, p_grid_r, q_grid_r, P_grid_r, Q_grid_r\n",
    "        )\n",
    "\n",
    "        model_results = []\n",
    "        if best_arima_order is not None:\n",
    "            try:\n",
    "                res = SARIMAX(endog=y_train, order=best_arima_order, seasonal_order=(0, 0, 0, 0),\n",
    "                              enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                fc = res.get_forecast(steps=len(y_test)).predicted_mean\n",
    "                y_pred = pd.Series(fc.values, index=y_test.index)\n",
    "                y_t, y_p = align_true_pred(y_test, y_pred)\n",
    "                model_results.append((\"ARIMA\", mae(y_t, y_p), rmse(y_t, y_p), mase(y_t, y_p, y_train), smape(y_t, y_p), best_arima_order))\n",
    "            except Exception as e:\n",
    "                log(f\"ARIMA failed for {reg['label']}: {e}\", icon=\"WARNING\", level=3)\n",
    "\n",
    "        if best_sarima_order is not None:\n",
    "            try:\n",
    "                res = SARIMAX(endog=y_train, order=best_sarima_order[:3], seasonal_order=best_sarima_order[3:],\n",
    "                              enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                fc = res.get_forecast(steps=len(y_test)).predicted_mean\n",
    "                y_pred = pd.Series(fc.values, index=y_test.index)\n",
    "                y_t, y_p = align_true_pred(y_test, y_pred)\n",
    "                model_results.append((\"SARIMA\", mae(y_t, y_p), rmse(y_t, y_p), mase(y_t, y_p, y_train), smape(y_t, y_p), best_sarima_order))\n",
    "            except Exception as e:\n",
    "                log(f\"SARIMA failed for {reg['label']}: {e}\", icon=\"WARNING\", level=3)\n",
    "\n",
    "        if not model_results:\n",
    "            log(f\"{reg['label']}: no model results\", icon=\"WARNING\", level=2)\n",
    "            continue\n",
    "\n",
    "        model_results = sorted(model_results, key=lambda x: (x[3], x[2]))\n",
    "        best = model_results[0]\n",
    "        log(\n",
    "            f\"{reg['label']} winner -> {best[0]} | MAE={best[1]:.6f} RMSE={best[2]:.6f} MASE={best[3]:.6f} SMAPE={best[4]:.2f}% | order={best[5]}\",\n",
    "            icon=\"SUCCESS\",\n",
    "            level=2,\n",
    "            bold=True\n",
    "        )\n",
    "else:\n",
    "    log(\"No regime change detected -> per-regime modeling skipped\", icon=\"INFO\", level=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
